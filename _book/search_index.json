[["index.html", "Landscape Genetic Data Analysis with R Introduction", " Landscape Genetic Data Analysis with R Editor: Helene Wagner (University of Toronto) 2021-05-04 Introduction This is a web-interface to the teaching materials for the lab course Landscape Genetic Data Analysis with R associated with the distributed graduate course DGS Landscape Genetics. All teaching materials are included in the R package LandGenCourse available on Github Code visiblity: use the Code button to show or hide all R code, either globally or by chunk. "],["how-to-use-this-book.html", "How to use this Book", " How to use this Book 1. Book Structure This book has weekly chapters that correspond to course modules, with three parts: a) Getting Started Review of R Skills: check whether you need to build or brush up your R skills before starting the course: Basic R Programming: introduction to R objects and functions. Prerequisite. R Graphics: learn to create figures with base R and with ggplot2. Optional. b) Basic Topics These 8 weekly modules are streamlined to build the necessary R skills and brush up statistics knowledge. Best complete these in sequence. Each module has the following components: Video: introduces the R and stats topics Interactive Tutorial: swirl course to practice R programming Worked Example: worked example by the weekly experts from the DGS Landscape Genetics course. Bonus Materials: some weeks include bonus vignettes with optional advanced topics. c) Advanced Topics These weekly modules build on the skills developed in Basic Topics. You may pick and choose from the Advanced Topics according to your interests. Each weekly modules contains: Worked Example: worked example by the weekly experts from the DGS Landscape Genetics course. Bonus Materials: some weeks include bonus vignettes with optional advanced topics. 2. Find what is relevant for you a) How to use the labs Weeks 1 - 8: Beginners: watch video -&gt; do tutorial -&gt; read WE Intermediate: watch video -&gt; tutorial? -&gt; understand WE -&gt; do R exercise Advanced: check video slides -&gt; R exercise? -&gt; adapt WE to own data By Week 9: you will be at least at intermediate if not a pro! b) Check contents Videos: Check first weekly slide, browse slides for each week Interactive tutorials: check weekly list of commands for each chapter Worked examples: check list of packages by vignette (below), check introduction section for each worked example (or bonus vignette) 3. Course R package LandGenCourse a) How to install (or update) if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) ## Loading required package: devtools ## Loading required package: usethis devtools::install_github(&quot;hhwagner1/LandGenCourse&quot;) ## Skipping install of &#39;LandGenCourse&#39; from a github remote, the SHA1 (158fb26a) has not changed since last install. ## Use `force = TRUE` to force installation library(LandGenCourse) b) How to use the R package LandGenCourse The package installs four Add-ins in RStudio. Each will provide you with some dropdown menu choices. Watch Course Video: opens a video resource from course Landscape Genetic Data Analysis with R. Start Tutorial: installs swirl course Landscape_Genetics_R_Course and prints instructions. Choose Worked Example: opens vignette file (.html, .Rmd, or .R) with a worked example from course Landscape Genetic Data Analysis with R. Open Cheat Sheet: opens selected R cheat sheet. c) Video instructions for beginners This video walks through the process of installing devtools, the course package, and using the RStudio Add-Ins. Intro_LandGenCourse_small.mp4 "],["list-of-packages-by-vignette.html", "List of Packages by Vignette", " List of Packages by Vignette Note: lowercase letters refer to bonus vignettes. E.g., 2a is the (first) bonus vignette of Week 2. {r echo=FALSE) Table &lt;- readRDS(paste0(here::here(), \"/output/Full.rds\")) Table %&gt;% kable() %&gt;% kable_styling(bootstrap_options = c(\"bordered\", \"striped\", \"condensed\", \"responsive\")) %&gt;% kable_styling(fixed_thead = T, full_width=F, font_size = 10) %&gt;% column_spec(1, border_left = T) %&gt;% column_spec(1, border_right = T) %&gt;% column_spec(c(1,3,13, ncol(Table)+1), border_right = T, width=2) %&gt;% column_spec(c(2,4:12,14:ncol(Table)), border_right = T) "],["review-of-r-skills.html", "Review of R Skills ", " Review of R Skills "],["basic-r-programming.html", "Basic R Programming", " Basic R Programming 1. Overview This worked example is adapted from Applied Population Genetics by Rodney Dyer. The entire book is available here: http://dyerlab.github.io/applied_population_genetics/index.html R is a statistical programming language, and it thus requires users to work with code. This can be intimidating at first. Working through this document will help you get up to speed with basic R concepts and notation. Whether you are new to R or need a refresher, this worked example will get you to the level expected for the Landscape Genetics with R lab course. The main topics covered here are: R data types (how data are stored in R: numeric, character, etc.) R containers (how data are organized: vectors, data frames, etc.) R functions (how to tell R what to do) See also video Week 0: Intro to R Notebooks for options how to work this this document as .html or .Rmd. Install packages needed for this worked example. Note: popgraph needs to be installed before installing gstudio. if(!requireNamespace(&quot;popgraph&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/popgraph&quot;) if(!requireNamespace(&quot;gstudio&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/gstudio&quot;) 2. Data Types The data we work with comes in many formsintegers, stratum, categories, genotypes, etc.all of which we need to be able to work with in our analyses. In this chapter, the basic data types we will commonly use in population genetic analyses. This section covers some of the basic types of data we will use in R. These include numbers, character, factors, and logical data types. We will also introduce the locus object from the gstudio library and see how it is just another data type that we can manipulate in R. The very first hurdle you need to get over is the oddness in the way in which R assigns values to variables. variable &lt;- value Yes that is a less-than and dash character. This is the assignment operator that historically has been used and it is the one that I will stick with. In some cases you can use the = to assign variables instead but then it takes away the R-ness of R itself. For decision making, the equality operator (e.g., is this equal to that) is the double equals sign ==. We will get into that below where we talk about logical types and later in decision making. If you are unaware of what type a particular variable may be, you can always use the type() function and R will tell you. class( variable ) R also has a pretty good help system built into itself. You can get help for any function by typing a question mark in front of the function name. This is a particularly awesome features because at the end of the help file, there is often examples of its usage, which are priceless. Here is the documentation for the help function as given by: ?help There are also package vignettes available (for most packages you download) that provide additional information on the routines, data sets, and other items included in these packages. You can get a list of vignettes currently installed on your machine by: vignette() and vignettes for a particular package by passing the package name as an argument to the function itself. 2.1. Numeric Data Types a. Numeric data The quantitative measurements we make are often numeric, in that they can be represented as as a number with a decimal component (think weight, height, latitude, soil moisture, ear wax viscosity, etc.). The most basic type of data in R, is the numeric type and represents both integers and floating point numbers (n.b., there is a strict integer data type but it is often only needed when interfacing with other C libraries and can for what we are doing be disregarded). Assigning a value to a variable is easy x &lt;- 3 x ## [1] 3 By default, R automatically outputs whole numbers numbers within decimal values appropriately. y &lt;- 22/7 y ## [1] 3.142857 If there is a mix of whole numbers and numbers with decimals together in a container such as c(x,y) ## [1] 3.000000 3.142857 then both are shown with decimals. The c() part here is a function that combines several data objects together into a vector and is very useful. In fact, the use of vectors are are central to working in R and functions almost all the functions we use on individual variables can also be applied to vectors. A word of caution should be made about numeric data types on any computer. Consider the following example. x &lt;- .3 / 3 x ## [1] 0.1 which is exactly what wed expect. However, the way in which computers store decimal numbers plays off our notion of significant digits pretty well. Look what happens when I print out x but carry out the number of decimal places. print(x, digits=20) ## [1] 0.099999999999999992 Not quite 0.1 is it? Not that far away from it but not exact. That is a general problem, not one that R has any more claim to than any other language and/or implementation. Does this matter much, probably not in the realm of the kinds of things we do in population genetics, it is just something that you should be aware of. You can make random sets of numeric data by using using functions describing various distributions. For example, some random numbers from the normal distribution are: rnorm(10) ## [1] 1.0843885 1.5240141 -0.1627353 0.6506739 -1.5411313 0.5455624 ## [7] -0.1726998 1.7320880 1.2617459 0.5080992 from the normal distribution with designated mean and standard deviation: rnorm(10,mean=42,sd=12) ## [1] 36.07236 39.68195 41.82852 43.84721 15.53284 55.33996 21.31400 55.79583 ## [9] 33.52674 21.95830 A poisson distribution with mean 2: rpois(10,lambda = 2) ## [1] 1 1 0 1 4 2 0 1 3 1 and the \\(\\chi^2\\) distribution with 1 degree of freedom: rchisq(10, df=1) ## [1] 0.0049214949 0.4399817710 1.0404346622 1.5771802503 1.4771469106 ## [6] 0.0002917578 0.9461987121 0.9434081571 2.1103061665 1.0926371679 There are several more distributions that if you need to access random numbers, quantiles, probability densities, and cumulative density values are available. b. Coercion to Numeric All data types have the potential ability to take another variable and coerce it into their type. Some combinations make sense, and some do not. For example, if you load in a CSV data file using read_csv(), and at some point a stray non-numeric character was inserted into one of the cells on your spreadsheet, R will interpret the entire column as a character type rather than as a numeric type. This can be a very frustrating thing, spreadsheets should generally be considered evil as they do all kinds of stuff behind the scenes and make your life less awesome. Here is an example of coercion of some data that is initially defined as a set of characters x &lt;- c(&quot;42&quot;,&quot;99&quot;) x ## [1] &quot;42&quot; &quot;99&quot; and is coerced into a numeric type using the as.numeric() function. y &lt;- as.numeric( x ) y ## [1] 42 99 It is a built-in feature of the data types in R that they all have (or should have if someone is producing a new data type and is being courteous to their users) an as.X() function. This is where the data type decides if the values asked to be coerced are reasonable or if you need to be reminded that what you are asking is not possible. Here is an example where I try to coerce a non-numeric variable into a number. x &lt;- &quot;The night is dark and full of terrors...&quot; as.numeric( x ) ## Warning: NAs introduced by coercion ## [1] NA By default, the result should be NA (missing data/non-applicable) if you ask for things that are not possible. 2.2. Characters a. Character data A collection of letters, number, and or punctuation is represented as a character data type. These are enclosed in either single or double quotes and are considered a single entity. For example, my name can be represented as: prof &lt;- &quot;Rodney J. Dyer&quot; prof ## [1] &quot;Rodney J. Dyer&quot; In R, character variables are considered to be a single entity, that is the entire prof variable is a single unit, not a collection of characters. This is in part due to the way in which vectors of variables are constructed in the language. For example, if you are looking at the length of the variable I assigned my name to you see length(prof) ## [1] 1 which shows that there is only one character variable. If, as is often the case, you are interested in knowing how many characters are in the variable prof, then you use the nchar(prof) ## [1] 14 function instead. This returns the number of characters (even the non-printing ones like tabs and spaces. nchar(&quot; \\t &quot;) ## [1] 3 As all other data types, you can define a vector of character values using the c() function. x &lt;- &quot;I am&quot; y &lt;- &quot;not&quot; z &lt;- &#39;a looser&#39; terms &lt;- c(x,y,z) terms ## [1] &quot;I am&quot; &quot;not&quot; &quot;a looser&quot; And looking at the length() and nchar() of this you can see how these operations differ. length(terms) ## [1] 3 nchar(terms) ## [1] 4 3 8 b. Concatenation of Characters Another common use of characters is concatenating them into single sequences. Here we use the function paste() and can set the separators (or characters that are inserted between entities when we collapse vectors). Here is an example, entirely fictional and only provided for instructional purposes only. paste(terms, collapse=&quot; &quot;) ## [1] &quot;I am not a looser&quot; paste(x,z) ## [1] &quot;I am a looser&quot; paste(x,z,sep=&quot; not &quot;) ## [1] &quot;I am not a looser&quot; c. Coercion to Characters A character data type is often the most basal type of data you can work with. For example, consider the case where you have named sample locations. These can be kept as a character data type or as a factor (see below). There are benefits and drawbacks to each representation of the same data (see below). By default (as of the version of R I am currently using when writing this book), if you use a function like read_table() to load in an external file, columns of character data will be treated as factors. This can be good behavior if all you are doing is loading in data and running an analysis, or it can be a total pain in the backside if you are doing more manipulative analyses. Here is an example of coercing a numeric type into a character type using the as.character() function. x &lt;- 42 x ## [1] 42 y &lt;- as.character(x) y ## [1] &quot;42&quot; 2.3. Factors a. Factor vs. Character A factor is a categorical data type. If you are coming from SAS, these are class variables. If you are not, then perhaps you can think of them as mutually exclusive classifications. For example, an sample may be assigned to one particular locale, one particular region, and one particular species. Across all the data you may have several species, regions, and locales. These are finite, and defined, sets of categories. One of the more common headaches encountered by people new to R is working with factor types and trying to add categories that are not already defined. Since factors are categorical, it is in your best interest to make sure you label them in as descriptive as a fashion as possible. You are not saving space or cutting down on computational time to take shortcuts and label the locale for Rancho Santa Maria as RSN or pop3d or 5. Our computers are fast and large enough, and our programmers are cleaver enough, to not have to rename our populations in numeric format to make them work (hello STRUCTURE Im calling you out here). The only thing you have to loose by adopting a reasonable naming scheme is confusion in your output. To define a factor type, you use the function factor() and pass it a vector of values. region &lt;- c(&quot;North&quot;,&quot;North&quot;,&quot;South&quot;,&quot;East&quot;,&quot;East&quot;,&quot;South&quot;,&quot;West&quot;,&quot;West&quot;,&quot;West&quot;) region &lt;- factor( region ) region ## [1] North North South East East South West West West ## Levels: East North South West When you print out the values, it shows you all the levels present for the factor. If you have levels that are not present in your data set, when you define it, you can tell R to consider additional levels of this factor by passing the optional levels= argument as: region &lt;- factor( region, levels=c(&quot;North&quot;,&quot;South&quot;,&quot;East&quot;,&quot;West&quot;,&quot;Central&quot;)) region ## [1] North North South East East South West West West ## Levels: North South East West Central If you try to add a data point to a factor list that does not have the factor that you are adding, it will give you an error (or barf as I like to say). region[1] &lt;- &quot;Bob&quot; ## Warning in `[&lt;-.factor`(`*tmp*`, 1, value = &quot;Bob&quot;): invalid factor level, NA ## generated Now, I have to admit that the Error message in its entirety, with its [&lt;-.factor(*tmp*, 1, value = Bob)` part is, perhaps, not the most informative. Agreed. However, the invalid factor level does tell you something useful. Unfortunately, the programmers that put in the error handling system in R did not quite adhere to the spirit of the fail loudly mantra. It is something you will have to get good at. Google is your friend, and if you post a questions to (http://stackoverflow.org) or the R user list without doing serious homework, put on your asbestos shorts! Unfortunately, the error above changed the first element of the region vector to NA (missing data). Ill turn it back before we move too much further. region[1] &lt;- &quot;North&quot; Factors in R can be either unordered (as say locale may be since locale A is not &gt;, =, or &lt; locale B) or they may be ordered categories as in Small &lt; Medium &lt; Large &lt; X-Large. When you create the factor, you need to indicate if it is an ordered type (by default it is not). If the factors are ordered in some way, you can also create an ordination on the data. If you do not pass a levels= option to the factors() function, it will take the order in which they occur in data you pass to it. If you want to specify an order for the factors specifically, pass the optional levels= and they will be ordinated in the order given there. region &lt;- factor( region, ordered=TRUE, levels = c(&quot;West&quot;, &quot;North&quot;, &quot;South&quot;, &quot;East&quot;) ) region ## [1] North North South East East South West West West ## Levels: West &lt; North &lt; South &lt; East b. Missing Levels in Factors There are times when you have a subset of data that do not have all the potential categories. subregion &lt;- region[ 3:9 ] subregion ## [1] South East East South West West West ## Levels: West &lt; North &lt; South &lt; East table( subregion ) ## subregion ## West North South East ## 3 0 2 2 2.4. Logical Types A logical type is either TRUE or FALSE, there is no in-between. It is common to use these types in making decisions (see if-else decisions) to check a specific condition being satisfied. To define logical variables you can either use the TRUE or FALSE directly canThrow &lt;- c(FALSE, TRUE, FALSE, FALSE, FALSE) canThrow ## [1] FALSE TRUE FALSE FALSE FALSE or can implement some logical condition stable &lt;- c( &quot;RGIII&quot; == 0, nchar(&quot;Marshawn&quot;) == 8) stable ## [1] FALSE TRUE on the variables. Notice here how each of the items is actually evaluated as to determine the truth of each expression. In the first case, the character is not equal to zero and in the second, the number of characters (what nchar() does) is indeed equal to 8 for the character string Marshawn. It is common to use logical types to serve as indices for vectors. Say for example, you have a vector of data that you want to select some subset from. data &lt;- rnorm(20) data ## [1] 0.73394781 -1.39813012 -0.10205639 1.07382826 0.86360540 -0.82034145 ## [7] 2.08606195 0.04605896 -1.35948447 -0.33159029 0.36301780 -1.08476098 ## [13] -0.41057148 -0.40434257 -1.62671566 0.11938732 0.01863762 0.86187701 ## [19] -1.34472168 -0.96515973 Perhaps you are on interested in the non-negative values data[ data &gt; 0 ] ## [1] 0.73394781 1.07382826 0.86360540 2.08606195 0.04605896 0.36301780 0.11938732 ## [8] 0.01863762 0.86187701 If you look at the condition being passed to as the index data &gt; 0 ## [1] TRUE FALSE FALSE TRUE TRUE FALSE TRUE TRUE FALSE FALSE TRUE FALSE ## [13] FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE you see that individually, each value in the data vector is being evaluated as a logical value, satisfying the condition that it is strictly greater than zero. When you pass that as indices to a vector it only shows the indices that are TRUE. You can coerce a value into a logical if you understand the rules. Numeric types that equal 0 (zero) are FALSE, always. Any non-zero value is considered TRUE. Here I use the modulus operator, %%, which provides the remainder of a division. 1:20 %% 2 ## [1] 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 which used as indices give us data[ (1:20 %% 2) &gt; 0 ] ## [1] 0.73394781 -0.10205639 0.86360540 2.08606195 -1.35948447 0.36301780 ## [7] -0.41057148 -1.62671566 0.01863762 -1.34472168 You can get as complicated in the creation of indices as you like, even using logical operators such as OR and AND. I leave that as an example for you to play with. 3. Data Containers We almost never work with a single datum1, rather we keep lots of data. Moreover, the kinds of data are often heterogeneous, including categorical (Populations, Regions), continuous (coordinates, rainfall, elevation), imagry (hyperspectral, LiDAR), and perhaps even genetic. R has a very rich set of containers into which we can stuff our data as we work with it. Here these container types are examined and the restrictions and benefits associated with each type are explained. 3.1. Vectors We have already seen several examples of several vectors in action (see the introduction to Numeric data types for example). A vector of objects is simply a collection of them, often created using the c() function (c for combine). Vectorized data is restricted to having homogeneous data typesyou cannot mix character and numeric types in the same vector. If you try to mix types, R will either coerce your data into a reasonable type x &lt;- c(1,2,3) x ## [1] 1 2 3 y &lt;- c(TRUE,TRUE,FALSE) y ## [1] TRUE TRUE FALSE z &lt;- c(&quot;I&quot;,&quot;am&quot;,&quot;not&quot;,&quot;a&quot;,&quot;looser&quot;) z ## [1] &quot;I&quot; &quot;am&quot; &quot;not&quot; &quot;a&quot; &quot;looser&quot; or coearce them into one type that is amenable to all the types of data that you have given it. In this example, a Logical, Character, Constant, and Function are combined resulting in a vector output of type Character. w &lt;- c(TRUE, &quot;1&quot;, pi, ls()) w ## [1] &quot;TRUE&quot; &quot;1&quot; &quot;3.14159265358979&quot; &quot;canThrow&quot; ## [5] &quot;data&quot; &quot;prof&quot; &quot;region&quot; &quot;stable&quot; ## [9] &quot;subregion&quot; &quot;terms&quot; &quot;x&quot; &quot;y&quot; ## [13] &quot;z&quot; class(w) ## [1] &quot;character&quot; Accessing elements within a vector are done using the square bracket [] notation. All indices (for vectors and matrices) start at 1 (not zero as is the case for some languages). Getting and setting the components within a vector are accomplished using numeric indices with the assignment operators just like we do for variables containing a single value. x ## [1] 1 2 3 x[1] &lt;- 2 x[3] &lt;- 1 x ## [1] 2 2 1 x[2] ## [1] 2 A common type of vector is that of a sequences. We use sequences all the time, to iterate through a list, to counting generations, etc. There are a few ways to generate sequences, depending upon the step sequence. For a sequence of whole numbers, the easiest is through the use of the colon operator. x &lt;- 1:6 x ## [1] 1 2 3 4 5 6 This provides a nice shorthand for getting the values X:Y from X to Y, inclusive. It is also possible to go backwards using this operator, counting down from X to Y as in: x &lt;- 5:2 x ## [1] 5 4 3 2 The only constraint here is that we are limited to a step size of 1.0. It is possible to use non-integers as the bounds, it will just count up by 1.0 each time. x &lt;- 3.2:8.4 x ## [1] 3.2 4.2 5.2 6.2 7.2 8.2 If you are interested in making a sequence with a step other than 1.0, you can use the seq() function. If you do not provide a step value, it defaults to 1.0. y &lt;- seq(1,6) y ## [1] 1 2 3 4 5 6 But if you do, it will use that instead. z &lt;- seq(1,20,by=2) z ## [1] 1 3 5 7 9 11 13 15 17 19 It is also possible to create a vector of objects as repetitions using the rep() (for repeat) function. rep(&quot;Beetlejuice&quot;,3) ## [1] &quot;Beetlejuice&quot; &quot;Beetlejuice&quot; &quot;Beetlejuice&quot; If you pass a vector of items to rep(), it can repeat these as either a vector being repeated (the default value) x &lt;- c(&quot;No&quot;,&quot;Free&quot;,&quot;Lunch&quot;) rep(x,time=3) ## [1] &quot;No&quot; &quot;Free&quot; &quot;Lunch&quot; &quot;No&quot; &quot;Free&quot; &quot;Lunch&quot; &quot;No&quot; &quot;Free&quot; &quot;Lunch&quot; or as each item in the vector repeated. rep(x,each=3) ## [1] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;Free&quot; &quot;Free&quot; &quot;Free&quot; &quot;Lunch&quot; &quot;Lunch&quot; &quot;Lunch&quot; 3.2. Matrices A matrix is a 2- or higher dimensional container, most commonly used to store numeric data types. There are some libraries that use matrices in more than two dimensions (rows and columns and sheets), though you will not run across them too often. Here I restrict myself to only 2-dimensional matrices. You can define a matrix by giving it a set of values and an indication of the number of rows and columns you want. The easiest matrix to try is one with empty values: matrix(nrow=2, ncol=2) ## [,1] [,2] ## [1,] NA NA ## [2,] NA NA Perhaps more useful is one that is pre-populated with values. matrix(1:4, nrow=2 ) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Notice that here, there were four entries and I only specified the number of rows required. By default the filling-in of the matrix will proceed down column (by-column). In this example, we have the first column with the first two entries and the last two entries down the second column. If you want it to fill by row, you can pass the optional argument matrix(1:4, nrow=2, byrow=TRUE) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 and it will fill by-row. When filling matrices, the default size and the size of the data being added to the matrix are critical. For example, I can create a matrix as: Y &lt;- matrix(c(1,2,3,4,5,6),ncol=2,byrow=TRUE) Y ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 or X &lt;- matrix(c(1,2,3,4,5,6),nrow=2) X ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 and both produce a similar matrix, only transposed. X == t(Y) ## [,1] [,2] [,3] ## [1,] TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE In the example above, the number of rows (or columns) was a clean multiple of the number of entries. However, if it is not, R will fill in values. X &lt;- matrix(c(1,2,3,4,5,6),ncol=4, byrow=TRUE) ## Warning in matrix(c(1, 2, 3, 4, 5, 6), ncol = 4, byrow = TRUE): data length [6] ## is not a sub-multiple or multiple of the number of columns [4] Notice how you get a warning from the interpreter. But that does not stop it from filling in the remaining slots by starting over in the sequence of numbers you passed to it. X ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 1 2 The dimensionality of a matrix (and data.frame as we will see shortly) is returned by the dim() function. This will provide the number of rows and columns as a vector. dim(X) ## [1] 2 4 Accessing elements to retrieve or set their values within a matrix is done using the square brackets just like for a vector but you need to give [row,col] indices. Again, these are 1-based so that X[1,3] ## [1] 3 is the entry in the 1st row and 3rd column. You can also use slices through a matrix to get the rows X[1,] ## [1] 1 2 3 4 or columns X[,3] ## [1] 3 1 of data. Here you just omit the index for the entity you want to span. Notice that when you grab a slice, even if it is a column, is given as a vector. length(X[,3]) ## [1] 2 You can grab a sub-matrix using slices if you give a range (or sequence) of indices. X[,2:3] ## [,1] [,2] ## [1,] 2 3 ## [2,] 6 1 If you ask for values from a matrix that exceed its dimensions, R will give you an error. X[1,8] ## Error in X[1, 8] : subscript out of bounds ## Calls: &lt;Anonymous&gt; ... handle -&gt; withCallingHandlers -&gt; withVisible -&gt; eval -&gt; eval ## Execution halted There are a few cool extensions of the rep() function that can be used to create matrices as well. They are optional values that can be passed to the function. times=x: This is the default option that was occupied by the 3 in the example above and represents the number of times that first argument will be repeated. each=x This will take each element in the first argument are repeat them each times. length.out=x: This make the result equal in length to x. In combination, these can be quite helpful. Here is an example using numeric sequences in which it is necessary to find the index of all entries in a 3x2 matrix. To make the indices, I bind two columns together using cbind(). There is a matching row binding function, denoted as rbind() (perhaps not so surprisingly). What is returned is a matrix indices &lt;- cbind( rep(1:2, each=3), rep(1:3,times=2), rep(5,length.out=6) ) indices ## [,1] [,2] [,3] ## [1,] 1 1 5 ## [2,] 1 2 5 ## [3,] 1 3 5 ## [4,] 2 1 5 ## [5,] 2 2 5 ## [6,] 2 3 5 3.3. Lists A list is a type of vector but is indexed by keys rather than by numeric indices. Moreover, lists can contain heterogeneous types of data (e.g., values of different class), which is not possible in a vector type. For example, consider the list theList &lt;- list( x=seq(2,40, by=2), dog=LETTERS[1:5], hasStyle=logical(5) ) summary(theList) ## Length Class Mode ## x 20 -none- numeric ## dog 5 -none- character ## hasStyle 5 -none- logical which is defined with a numeric, a character, and a logical component. Each of these entries can be different in length as well as type. Once defined, the entries may be observed as: theList ## $x ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 ## ## $dog ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; ## ## $hasStyle ## [1] FALSE FALSE FALSE FALSE FALSE Once created, you can add variables to the list using the $-operator followed by the name of the key for the new entry. theList$my_favoriate_number &lt;- 2.9 + 3i or use double brackets and the name of the variable as a character string. theList[[&quot;lotto numbers&quot;]] &lt;- rpois(7,lambda=42) The keys currently in the list are given by the names() function names(theList) ## [1] &quot;x&quot; &quot;dog&quot; &quot;hasStyle&quot; ## [4] &quot;my_favoriate_number&quot; &quot;lotto numbers&quot; Getting and setting values within a list are done the same way using either the $-operator theList$x ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 theList$x[2] &lt;- 42 theList$x ## [1] 2 42 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 or the double brackets theList[[&quot;x&quot;]] ## [1] 2 42 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 or using a numeric index, but that numeric index is looks to the results of names() to figure out which key to use. theList[[2]] ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; The use of the double brackets in essence provides a direct link to the variable in the list whose name is second in the names() function (dog in this case). If you want to access elements within that variable, then you add a second set of brackets on after the double ones. theList[[1]][3] ## [1] 6 This deviates from the matrix approach as well as from how we access entries in a data.frame (described next). It is not a single square bracket with two indices, that gives you an error: theList[1,3] ## Error in theList[1, 3] : incorrect number of dimensions ## Calls: &lt;Anonymous&gt; ... handle -&gt; withCallingHandlers -&gt; withVisible -&gt; eval -&gt; eval ## Execution halted List are rather robust objects that allow you to store a wide variety of data types (including nested lists). Once you get the indexing scheme down, it they will provide nice solutions for many of your computational needs. 3.4. Data Frames a. Data Frames as spreadsheets The data.frame is the default data container in R. It is analogous to both a spreadsheet, at least in the way that I have used spreadsheets in the past, as well as a database. If you consider a single spreadsheet containing measurements and observations from your research, you may have many columns of data, each of which may be a different kind of data. There may be factors representing designations such as species, regions, populations, sex, flower color, etc. Other columns may contain numeric data types for items such as latitude, longitude, dbh, and nectar sugar content. You may also have specialized columns such as dates collected, genetic loci, and any other information you may be collecting. On a spreadsheet, each column has a unified data type, either quantified with a value or as a missing value, NA, in each row. Rows typically represent the sampling unit, perhaps individual or site, along which all of these various items have been measured or determined. A data.frame is similar to this, at least conceptually. You define a data.frame by designating the columns of data to be used. You do not need to define all of them, more can be added later. The values passed can be sequences, collections of values, or computed parameters. For example: df &lt;- data.frame( ID=1:5, Names=c(&quot;Bob&quot;,&quot;Alice&quot;,&quot;Vicki&quot;,&quot;John&quot;,&quot;Sarah&quot;), Score=100 - rpois(5,lambda=10)) df ## ID Names Score ## 1 1 Bob 86 ## 2 2 Alice 94 ## 3 3 Vicki 84 ## 4 4 John 92 ## 5 5 Sarah 90 You can see that each column is a unified type of data and each row is equivalent to a record. Additional data columns may be added to an existing data.frame as: df$Passed_Class &lt;- c(TRUE,TRUE,TRUE,FALSE,TRUE) Since we may have many (thousands?) of rows of observations, a summary() of the data.frame can provide a more compact description. summary(df) ## ID Names Score Passed_Class ## Min. :1 Length:5 Min. :84.0 Mode :logical ## 1st Qu.:2 Class :character 1st Qu.:86.0 FALSE:1 ## Median :3 Mode :character Median :90.0 TRUE :4 ## Mean :3 Mean :89.2 ## 3rd Qu.:4 3rd Qu.:92.0 ## Max. :5 Max. :94.0 We can add columns of data to the data.frame after the fact using the $-operator to indicate the column name. Depending upon the data type, the summary will provide an overview of what is there. b. Indexing Data Frames You can access individual items within a data.frame by numeric index such as: df[1,3] ## [1] 86 You can slide indices along rows (which return a new data.frame for you) df[1,] ## ID Names Score Passed_Class ## 1 1 Bob 86 TRUE or along columns (which give you a vector of data) df[,3] ## [1] 86 94 84 92 90 or use the $-operator as you did for the list data type to get direct access to a either all the data or a specific subset therein. df$Names[3] ## [1] &quot;Vicki&quot; Indices are ordered just like for matrices, rows first then columns. You can also pass a set of indices such as: df[1:3,] ## ID Names Score Passed_Class ## 1 1 Bob 86 TRUE ## 2 2 Alice 94 TRUE ## 3 3 Vicki 84 TRUE It is also possible to use logical operators as indices. Here I select only those names in the data.frame whose score was &gt;90 and they passed popgen. df$Names[df$Score &gt; 90 &amp; df$Passed_Class==TRUE] ## [1] &quot;Alice&quot; This is why data.frame objects are very database like. They can contain lots of data and you can extract from them subsets that you need to work on. This is a VERY important feature, one that is vital for reproducible research. Keep you data in one and only one place. 4. Programming One of the strengths of R as an analysis platform is that it is a language rather than a program. With programs, such as SPSS &amp; JMP, you are limited by the functionality that the designers thought would be necessary to meet the broadest audience. In R, you can rely upon simple functions or you can create entire analysis and simulation programs de novo. To do this, we need to dig into flow control and decision making processes, both of which you need for doing more in-depth programming. 4.1. Function Writing Here we look at how to create an R function. Writing small functions like this is a huge benefit to you as an analyst and this is a great place to start. A function in R is defined as: function_name &lt;- function( arguments ) { Stuff you want the function to do } You define a function name for whatever you like and assign it the stuff to the right. In R, the function named function() is a special one, it tells R that you are about to create a little routine and you want that set of code to be available to you for later use under the name of whatever you named it. This allows a tremendous amount of flexibility as you develop your own set of routines and analyses for your work. The part that actually does stuff is after the function call. It may be that the function that you create does need some data (those are the arguments) or you may not need any input in to the function (in which you pass no arguments). It all depends upon what you are creating. The key to understanding functions is that they are encapsulations of codea shortcut for a sequence of instructions if you will not have to type over and over again. The less typing you do, the lower the probability that you will have errors (and all code has errors). Here is an example of some code that Im going to develop into a function. This function will allow me to determine if one genotype could possibly be the offspring of the other genotype. library(gstudio) loc1 &lt;- locus( c(128,130) ) loc2 &lt;- locus( c(128,128) ) cat( loc1, loc2 ) ## 128:130 128:128 We start out with two loci, a 128:130 heterozygote and a 128:128 homozygote. These may represent repeat motifs at a microsatellite locus or some other co-dominant genotype. First, Ill break the locus into a vector of genotypes. off.alleles &lt;- alleles( loc1 ) off.alleles ## [1] &quot;128&quot; &quot;130&quot; mom.alleles &lt;- alleles( loc2 ) mom.alleles ## [1] &quot;128&quot; &quot;128&quot; To be a valid potential offspring there should be at least one of the alleles in the parent that matches the allele in the offspring. The intersect() function returns the set of values common to both vectors. shared &lt;- intersect( off.alleles, mom.alleles ) shared ## [1] &quot;128&quot; If it has at least one of the alleles present (it could have both if parent and offspring are both the same heterozygote) then you cannot exclude this individual as a potential offspring. If there are no alleles in common, then the value returned is an empty vector. loc3 &lt;- locus( c(132,132)) dad.alleles &lt;- alleles( loc3 ) intersect( mom.alleles, dad.alleles ) ## character(0) This logic can be shoved into a function. You have to wrap it into a set of curly brackets. I use the length of the result from the intersect() to return from the function. Potential values for potential_offspring &lt;- function( parent, offspring ) { off &lt;- alleles( offspring ) par &lt;- alleles( loc2 ) shared &lt;- intersect( off, par ) return( length( shared ) &gt; 0 ) } Now, you can call this function anytime you need, just passing it two genotypes. If they can be offspring it returns TRUE, as in the comparison between 128:130 and 128:128 genotypes. potential_offspring(loc1, loc2) ## [1] TRUE And it returns FALSE for the comparison between 128:128 and 132:132. potential_offspring(loc2, loc3) ## [1] FALSE 4.2. Variable Scope There is a lot more information on writing functions and we will get into that as we progress through the text. However, it is important that I bring this up now. The value assigned to a variable is defined by its scope. Consider the following code x &lt;- 10 and the function defined as do_it &lt;- function( x ) { x &lt;- x + 10 return( x ) } When I call the function, the variable x that is the argument of the function is not the same variable that is in the environment that I assigned a value of 10. The x in the function argument is what we call local to that function in that within the curly brackets that follow (and any number of curly brackets nested within those, the value of x is given whatever was passed to the function. 4.3. Decision Making We interact with our data in many ways and introspection of the values we have in the variables we are working with are of prime importance. Decision making in your code is where you evaluate your data and make a choice of outcomes based upon some criteria. Here is some example data that we can use as we explore the basics of if(), if(){} else{}, and if(){} elif(){} else{} coding patterns. a. The if Pattern The most basic version of decision making is asking a single question and if the answer is TRUE then do something. The if(){} function does this and has the form if( CRITERIA ) { DO_SOMETHING } You pass a logical statement (or something that can be coerced into a logical type) to the function as the CRITERIA and if it evaluates to TRUE, then the contents of the DO_SOMETHING are executed. If the value of CRITERIA is not TRUE the DO_SOMETHING is skipped entirelyit is not even seen by the interpreter. Here we can test this out using the loci defined above along with the is_heterozygote() function. This function takes one or more locus objects and returns TRUE/FALSE if they are or are not a heterozygote. is_heterozygote( c(loc1, loc2) ) ## [1] TRUE FALSE If we shove that function into the if() parameters we can use its evaluation of the heterozygous state of the locus to do something interesting, say tell us it is a heterozygoteit is admittedly a contrived example, but hey you try to make easy examples, it is not easy. if( is_heterozygote(loc1) ){ print(&quot;It&#39;s a het!&quot;) } ## [1] &quot;It&#39;s a het!&quot; If the is_heterozygote() function returns a value of FALSE, then the contents of the if() function (the stuff within the curly brackets is skipped entirely. if( is_heterozygote(loc2) ){ print(&quot;It&#39;s a het!&quot;) } Notice, there was no indication of any of that code inside the curly brackets. The if-else Pattern If there are more than on thing you want to potentially do when making a decision, you can add an else clause after the if pattern. Here if is_heterozygote() returns FALSE, the contents of the else{} clause will be executed. Here is the heterozygote example if( is_heterozygote(loc1) ) { cat(loc1, &quot;is a heterozygote&quot;) } else { cat(loc1, &quot;is a homozygote&quot;) } ## 128:130 is a heterozygote and the homozygote one if( is_heterozygote(loc2) ) { cat(loc2, &quot;is a heterozygote&quot;) } else { cat(loc2, &quot;is a homozygote&quot;) } ## 128:128 is a homozygote There is a slightly shorter version of this that is available for the lazy programmer and lets be honest, all programmers are lazy and the more you can accomplish with fewer strokes on the keyboard the better (this is how we got emacs and vim). I generally dont teach the shortcuts up front, but this one is short and readily apparent so it may be more helpful than confusing. The ifelse() function has three parts, the condition, the result if TRUE, and the result if FALSE. ans &lt;- ifelse( is_heterozygote( c(loc1, loc2)) , &quot;heterozygote&quot;, &quot;Not&quot;) ans ## [1] &quot;heterozygote&quot; &quot;Not&quot; So iterating through the x vector, the condition x&gt;0 is evaluated and if TRUE the sqrt() of the value is returned, else the NA is given. It is compact and easy to use so you may run into it often. b. The if-else Pattern It is possible to test many conditions in a single sequence by stringing together else-if conditions. The point that is important here is that the first condition that evaluates to TRUE will be executed and all remaining ones will be skipped, even if they also are logically TRUE. This means that it is important to figure out the proper order of asking your conditions. Here is an example function that determines if none, one, or both of the genotypes passed to it are heterozygotes. By default, I step through every one of the potential options of available on this comparison. 1. The first is a heterozygote and the second one isnt 2. The first one isnt and the second one is 3. Both are heterozygotes 4. The last state (both are not) Here is the function. which_is_het &lt;- function( A, B) { if( is_heterozygote(A) &amp; !is_heterozygote(B) ) { print(&quot;First is heterozygote&quot;) } else if( !is_heterozygote(A) &amp; is_heterozygote(B) ){ print(&quot;Second is heterozygote&quot;) } else if( is_heterozygote(A) &amp; is_heterozygote(B) ){ print(&quot;Both are heterozygotes&quot;) } else { print( &quot;Neither are heterozygotes&quot;) } } It is possible that the order of these CRITERIA could be changed, the important thing to remember is that the sequence of if - else if - else if etc. will terminate the very first time one of the CRITERIA is evaluated to be TRUE. 4.4. Flow Control Flow control is the process of iterating across objects and perhaps doing operations on those objects. The R language has several mechanisms that you can use to control the flow of a script or bit of code. a. The for() Loop x &lt;- c(3,8,5,4,6) x ## [1] 3 8 5 4 6 You can iterate through this vector using a for() loop. This is a simple function that has the form: for( SOME_SEQUENCE ){ DO_SOMETHING } Where the SOME_SEQUENCE component is a sequence of values either specified OR calculated and the DO_SOMETHING is the thing you want to do with each of the values in the sequence. Usually, there is a variable defined in the SOME_SEQUENCE component and the value of that variable is used. Here are a few examples. The first goes through the existing vector directly and assigns (in sequential order) the entries of x to the variable val. We can then do whatever we want with the value in val (though if we change it, nothing happens to the original x vector). for( val in x ){ print(val) } ## [1] 3 ## [1] 8 ## [1] 5 ## [1] 4 ## [1] 6 We can also specify a sequence directly and then use it as an index. Here I use an index variable named i to take on the integer seqeunce equal in length to the length of the original x variable. Then I can iterate through the original vector and use that index variable to grab the value I want. for( i in 1:length(x)){ print( x[i] ) } ## [1] 3 ## [1] 8 ## [1] 5 ## [1] 4 ## [1] 6 Both give us the same output, namely a way to go through the variable x. However, there may be a need to use the latter approach in your calculations. For example, perhaps I want to do some other operation on the values. In this very contrived example that follows, I want to perform operations on the values in x depending on if they are even or odd. For the odd ones, I add the corresponding value in y and if not I subtract it. Sure, this is totally contrived and I cannot think of a reason why I would be doing this, but if I need to know what index (row, column or whatever) an entry is during the iteration process, then I need to use this approach over the for( val in x) approach. y &lt;- 1:5 for( i in 1:length(x)){ if( x[i] %% 2) print( x[i] + y[i]) else print( x[i] - y[i] ) } ## [1] 4 ## [1] 6 ## [1] 8 ## [1] 0 ## [1] 1 b. Short Circuiting the Loop It is possible to short circuit the looping process using the keywords next and break, though in my programming style, I consider their use in my source files as evidence of inelegant code. That said, you may need them on occasion. The next keyword basically stops all commands after that during the current iteration of the loop. It does not terminate the loop itself, it just stops the commands that follow it this time through. Here is an example that uses the modulus operator, %% (e.g., the remainder after division), to print out only those numbers that are divisible by three. for( i in 1:20 ){ if( i %% 3 ) next cat(&quot;The value of i =&quot;,i,&quot;\\n&quot;) } ## The value of i = 3 ## The value of i = 6 ## The value of i = 9 ## The value of i = 12 ## The value of i = 15 ## The value of i = 18 The use of break to exit the loop entirely is perhaps more commonly encountered. When this keyword is encountered, the loop terminates immediately, as if it reached the send of the sequence. for( i in 1:10){ if( i &gt; 2 ) break cat(&quot;The value of i=&quot;,i,&quot;\\n&quot;) } ## The value of i= 1 ## The value of i= 2 The word data is plural, datum is singular "],["r-graphics.html", "R Graphics", " R Graphics 1. Overview This worked example is adapted from Applied Population Genetics by Rodney Dyer. The entire book is available here: http://dyerlab.github.io/applied_population_genetics/index.html One of the most critical features of data analysis is the ability to present your results in a logical and meaningful fashion. R has built-in functions that can provide you graphical output that will suffice for your understanding and interpretations. However, there are also third-party packages that make some truly beautiful output. In this section, both built-in graphics and graphical output from the ggplot2 library are explained and highlighted. We are going to use the venerable iris dataset that was used by Anderson (1935) and Fisher (1936). These data are measurements of four morphological variables (Sepal Length, Sepal Width, Petal Length, and Petal Width) measured on fifty individual iris plants from three recognized species. Here is a summary of this data set. summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## I will also provide examples using two different plotting approaches. R has a robust set of built-in graphical routines that you can use. However, the development community has also provided several additional graphics libraries available for creating output. The one I prefer is ggplot2 written by Hadley Wickham. His approach in designing this library a philosophy of graphics depicted in Leland Wilksons (2005) book The Grammar of Graphics. As I understand it, the idea is that a graphical display consists of several layers of information. These layers may include: The underlying data. Mapping of the data onto one or more axes. Geometric representations of data as points, lines, and/or areas. Transformations of the axes into different coordinate spaces (e.g., cartesian, polar, etc.) or the data onto different scales (e.g., logrithmic) Specification of subplots. In the normal plotting routines discussed before, configuration of these layers were specified as arguments passed to the plotting function (plot(), boxplot(), etc.). The ggplot2 library takes a different approach, allowing you to specify these components separately and literally add them together like components of a linear model. Required packages: library(ggplot2) library(RColorBrewer) 2. Univariate plots 2.1. Barplots and histograms Univariate data can represent either counts (e.g., integers) of items or vectors of data that have decimal components (e.g., frequency distributions, etc.). If the sampling units are discrete, then a the barplot() function can make a nice visual representation. Here is an example using some fictions data. x &lt;- c(2, 3, 6, 3, 2, 4) names(x) &lt;- c(&quot;Bob&quot;, &quot;Alice&quot;, &quot;Jane&quot;, &quot;Fred&quot;, &quot;Barney&quot;, &quot;Lucy&quot;) x ## Bob Alice Jane Fred Barney Lucy ## 2 3 6 3 2 4 From this, a barplot can be constructed where the x-axis has discrete entities for each name and the y-axis represents the magnitude of whatever it is we are measuring. barplot(x, xlab = &quot;People&quot;, ylab = &quot;Thinn-A-Ma-Jigs&quot;) Notice here that I included labels for both the x- and y-axes. You do this by inserting these arguments into the parameters passed to the barplot() (it is the same for all built-in plotting as we will see below). To use ggplot for displaying this, you have to present the data in a slightly different way. There are shortcuts using the qplot() function but I prefer to use the more verbose approach. Here is how we would plot the same output using ggplot. library(ggplot2) df &lt;- data.frame(Thinn_A_Ma_Jigs=x,names=names(x)) ggplot( df, aes(x=names,y=Thinn_A_Ma_Jigs)) + geom_bar(stat=&quot;identity&quot;) There are a couple of things to point out about this. This is a compound statement, the plotting commands are literally added together. The ggplot() function specifies a data.frame from which your data will be extracted. The aes() component within the ggplot() function describes the aesthetics of the plot. This is how you designate which variable columns in the data.frame will be on the x- and y- axes, colors, plot shapes, categories, etc. To the basic description of the data and aesthetics, a geometry is added, that of a barplot. If necessary, you can pass additional information to the geom_bar() function. In this case, I had to tell it that you use the raw data stat='identity' instead of trying to summarize many observations. The axis labels are taken directly from the names of the columns in the data.frame. If the data on the x-axis is not discrete but measured on a single variable, then the hist() function can take the data and categorize it into groups based upon the density of the underlying data. h &lt;- hist(iris$Sepal.Length) The hist() function itself returns a bit of information that may be of interest. It is not necessary that you capture these data, R will ignore it if you do not assign it to a variable. What is of interest though is that it returns a list() object with the following keys: names(h) ## [1] &quot;breaks&quot; &quot;counts&quot; &quot;density&quot; &quot;mids&quot; &quot;xname&quot; &quot;equidist&quot; The data within these keys can be accessed directly as: h ## $breaks ## [1] 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 ## ## $counts ## [1] 5 27 27 30 31 18 6 6 ## ## $density ## [1] 0.06666667 0.36000000 0.36000000 0.40000000 0.41333333 0.24000000 0.08000000 ## [8] 0.08000000 ## ## $mids ## [1] 4.25 4.75 5.25 5.75 6.25 6.75 7.25 7.75 ## ## $xname ## [1] &quot;iris$Sepal.Length&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; and provide the basic information on how to construct the histogram as shown below. The values defined for the break locations for each of the bars and the midpoints are the default ones for the hist() function. If you wish, you can provide the number of breaks in the data, the location of the breaks, including the lowest and highest terms, plot as a count versus a frequency, etc. The corresponding ggplot approach is ggplot( iris, aes(x=Sepal.Length)) + geom_bar() which also takes a few liberties with the default binning parameters. By default, both the R and ggplot approches make a stab at creating enough of the meta data around the plot to make it somewhat useful. However, it is helpful if we add a bit to that and create graphics that suck just a little bit less. There are many options that we can add to the standard plot command to make it more informative and/or appropriate for your use. The most common ones include: xlab Change the label on the x-axis ylab Change the lable on the y-axis main Change the title of the graph col Either a single value, or a vector of values, indicating the color to be plot. xlim/ylim The limits for the x- and y-axes. Additional options are given in tabular form in the next section. Here is another plot of the same data but spiffied up a bit. x &lt;- hist( iris$Sepal.Length,xlab=&quot;Sepal Length (cm)&quot;, ylab=&quot;Count&quot;, main=&quot;&quot;, col=&quot;lightblue&quot;) A few things to notice: The main title over the top of the image was visually removed by assigning an empty characters string to it. The color parameter refers to the fill color of the bars, not the border color. In ggplot we add those additional components either to the geometry where it is defined (e.g., the color in the geom_bar) or to the overall plot (as in the addition of labels on the axes). ggplot( iris, aes(x=Sepal.Length)) + geom_bar(fill=&quot;lightblue&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Count&quot;) 2.2. Plotting Parameters There are several parameters common to the various plotting functions used in the basic R plotting functions. Here is a short table of the most commonly used ones. Command Usage Description bg bg=white Sets the background color for the entire figure. bty bty=n Sets the style of the box type around the graph. Useful values are o for complete box (the default), l, 7, c, u, ] which will make a box with sides around the plot area resembling the upper case version of these letters, and n for no box. cex cex=2 Magnifies the default font size by the corresponding factor. cex.axis cex.axis=2 Magnifies the font size on the axes. col col=blue Sets the plot color for the points, lines, etc. fg fg=blue Sets the foreground color for the image. lty lty=1 Sets the type of line as 0-none, 1-solid, 2-dashed, 3-dotted, etc. lwd lwd=3 Specifies the width of the line. main main=title Sets the title to be displayed over the top of the graph. mfrow mfrow=c(2,2) Creates a matrix of plots in a single figure. pch pch=16 Sets the type of symbol to be used in a scatter plot. sub sub=subtitle Sets the subtitle under the main title on the graph. type type=l Specifies the type of the graph to be shown using a generic plot() command. Types are p for points (default), l for lines, and b for both. xlab xlab=Size (m) Sets the label attached to the x-axis. ylab ylab=Frequency Sets the label attached to the y-axis. 2.3. Density Plots Another way of looking at the distribution of univariate data is through the use of the density() function. This function takes a vector of values and creates a probability density function of it returning an object of class density. d &lt;- density( iris$Sepal.Length) attributes(d) ## $names ## [1] &quot;x&quot; &quot;y&quot; &quot;bw&quot; &quot;n&quot; &quot;call&quot; &quot;data.name&quot; ## [7] &quot;has.na&quot; ## ## $class ## [1] &quot;density&quot; The density() function takes the univariate data and fits its density (internally represented by d$y) along the range of values (in d$x). A plot can be produced using d as the variables as follows (with a bit of extra plotting parameters as depicted in the previous table). plot( d, col = &quot;red&quot;, lwd = 2, xlab = &quot;Value of X&quot;, ylab = &quot;Frequency&quot;, bty = &quot;n&quot;, main = &quot;Density Plot&quot;) The corresponding ggplot approach is: ggplot( iris, aes(x=Sepal.Length)) + geom_density() 3. Bivariate plots 3.1. Overlaying Plots by grouping The data in the previous density plot represents the sepal lengths across all three iris species. It may be more useful to plot them as the density of each species instead of combined. Overlaying plots is a pretty easy feature built into the default R plotting functionalities. Lets look at the data and see if the mean length differs across species. I do this using the by() command, which takes three parameters; the first parameter is the raw data you want to examine, the second one is the way you would like to partition that data, and the third one is the function you want to call on those groupings of raw data. This general form by( data, grouping, function ) mixing both data and functions to be used, is pretty common in R and you will see it over and over again. The generic call below asks to take the sepal data and partition it by species and estimate the mean. by( iris$Sepal.Length, iris$Species, mean) ## iris$Species: setosa ## [1] 5.006 ## ------------------------------------------------------------ ## iris$Species: versicolor ## [1] 5.936 ## ------------------------------------------------------------ ## iris$Species: virginica ## [1] 6.588 So there may be differences. Lets pull the data out and create density plots for each. d.setosa &lt;- iris$Sepal.Length[ iris$Species==&quot;setosa&quot; ] d.versicolor &lt;- iris$Sepal.Length[ iris$Species==&quot;versicolor&quot; ] d.virginica &lt;- iris$Sepal.Length[ iris$Species==&quot;virginica&quot; ] d.se &lt;- density( d.setosa ) d.ve &lt;- density( d.versicolor ) d.vi &lt;- density( d.virginica ) I can now plot the densities independently. After the first plot() function is called, you can add to it by using lines() or points() function calls. They will overlay the subsequent plots over the first one. One of the things you need to be careful of is that you need to make sure the x- and y-axes are properly scaled so that subsequent calls to points() or lines() does not plot stuff outside the boundaries of your initial plot. Here I plot the setosa data first, specify the xlim (limits of the x-axis), set the color, and labels. On subsequent plotting calls, I do not specify labels but set alternative colors. Then a nice legend is placed on the graph, the coordinates of which are specified on the values of the x- and y-axis (I also dislike the boxes around graphic so I remove them as well with bty=\"n\"). plot(d.se,xlim=c(4,8),col=&quot;red&quot;, lwd=2, bty=&quot;n&quot;, xlab=&quot;Sepal Length (cm)&quot;, main=&quot;Sepal Lengths&quot;) lines( d.ve, xlim=c(4,8), col=&quot;green&quot;,lwd=2, bty=&quot;n&quot;) lines( d.vi, xlim=c(4,8), col=&quot;blue&quot;, lwd=2, bty=&quot;n&quot;) legend( 6.5,1.1,c(&quot;I. setosa&quot;, &quot;I. versicolor&quot;, &quot;I. virginica&quot;), col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;), lwd=2,bty=&quot;n&quot;) A lot of that background material is unnecessary using geom_density() because we can specify to the plotting commands that the data are partitioned by the values contained in the Species column of the data.frame. This allows us to make this plot ggplot(iris, aes(Sepal.Length,color=Species)) + geom_density() or a correpsonding one using fill=Species instead (I set the alpha transparency to allow you to see the plots in the background). ggplot(iris, aes(Sepal.Length,fill=Species)) + geom_density( alpha=0.8) We can use the barplot() function here as well and either stack or stagger the density of sepal lengths using discrete bars. Here I make a matrix of bins using the hist() function with a specified set of breaks and then use it to plot discrete bin counts using the barplot() function. I include this example here because there are times when you want to produce a stacked bar plot (rarely) or a staggered barplot (more common) for some univariate data source and I always forget how to specifically do that. breaks &lt;- seq(4,8,by=0.2) h.se &lt;- hist(d.setosa, breaks=breaks, plot = FALSE) h.ve &lt;- hist(d.versicolor, breaks=breaks, plot=FALSE) h.vi &lt;- hist(d.virginica, breaks=breaks, plot=FALSE) vals &lt;- rbind( h.se$counts, h.ve$counts, h.vi$counts ) rownames(vals) &lt;- levels(iris$Species) colnames(vals) &lt;- breaks[1:20] vals ## 4 4.2 4.4 4.6 4.8 5 5.2 5.4 5.6 5.8 6 6.2 6.4 6.6 6.8 7 7.2 7.4 7.6 ## setosa 0 4 5 7 12 11 6 2 3 0 0 0 0 0 0 0 0 0 0 ## versicolor 0 0 0 0 3 2 1 10 8 6 6 5 3 4 2 0 0 0 0 ## virginica 0 0 0 0 1 0 0 1 4 3 4 11 4 7 3 4 2 1 4 ## 7.8 ## setosa 0 ## versicolor 0 ## virginica 1 The matrix of data barplot(vals,xlab=&quot;Sepal Length&quot;, ylab=&quot;Frequency&quot;) Stacked barplots may or may not be that informative, depending upon the complexity of your underlying data. It is helpful though to be able to stagger them. In the basic barplot() function allows you to specify the bars to be spaced beside each other as: barplot(vals,xlab=&quot;Sepal Length&quot;, ylab=&quot;Frequency&quot;, col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;), beside=TRUE) legend(60, 10, c(&quot;I. setosa&quot;, &quot;I. versicolor&quot;, &quot;I. virginica&quot;), fill = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;), bty=&quot;n&quot;) For plotting onto a barplot object, the x-axis number is not based upon the labels on the x-axis. It is an integer that relates to the number of bar-widths and separations. In this example, there are three bars for each category plus one separator (e.g., the area between two categories). So I had to plot the legend at 60 units on the x-coordinate, which puts it just after the 15th category (e.g., 15*4=60). We can do the same thing with geom_histogram(), though again with a bit less typing involved. Here is the raw plot (which by default stacks just like in the barplot() example) ggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. and the correspondingly staggered plot with bars positioned next to eachother. ggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_histogram(position=&quot;dodge&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Notice how we add the fill color to the aes() function because the categories for assigning colors will be extracted from the data.frame, whereas when we just wanted to set them all light blue, the color is specified outside the aes() function. Also notice when we specify something to the aes() command in this way, we do not quote the name of the column, we call it just as if it were a normal variable. 3.2. Boxplots Boxplots are a display of distributions of continuous data within categories, much like what we displayed in the previous staggered barplot. It is often the case that boxplots depict differences in mean values while giving some indication of the dispersal around that mean. This can be done in the staggered barplot as above but not quite as specifically. The default display for boxplots in R provides the requires a factor and a numeric data type. The factor will be used as the categories for the x-axis on which the following visual components will be displayed: The median of the data for each factor, An optional 5% confidence interval (shown by passing notch=TRUE) around the mean. The inner 50th percentile of the data, Outlier data points. Here is how the sepal length can be plot as a function of the Iris species. boxplot(Sepal.Length ~ Species, data=iris, notch=TRUE, xlab=&quot;Species&quot;, ylab=&quot;Sepal Length (cm)&quot;, frame.plot=FALSE) A couple of things should be noted on this plot call. First, I used a functional form for the relationship between the continuous variable (Sepal.Length) and the factor (Species) with the response variable indicated on the left side of the tilde and the predictor variables on the right side. It is just as reasonable to use boxplot(Species,Sepal.Length, ...) in the call as x- and y- variables in the first two positions. However, it reads a bit better to do it as a function, like a regression. Also, it should be noted that I added the optional term, data=iris, to the call. This allowed me to reference the columns in the iris data.frame without the dollar sign notation. Without that I would have had to write boxplot( data$Sepal.Length ~ data$Species, ...), a much more verbose way of doing it but most programmers are relatively lazy and anything they can do to get more functionality out of less typing Finally, I set frame.plot=FALSE to remove the box around the plot since the bty=\"n\" does not work on boxplots (big shout out for consistency!). I dont know why these are different, I just hate the box. The corresponding ggplot approach is the same as the boxplot() one, we just have to specify the notch=TRUE in the geom_boxplot() function. ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_boxplot(notch = TRUE) + xlab(&quot;Iris Species&quot;) + ylab(&quot;Sepal Width (cm)&quot;) 3.3. Scatter Plots With two sets of continuous data, we can produce scatter plots. These consist of 2- (or 3-) coordinates where we can plot our data. With the addition of alternative colors (col=) and/or plot shapes (pch=) passed to the generic plot() command, we can make really informative graphical output. Here I plot two characteristics of the iris data set, sepal length and width, and use the species to indicate alternative symbols. plot( iris$Sepal.Length, iris$Sepal.Width, pch=as.numeric(iris$Species), bty=&quot;n&quot;, xlab=&quot;Sepal Length (cm)&quot;, ylab=&quot;Sepal Width (cm)&quot;) legend( 6.75, 4.3, c(&quot;I. setosa&quot;, &quot;I. versicolor&quot;, &quot;I. virginica&quot;), pch=1:3, bty=&quot;n&quot;) The call to pch= that I used coerced a factor into a numeric data type. By default, this will create a numeric sequence (starting at 1) for all levels of the factor, including ones that may not be in the data you are doing the conversion on. The parameter passed to pch is an integer that determines the shape of the symbol being plot. I typically forget which numbers correspond to which symbols (there are 25 of them in total ) and have to look them up when I need them. One of the easiest ways is just to plot them as: plot(1:25,1:25,pch=1:25, bty=&quot;n&quot;, xlab=&quot;pch&quot;, ylab=&quot;pch&quot;) which produces the following figure where you can choose the appropriate symbols for your plotting needs. A scatter plot in ggplot is created using the geom_point() layer. ggplot(iris,aes(x=Sepal.Length,y=Sepal.Width, shape=Species)) + geom_point() and the shapes are specified automatically. 4. Advanced plotting 4.1. Multiple Plots The last figure was a bit confusing. You can see a few points where the three species have the same measurements for both sepal length and widthboth I. versicolor and I. virginica have examples with sepal widths of 3.0cm and lengths of 6.7cm. For some of these it would be difficult to determine using a graphical output like this if there were more overlap. In this case, it may be a more informative approach to make plots for each species rather than using different symbols. In ggplot, there is a facet geometry that can be added to a plot that can pull apart the individual species plots (in this case) and plot them either next to each other (if you are looking at y-axis differences), on top of each other (for x-axis comparisons), or as a grid (for both x- and y- axis comparisons). This layer is added to the plot using facet_grid() and take a functional argument (as I used for the boxplot example above) on which factors will define rows and columns of plots. Here is an example where I stack plots by species. ggplot( iris, aes(x=Sepal.Width, y=Sepal.Length ) ) + geom_point() + facet_grid(Species~.) + xlab(&quot;Sepal Width (cm)&quot;) + ylab(&quot;Sepal Length&quot;) The Species ~ . means that the rows will be defined by the levels indicatd in iris$Species and all the plotting (the period part) will be done as columns. If you have more than one factor, you can specify RowFactor ~ ColumnFactor and it will make the corresponding grid. I would argue that this is a much more intuitive display of differences in width than the previous plot. You can achieve a similar effect using built-in plotting functions as well. To do this, we need to mess around a bit with the plotting attributes. These are default parameters (hence the name par) for plotting that are used each time we make a new graphic. Here are all of them (consult with the previous Table for some of the meanings). names( par() ) ## [1] &quot;xlog&quot; &quot;ylog&quot; &quot;adj&quot; &quot;ann&quot; &quot;ask&quot; &quot;bg&quot; ## [7] &quot;bty&quot; &quot;cex&quot; &quot;cex.axis&quot; &quot;cex.lab&quot; &quot;cex.main&quot; &quot;cex.sub&quot; ## [13] &quot;cin&quot; &quot;col&quot; &quot;col.axis&quot; &quot;col.lab&quot; &quot;col.main&quot; &quot;col.sub&quot; ## [19] &quot;cra&quot; &quot;crt&quot; &quot;csi&quot; &quot;cxy&quot; &quot;din&quot; &quot;err&quot; ## [25] &quot;family&quot; &quot;fg&quot; &quot;fig&quot; &quot;fin&quot; &quot;font&quot; &quot;font.axis&quot; ## [31] &quot;font.lab&quot; &quot;font.main&quot; &quot;font.sub&quot; &quot;lab&quot; &quot;las&quot; &quot;lend&quot; ## [37] &quot;lheight&quot; &quot;ljoin&quot; &quot;lmitre&quot; &quot;lty&quot; &quot;lwd&quot; &quot;mai&quot; ## [43] &quot;mar&quot; &quot;mex&quot; &quot;mfcol&quot; &quot;mfg&quot; &quot;mfrow&quot; &quot;mgp&quot; ## [49] &quot;mkh&quot; &quot;new&quot; &quot;oma&quot; &quot;omd&quot; &quot;omi&quot; &quot;page&quot; ## [55] &quot;pch&quot; &quot;pin&quot; &quot;plt&quot; &quot;ps&quot; &quot;pty&quot; &quot;smo&quot; ## [61] &quot;srt&quot; &quot;tck&quot; &quot;tcl&quot; &quot;usr&quot; &quot;xaxp&quot; &quot;xaxs&quot; ## [67] &quot;xaxt&quot; &quot;xpd&quot; &quot;yaxp&quot; &quot;yaxs&quot; &quot;yaxt&quot; &quot;ylbias&quot; To create multiple plots on one graphic, we need to modify either the mfrow or mfcol property (either will do). They represent the number of figures to plot as a 2-integer vector for rows and columns. By default, it is set to par()$mfrow ## [1] 1 1 because there is only 1 row and 1 column in the plot. To change this, we simply pass a new value to the par() command and then do the plotting. In the following example, I plot the results of a linear model (lm()) function call. This returns four plots looking at the normality of the data, residuals, etc. This time, instead of seeing them one at a time, Im going to plot all four into one figure. par(mfrow=c(2,2)) plot( lm( Sepal.Length ~ Sepal.Width, data=iris)) 4.2. Color Palettes The default plotting colors in R are black and white. However, there is a rich set of colors available for your plotting needs. The easiest set are named colors. At the time of this writing, there are length(colors()) ## [1] 657 different colors available for your use. Not all are distinct as some overlap. However the benefit of these colors is that they have specific names, making it easier for you to remember than RGB or hex representations. Here are a random set of 20 color names. colors()[ sample.int( length(colors()), size=20) ] ## [1] &quot;gray14&quot; &quot;chocolate3&quot; &quot;lightslategrey&quot; &quot;gray15&quot; ## [5] &quot;palegreen2&quot; &quot;brown2&quot; &quot;tan&quot; &quot;gray39&quot; ## [9] &quot;black&quot; &quot;azure4&quot; &quot;khaki2&quot; &quot;brown4&quot; ## [13] &quot;pink1&quot; &quot;lightpink&quot; &quot;darkkhaki&quot; &quot;maroon3&quot; ## [17] &quot;firebrick&quot; &quot;seagreen2&quot; &quot;palegreen1&quot; &quot;mediumturquoise&quot; To use these colors you can call them by name in the col= option to a plot. Here is an example where I define three named colors and then coerce the iris$Species variable into an integer to select the color by species and plot it in a scatter plot (another version of the pch= example previously). colors &lt;- c(&quot;royalblue1&quot;, &quot;orange1&quot;, &quot;green3&quot;) cols &lt;- colors[ iris$Species ] plot( Sepal.Width ~ Sepal.Length, data=iris, col=cols, xlab=&quot;Sepal Length (cm)&quot;, ylab=&quot;Sepal Width (cm)&quot;, bty=&quot;n&quot;, pch=16) There is a lot of colors to choose from, and you are undoubtedly able to fit any color scheme on any presentation you may encounter. In addition to named colors, there are color palettes available for you to grab the hex value for individual color along a pre-defined gradient. These colors ramps are: rainbow(): A palette covering the visible spectrum heat.colors(): A palette ranging from red, through orange and yellow, to white. terrain.colors(): A palette for plotting topography with lower values as green and increasing through yellow, brown, and finally white. topo.colors(): A palette starting at dark blue (water) and going through green (land) and yellow to beige (mountains). cm.colors(): A palette going from light blue through white to pink. and are displayed in the following figure. The individual palette functions return the hex value for an equally separated number of colors along the palette, you only need to ask for the number of colors you are requesting. Here is an example from the rainbow() function. rainbow(10) ## [1] &quot;#FF0000&quot; &quot;#FF9900&quot; &quot;#CCFF00&quot; &quot;#33FF00&quot; &quot;#00FF66&quot; &quot;#00FFFF&quot; &quot;#0066FF&quot; ## [8] &quot;#3300FF&quot; &quot;#CC00FF&quot; &quot;#FF0099&quot; 4.3. RColorBrewer The ggplot2 library has a slightly more interesting color palette, but it too is often a bit lacking. You have total control over the colors you produce and can specify them as RGB, hex (e.g. the way the web makes colors), or CMYK. You can also define your own color palettes. To start, the RColorBrewer library provides several palettes for plotting either quantitative, qualitative, or divergent data. These three groups and the names of individual color palettes can be viewed as: library(RColorBrewer) display.brewer.all() In ggplot, we can use these palettes as follows. You can change the default palette in ggplot output adding the scale_color_brewer() (and scale_fill_brewer() if you are doing fill colors like in a barplot) to the plot. Here is an example where I change the default palette to the 6th divergent palette. p &lt;- ggplot(iris,aes(x=Sepal.Length,y=Sepal.Width, color=Species)) + geom_point() p &lt;- p + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) p + scale_color_brewer(type=&quot;div&quot;, palette=6) 4.4. Saving Imagery Creating a graphic for display on your screen is only the first step. For it to be truly useful, you need to save it to a file, often with specific parameters such as DPI or image size, so that you can use it outside of R. For normal R graphics, you can copy the current graphical device to a file using dev.copy() or you can create a graphic file object and plot directly to it instead of the display. In both cases, you will need to make sure that you have your graphic formatted the way you like. Once you have the file configured the way you like, decide on the image format you will want. Common ones are jpg, png, tiff and pdf. Here are the function arguments for each of the formats. Notes: png is preferred over jpg because jpg compression results in loss. The command pdf does not take a units argument, it assumes that the unit is inches. Only the most commonly used arguments are listed here, for further arguments, see the help file for each function. jpeg(filename = &quot;Rplot%03d.jpeg&quot;, width = 480, height = 480, units = &quot;px&quot;, pointsize = 12) png(filename = &quot;Rplot%03d.png&quot;, width = 480, height = 480, units = &quot;px&quot;, pointsize = 12, bg = &quot;white&quot;, res = 300) tiff(filename = &quot;Rplot%03d.tiff&quot;, width = 480, height = 480, units = &quot;px&quot;, pointsize = 12, compression = &quot;none&quot;, bg = &quot;white&quot;) pdf(file = &quot;Rplot%03d.pdf&quot;, width = 6, height = 6, pointsize = 12, bg = &quot;white&quot;) These are the default values. If you need a transparent background, it is easiest to use the png file and set bg=\"transparent\". If you are producing an image for a publication, you will most likely be given specific parameters about the image format in terms of DPI, pointsize, dimensions, etc. They can be set here. To create the image, call the appropriate function above, and a file will be created for your plotting. You must then call the plotting functions to create your graphic. Instead of showing up in a graphical display window, they will instead be plot to the file. When done you must tell R that your plotting is now finished and it can close the graphics file. This is done by calling dev.off(). Here is an example workflow saving an image of a scatter plot with a smoothed line through it using ggplot. png( filename = &quot;MyCoolGraphic.png&quot;, quality=300, bg=&quot;transparent&quot;, width=1080, height=1080) ggplot( df, aes(x=PredictorVar, y=ResponseVar)) + geom_point() + stat_smooth(method=&quot;loess&quot;) + theme_bw() dev.off() This will create a file MyCoolGraphic.png saved in the same folder as the current working directory. To use dev.copy() instead, you first make the graphic and then copy the current graphic to a file. For this example, it would be: ggplot( df, aes(x=PredictorVar, y=ResponseVar)) + geom_point() + stat_smooth(method=&quot;loess&quot;) + theme_bw() dev.copy(device=png,file=&quot;MyDevCopyGraphic.png&quot;) dev.off() In general, I prefer the first method as it allows me to specify the specifics of the file in an easier fashion than the second approach. If you are using ggplot2 graphics, there is a built-in function ggsave() that can also be used to save your currently displaying graphic (or others if you so specify) to file. Here are the specifics on that function call. ggsave(filename = default_name(plot), plot = last_plot(), device = default_device(filename), path = NULL, scale = 1, width = par(&quot;din&quot;)[1], height = par(&quot;din&quot;)[2], units = c(&quot;in&quot;, &quot;cm&quot;, &quot;mm&quot;), dpi = 300, limitsize = TRUE, ...) Either approach will allow you to produce publication quality graphics. 4.5. Interactive Graphics There are several other libraries available for creating useful plots in R. The most dynamical ones are based on javascript and as such are restricted to web or ebook displays. Throughout this book, I will be showing how you can use various libraries to create interactive content as well as providing some widgets that will demonstrate how some feature of popualtion or landscape structure changes under a specified model. These widgets are also written in R and are hosted online as a shiny application. I would encourage you to look into creating dynamcial reports using these techniques, they can facilitate a better understanding of the underlying biology than static approaches. "],["further-r-resources.html", "Further R Resources", " Further R Resources "],["Week1.html", "Week 1: Importing Genetic Data", " Week 1: Importing Genetic Data In this first lab of the course, we will explore different ways of importing genetic data into R. Along the way, we will learn about data models in R and how to avoid common errors. "],["video-1.html", "View Course Video", " View Course Video Preview Slides Download "],["tutorial-1.html", "Interactive Tutorial 1", " Interactive Tutorial 1 List of R commands used Function Package data utils View utils write.csv utils read.csv utils head utils class base lapply base as.character base read_csv readr read_population gstudio df2genind adegenet dim base Instructions a) How to access tutorials: Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (Landscape_Genetics_R_Course) and tutorial (Weeks 1 - 8) b) How to complete tutorial: Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: MyName2) c) How to submit answers (participating institutions only): The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose yes, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use skip). If you used skip because you could not answer a question, please contact your instructor for advice. "],["WE-1.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals This worked example shows: How microsatellite data may be coded in a spreadsheet. How such a file is imported into R. How the data are imported into a genind object using package adegenet. How to view information stored in a genind object. How to import the data with the gstudio package. Try modifying the code to import your own data! b. Data set This code imports genetic data from 181 individuals of Colombia spotted frogs (Rana luteiventris) from 12 populations. The data are a subsample of the full data set analyzed in Funk et al. (2005) and Murphy et al. (2010). Please see the separate introduction to the data set. ralu.loci: Data frame with populations and genetic data (181 rows x 9 columns). Included in package LandGenCourse. c. Required R libraries Install some packages needed for this worked example. Note: popgraph needs to be installed before installing gstudio. if(!require(&quot;adegenet&quot;)) install.packages(&quot;adegenet&quot;) ## Loading required package: adegenet ## Loading required package: ade4 ## Registered S3 method overwritten by &#39;spdep&#39;: ## method from ## plot.mst ape ## ## /// adegenet 2.1.3 is loaded //////////// ## ## &gt; overview: &#39;?adegenet&#39; ## &gt; tutorials/doc/questions: &#39;adegenetWeb()&#39; ## &gt; bug reports/feature requests: adegenetIssues() if(!requireNamespace(&quot;popgraph&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/popgraph&quot;) if(!requireNamespace(&quot;gstudio&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/gstudio&quot;) Note: the function library will always load the package, even if it is already loaded, whereas require will only load it if it is not yet loaded. Either will work. library(adegenet) library(gstudio) library(LandGenCourse) library(tibble) library(here) 2. Import data from .csv file a. Preparation Create a new R project for this lab: File &gt; New Project &gt; New Directory &gt; New Project. This will automatically set your working directory to the new project folder. Thats where R will look for files and save anything you export. Note: R does not distinguish between single quotes and double quotes, they are treated as synonyms, but the opening and closing symbols need to match. The dataset ralu.loci is available in the package LandGenCourse and can be loaded with data(ralu.loci). data(ralu.loci) Below, however, well import it from a comma separated file (.csv), as this is the recommended format for importing your own data (e.g. from Excel). b. Excel file First we need to copy the file ralu.loci.csv to the downloads folder inside your project folder. The first line checks that the folder downloads exists and creates it if needed. if(!dir.exists(paste0(here(),&quot;/downloads&quot;))) dir.create(paste0(here(),&quot;/downloads&quot;)) file.copy(system.file(&quot;extdata&quot;, &quot;ralu.loci.csv&quot;, package = &quot;LandGenCourse&quot;), paste0(here(), &quot;/downloads/ralu.loci.csv&quot;), overwrite=FALSE) ## [1] FALSE Check that the file ralu.loci.csv is now in the folder downloads within your project folder. From your file system, you can open it with Excel. Note that in Excel, the columns with the loci must be explicitly defined as text, otherwise Excel is likely to interpret them as times. Excel instructions: Opening an existing .csv file with genetic data: Excel menu: File &gt; New Workbook. Excel menu: File &gt; Import &gt; CSV File &gt; Import. Navigate to file. Text Import Wizard: select Delimited, click Next. Delimiters: check comma. Click Next. Use the Shift key to select all columns with loci A - H. Select text. Click Finish &gt; OK. Creating a new Excel file to enter genetic data: Label the loci columns. Select all loci columns by clicking on the column headers (hold the Shift key to select multiple adjacent columns). Define the data format: Format &gt; Cell &gt; Text. Enter genetic data, separating the alleles by a colon (e.g., 1:3). Saving Excel file for import in R: File Format: Comma Separated Values (.csv) Here is a screenshot of the first few lines of the Excel file with the genetic data: knitr::include_graphics(system.file(&quot;extdata&quot;, &quot;ExcelTable.png&quot;, package = &quot;LandGenCourse&quot;)) c. Import with function read.csv As the file is saved in csv format, we use the function read.csv to import it into an R object called Frogs. Display the first few rows and columns to check whether the data have been imported correctly: Frogs &lt;- read.csv(paste0(here(), &quot;/downloads/ralu.loci.csv&quot;), header=TRUE) as_tibble(Frogs) ## # A tibble: 181 x 10 ## SiteName Pop A B C D E F G H ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AirplaneLake Airplane 1:1 1:1 1:1 1:1 1:2 1:1 1:1 4:5 ## 2 AirplaneLake Airplane 2:2 1:1 NA:NA 1:1 1:1 NA:NA 2:2 NA:NA ## 3 AirplaneLake Airplane 1:1 1:1 1:1 1:1 3:3 1:1 1:1 3:3 ## 4 AirplaneLake Airplane 1:1 1:1 NA:NA 2:2 1:2 NA:NA NA:NA NA:NA ## 5 AirplaneLake Airplane 1:2 1:3 1:1 1:1 1:2 1:1 1:2 4:5 ## 6 AirplaneLake Airplane 1:2 1:1 1:1 3:1 1:1 1:1 1:2 4:5 ## 7 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 8 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 9 AirplaneLake Airplane 3:1 1:1 1:1 1:1 1:7 1:1 1:1 3:5 ## 10 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:7 1:1 1:1 3:3 ## # ... with 171 more rows The data frame Frogs contains 181 rows and 11 columns. Only the first few are shown. The file has a header row with column names. Notice the abbreviations for the R data type under each variable name: &lt;fctr&gt; means that the variable has been coded as a factor. Each row is one individual. Note that for this data set, there is no column with ID or names of individuals. The first two columns indicates the site in long (SiteName) and short (Pop) format. There are 8 columns with loci, named A - H. Each locus has two alleles, coded with numbers and separated by a colon (:). This coding suggests that the markers are codominant and that the species is diploid. The code for missing values is NA, (which shows up as NA:NA for missing genetic data). There seem to be quite a few missing values. d. Create ID variable Well add unique IDs for the frogs (to show how these would be imported). Here we take, for each frog, a substring of Pop containing the first three letters, add the row number, and separate them by a period. Frogs &lt;- data.frame(FrogID = paste(substr(Frogs$Pop, 1, 3), row.names(Frogs), sep=&quot;.&quot;), Frogs) as_tibble(Frogs) ## # A tibble: 181 x 11 ## FrogID SiteName Pop A B C D E F G H ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Air.1 AirplaneLake Airplane 1:1 1:1 1:1 1:1 1:2 1:1 1:1 4:5 ## 2 Air.2 AirplaneLake Airplane 2:2 1:1 NA:NA 1:1 1:1 NA:NA 2:2 NA:NA ## 3 Air.3 AirplaneLake Airplane 1:1 1:1 1:1 1:1 3:3 1:1 1:1 3:3 ## 4 Air.4 AirplaneLake Airplane 1:1 1:1 NA:NA 2:2 1:2 NA:NA NA:NA NA:NA ## 5 Air.5 AirplaneLake Airplane 1:2 1:3 1:1 1:1 1:2 1:1 1:2 4:5 ## 6 Air.6 AirplaneLake Airplane 1:2 1:1 1:1 3:1 1:1 1:1 1:2 4:5 ## 7 Air.7 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 8 Air.8 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 9 Air.9 AirplaneLake Airplane 3:1 1:1 1:1 1:1 1:7 1:1 1:1 3:5 ## 10 Air.10 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:7 1:1 1:1 3:3 ## # ... with 171 more rows e. Export with function write.csv If you want to export (and import) your own files, adapt this code. Note: when using R Notebooks, problems can arise if R looks for files in a different place (more on this in Week 8 Bonus material). This is a safe work-around that will help make your work reproducible and transferable to other computers. For this to work properly, you must work within an R project, and you must have writing permission for the project folder. The function here from the package here finds the path to your project folder. Remove the hashtags and run the two lines to see the path to your project folder, and the path to the (to be created) output folder. The function paste0 simply pastes strings of text together, without any separator (hence the 0 in the function name). #here() #paste0(here(),&quot;/output&quot;) The following line checks whether an output folder exists in the project folder, and if not, creates one. if(!dir.exists(paste0(here(),&quot;/output&quot;))) dir.create(paste0(here(),&quot;/output&quot;)) Now we can write our data into a csv file in the output folder: write.csv(ralu.loci, paste0(here(),&quot;/output/ralu.loci.csv&quot;), quote=FALSE, row.names=FALSE) To import the file again from the output folder, you could do this: ralu.loci.2 &lt;- read.csv(paste0(here(),&quot;/output/ralu.loci.csv&quot;), header=TRUE) 3. Create genind Object a. Read the helpfile We use the function df2genind of the adegenet package to import the loci into a genind object named Frogs.genind. Check the help file for a definition of all arguments and for example code. ?df2genind ## starting httpd help server ... done b. Set all arguments of function df2genind Some explanations: X: this is the data frame (or matrix) containing the loci only. Hence we need columns 4 - 11. sep: need to specify the separator between alleles (here: colon) ncode: not needed here because we used a separator, defined under sep. ind.names: here we should indicate the column FrogID. loc.names: by default this will use the column names of the loci. pop: here we should indicate either the column Pop or SiteName. NA.char: how were missing values coded? Here: NA. ploidy: this species is diploid, hence 2. type: marker type, either \"codom\" for codominant markers like microsatellites, or \"PA\" for presence-absence, such as SNP or AFLP markers. Note: you can use either \"\" or . strata: one way of defining hierarchical sampling levels. We will add this in Week 3. hierarchy: another way of defining hierarchical sampling levels. Well skip it for now. Frogs.genind &lt;- df2genind(X=Frogs[,c(4:11)], sep=&quot;:&quot;, ncode=NULL, ind.names= Frogs$FrogID, loc.names=NULL, pop=Frogs$Pop, NA.char=&quot;NA&quot;, ploidy=2, type=&quot;codom&quot;, strata=NULL, hierarchy=NULL) c. Check imported data Printing the genind objects name lists the number of individuals, loci and alleles, and lists all slots (prefaced by @). Frogs.genind ## /// GENIND OBJECT ///////// ## ## // 181 individuals; 8 loci; 39 alleles; size: 55.2 Kb ## ## // Basic content ## @tab: 181 x 39 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 3-9) ## @loc.fac: locus factor for the 39 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: df2genind(X = Frogs[, c(4:11)], sep = &quot;:&quot;, ncode = NULL, ind.names = Frogs$FrogID, ## loc.names = NULL, pop = Frogs$Pop, NA.char = &quot;NA&quot;, ploidy = 2, ## type = &quot;codom&quot;, strata = NULL, hierarchy = NULL) ## ## // Optional content ## @pop: population of each individual (group size range: 7-23) d. Summarize genind object There is a summary function for genind objects. Notes: Group here refers to the populations, i.e., Group sizes means number of individuals per population. It is a good idea to check the percentage of missing values here. If it is 0%, this may indicate that the coding for missing values was not recognized. If the % missing is very high, on the other hand, there may be some other importing problem. summary(Frogs.genind) ## ## // Number of individuals: 181 ## // Group sizes: 21 8 14 13 7 17 9 20 19 13 17 23 ## // Number of alleles per locus: 3 4 4 4 9 3 4 8 ## // Number of alleles per group: 21 21 20 22 20 19 19 25 18 14 18 26 ## // Percentage of missing data: 10.64 % ## // Observed heterozygosity: 0.1 0.4 0.09 0.36 0.68 0.02 0.38 0.68 ## // Expected heterozygosity: 0.17 0.47 0.14 0.59 0.78 0.02 0.48 0.74 4. View information stored in genind object The summary lists each attribute or slot of the genind object and summarizes its content. Technically speaking, genind objects are S4 objects in R, which means that their slots are accessed with the @ sign, rather than the $ sign for attributes of the more commonly used S3 objects. Interestingly, the object does not store the data table that was imported, but converts it to a table of allele counts. a. Slot @tab Table with allele counts, where each column is an allele. Allele name A.1 means locus A, allele 1. For a codominant marker (e.g. microsatellite) and a diploid species, allele counts per individual can be 0, 1, 2, or NA=missing. Here are the first few lines: as_tibble(Frogs.genind@tab) ## # A tibble: 181 x 39 ## A.1 A.2 A.3 B.1 B.3 B.2 B.4 C.1 C.2 C.4 C.5 D.1 D.2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 0 0 2 0 0 0 2 0 0 0 2 0 ## 2 0 2 0 2 0 0 0 NA NA NA NA 2 0 ## 3 2 0 0 2 0 0 0 2 0 0 0 2 0 ## 4 2 0 0 2 0 0 0 NA NA NA NA 0 2 ## 5 1 1 0 1 1 0 0 2 0 0 0 2 0 ## 6 1 1 0 2 0 0 0 2 0 0 0 1 0 ## 7 0 2 0 1 1 0 0 2 0 0 0 2 0 ## 8 0 2 0 1 1 0 0 2 0 0 0 2 0 ## 9 1 0 1 2 0 0 0 2 0 0 0 2 0 ## 10 0 2 0 1 1 0 0 2 0 0 0 2 0 ## # ... with 171 more rows, and 26 more variables: D.3 &lt;int&gt;, D.4 &lt;int&gt;, ## # E.1 &lt;int&gt;, E.2 &lt;int&gt;, E.3 &lt;int&gt;, E.7 &lt;int&gt;, E.6 &lt;int&gt;, E.8 &lt;int&gt;, ## # E.5 &lt;int&gt;, E.4 &lt;int&gt;, E.10 &lt;int&gt;, F.1 &lt;int&gt;, F.2 &lt;int&gt;, F.3 &lt;int&gt;, ## # G.1 &lt;int&gt;, G.2 &lt;int&gt;, G.3 &lt;int&gt;, G.5 &lt;int&gt;, H.4 &lt;int&gt;, H.5 &lt;int&gt;, ## # H.3 &lt;int&gt;, H.2 &lt;int&gt;, H.1 &lt;int&gt;, H.6 &lt;int&gt;, H.8 &lt;int&gt;, H.7 &lt;int&gt; b. Slot @loc.n.all Number of alleles per locus, across all populations. Frogs.genind@loc.n.all ## A B C D E F G H ## 3 4 4 4 9 3 4 8 c. Slot @loc.fac This is a factor that indicates for each allele (column in @tab) which locus it belongs to. The levels of the factor correspond to the loci. Frogs.genind@loc.fac ## [1] A A A B B B B C C C C D D D D E E E E E E E E E F F F G G G G H H H H H H H ## [39] H ## Levels: A B C D E F G H d. Slot @all.names List of allele names, separately for each locus. Note: the alleles are treated as text (character), even if they are coded as numbers. They are sorted in order of occurrence in data set. Frogs.genind@all.names ## $A ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## ## $B ## [1] &quot;1&quot; &quot;3&quot; &quot;2&quot; &quot;4&quot; ## ## $C ## [1] &quot;1&quot; &quot;2&quot; &quot;4&quot; &quot;5&quot; ## ## $D ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## ## $E ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;7&quot; &quot;6&quot; &quot;8&quot; &quot;5&quot; &quot;4&quot; &quot;10&quot; ## ## $F ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## ## $G ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;5&quot; ## ## $H ## [1] &quot;4&quot; &quot;5&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;6&quot; &quot;8&quot; &quot;7&quot; e. Further slots The following slots are automatically filled with default values unless specified by user during import into genind object: Slot @ploidy: ploidy for each individual. Slot @type: codominant (microsatellites) or presence/absence (SNP, AFLP) The following slots may be empty unless specified during or after import into genind object: Slot @other: placeholder for non-genetic data, e.g. spatial coordinates of individuals. Slot @pop: vector indicating the population that each individual belongs to. Slot @strata: stratification of populations, e.g. within regions or treatments. Slot @hierarchy: optional formula defining hierarchical levels in strata. 5. Import data with gstudio package a. Function read_population While genind objects are used by many functions for analyzing genetic data, the gstudio package provides an interface for additional analysis. We use the function read_population to import the data. ?read_population b. Set arguments Some explanations: path: This is the path to the .csv file. If it is in the current working directory, then the filename is sufficient. type: here we have separated data, as the alleles at each locus are separated by a colon. locus.columns: this would be columns 3 - 10 in this case, because we are importing from the .csv file directly, which does not contain the FrogID variable. We can add that later. phased: default is fine here. sep: here this refers to the .csv file! The separater is a comma. header: yes we have column names. Working directory confusion: If you are working within an R project folder, then that folder is automatically set as your working directory. If you are working in an R session outside of an R project, then you need to actively set the working directory in the R Studio menu under: Session &gt; Set working directory &gt; Choose Directory. Frogs.gstudio &lt;- read_population(path=system.file(&quot;extdata&quot;, &quot;ralu.loci.csv&quot;, package = &quot;LandGenCourse&quot;), type=&quot;separated&quot;, locus.columns=c(3:10), phased=FALSE, sep=&quot;,&quot;, header=TRUE) c. Check imported data Check the column types: SiteName and Pop are now character vectors, and the loci columns A - H are now of class locus (it is indicated that locus is a S3 object type). str(Frogs.gstudio) ## &#39;data.frame&#39;: 181 obs. of 10 variables: ## $ SiteName: chr &quot;AirplaneLake&quot; &quot;AirplaneLake&quot; &quot;AirplaneLake&quot; &quot;AirplaneLake&quot; ... ## $ Pop : chr &quot;Airplane&quot; &quot;Airplane&quot; &quot;Airplane&quot; &quot;Airplane&quot; ... ## $ A : &#39;locus&#39; chr &quot;1:1&quot; &quot;2:2&quot; &quot;1:1&quot; &quot;1:1&quot; ... ## $ B : &#39;locus&#39; chr &quot;1:1&quot; &quot;1:1&quot; &quot;1:1&quot; &quot;1:1&quot; ... ## $ C : &#39;locus&#39; chr &quot;1:1&quot; &quot;&quot; &quot;1:1&quot; &quot;&quot; ... ## $ D : &#39;locus&#39; chr &quot;1:1&quot; &quot;1:1&quot; &quot;1:1&quot; &quot;2:2&quot; ... ## $ E : &#39;locus&#39; chr &quot;1:2&quot; &quot;1:1&quot; &quot;3:3&quot; &quot;1:2&quot; ... ## $ F : &#39;locus&#39; chr &quot;1:1&quot; &quot;&quot; &quot;1:1&quot; &quot;&quot; ... ## $ G : &#39;locus&#39; chr &quot;1:1&quot; &quot;2:2&quot; &quot;1:1&quot; &quot;&quot; ... ## $ H : &#39;locus&#39; chr &quot;4:5&quot; &quot;&quot; &quot;3:3&quot; &quot;&quot; ... d. Add FrogID Frogs.gstudio is a data frame, and we can manipulate like any other data frame. Here we will add the variable FrogID as the first column. Frogs.gstudio &lt;- data.frame(FrogID=Frogs$FrogID, Frogs.gstudio) head(Frogs.gstudio) ## FrogID SiteName Pop A B C D E F G H ## 1 Air.1 AirplaneLake Airplane 1:1 1:1 1:1 1:1 1:2 1:1 1:1 4:5 ## 2 Air.2 AirplaneLake Airplane 2:2 1:1 1:1 1:1 2:2 ## 3 Air.3 AirplaneLake Airplane 1:1 1:1 1:1 1:1 3:3 1:1 1:1 3:3 ## 4 Air.4 AirplaneLake Airplane 1:1 1:1 2:2 1:2 ## 5 Air.5 AirplaneLake Airplane 1:2 1:3 1:1 1:1 1:2 1:1 1:2 4:5 ## 6 Air.6 AirplaneLake Airplane 1:2 1:1 1:1 1:3 1:1 1:1 1:2 4:5 e. Convert to genind object We can use the dataframe with the locus objects from gstudio to import the data into a genind object. In this case, we set sep=\":\", because the locus object in gstudio stores the alleles at each locus separated by a colon, and NA.char=\"\", because gstudio stores missing values as empty cells. Frogs.genind2 &lt;- adegenet::df2genind(X=Frogs.gstudio[,c(4:11)], sep=&quot;:&quot;, ncode=NULL, ind.names=Frogs.gstudio$FrogID, loc.names=NULL, pop=Frogs.gstudio$Pop, NA.char=&quot;&quot;, ploidy=2, type=&quot;codom&quot;, strata=NULL, hierarchy=NULL) Frogs.genind2 ## /// GENIND OBJECT ///////// ## ## // 181 individuals; 8 loci; 39 alleles; size: 55.5 Kb ## ## // Basic content ## @tab: 181 x 39 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 3-9) ## @loc.fac: locus factor for the 39 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: adegenet::df2genind(X = Frogs.gstudio[, c(4:11)], sep = &quot;:&quot;, ## ncode = NULL, ind.names = Frogs.gstudio$FrogID, loc.names = NULL, ## pop = Frogs.gstudio$Pop, NA.char = &quot;&quot;, ploidy = 2, type = &quot;codom&quot;, ## strata = NULL, hierarchy = NULL) ## ## // Optional content ## @pop: population of each individual (group size range: 7-23) "],["r-exercise-week-1.html", "R exercise Week 1", " R exercise Week 1 R Notebook: Create a new R Notebook for each weekly exercise. Watch the course video Week 0: Intro to R Notebooks as needed. At the end, knit it to an html file and view it in your browser. Good news: if you can knit the file, the code can stand by itself (it does not depend on what dyou did in your R session before) and runs without errors. This is a good check. If there are error messages, check the R Markdown tab for the code line number and try to fix it. Task: Import the data set pulsatilla_genotypes, which well use in a later lab (Week 14), into gstudio and convert it to a genind object. Data: This file contains microsatellite data for adults and seeds of the herb Pulsatilla vulgaris sampled at seven sites. Reference: DiLeo et al. (2018), Journal of Ecology 106:2242-2255. https://besjournals.onlinelibrary.wiley.com/doi/10.1111/1365-2745.12992 The following code copies the file into the dowloads folder in your R project folder: file.copy(system.file(&quot;extdata&quot;, &quot;pulsatilla_genotypes.csv&quot;, package = &quot;LandGenCourse&quot;), paste0(here(), &quot;/downloads/pulsatilla_genotypes.csv&quot;), overwrite=FALSE) ## [1] FALSE Variables: ID: Family ID (i.e., mother and her offspring have the same ID) OffID: 0 for adults, seeds from the same mom are numbered 1, 2, etc. Population: site ID Coordinates: X and Y coordinates (Projection info: EPSG Projection 31468) Loci: seven diploid microsatellites, each with two columns (1 allele per column) Hints: Load packages: Make sure the following packages are loaded: gstudio, here,tibble and adegenet. View data file: Adapt the code from section 2.c to import the raw data set. The file has a header row. View it. How are the genetic data coded? Import data into gstudio: Adapt the code from section 5.b to import the genetic data with gstudio. The loci are in columns 6 - 19. What setting for type is appropriate for this data set? Check the help file for read_population. Check imported data: Use str or as_tibble to check the imported data. Does each variable have the correct data type? Note: there should be 7 variables of type locus. Check variable types: Create a bulleted list, like the one above, with the variables. For each variable, list their R data type (e.g., numeric, integer, character, logical, factor, locus). Check the cheatsheet R markdown language as needed. Convert to genind object: Modify the code from section 5.e to convert the data from gstudio to a genind object. Ignore the warning about duplicate labels. Print a summary of the genind object. Question: What is the range of the number of genotyped individuals per population in this dataset? "],["bonus-1a.html", "Bonus: Importing SNP data", " Bonus: Importing SNP data "],["Week2.html", "Week 2: Spatial Data ", " Week 2: Spatial Data "],["video-2.html", "View Course Video", " View Course Video Preview Slides Download "],["tutorial-2.html", "Interactive Tutorial 2", " Interactive Tutorial 2 List of R commands used Function Package %&gt;% dplyr group_by dplyr summarize dplyr n dplyr filter dplyr left_join dplyr ==, &gt;, &lt;= base != base sum base merge base names base slotNames methods coordinates sp proj4string sp bubble sp extract raster Instructions a) How to access tutorials: Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (Landscape_Genetics_R_Course) and tutorial (Weeks 1 - 8) b) How to complete tutorial: Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: MyName2) c) How to submit answers (participating institutions only): The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose yes, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use skip). If you used skip because you could not answer a question, please contact your instructor for advice. "],["WE-2.html", "Worked Example", " Worked Example 1. Overview of Worked Example This code builds on data and code from the GeNetIt package by Jeff Evans and Melanie Murphy. Landscape metrics will be calculated with the landscapemetrics package described in: Hesselbarth et al. (2019), Ecography 42: 1648-1657. a. Goals This worked example shows: How to import spatial coordinates and site attributes as spatially referenced data. How to plot raster data in R and overlay sampling locations. How to calculate landscape metrics. How to extract landscape data at sampling locations and within a buffer around them. Try modifying the code to import your own data! b. Data set This code uses landscape data and spatial coordinates from 30 locations where Colombia spotted frogs (Rana luteiventris) were sampled for the full data set analyzed by Funk et al. (2005) and Murphy et al. (2010). Please see the separate introduction to the data set. ralu.site: SpatialPointsDataFrame object with UTM coordinates (zone 11) in slot @coords and 17 site variables in slot @data for 31 sites. The data are included in the GeNetIt package, for meta data type: ?ralu.site We will extract values at sampling point locations and within a local neighborhood (buffer) from six raster maps (see Murphy et al. 2010 for definitions), which are included with the GeNetIt package as a SpatialPixelsDataFrame called rasters: cti: Compound Topographic Index (wetness) err27: Elevation Relief Ratio ffp: Frost Free Period gsp: Growing Season Precipitation hli: Heat Load Index nlcd: USGS Landcover (categorical map) c. Required R libraries Install some packages needed for this worked example. if(!requireNamespace(&quot;GeNetIt&quot;, quietly = TRUE)) remotes::install_github(&quot;jeffreyevans/GeNetIt&quot;) library(LandGenCourse) library(landscapemetrics) library(dplyr) library(sp) library(raster) library(GeNetIt) library(tibble) Package tmaptools not automatically installed with LandGenCourse: if(!require(tmaptools)) install.packages(&quot;tmaptools&quot;) 2. Import site data from .csv file a. Import data into SpatialPointsDataFrame The site data are already in a SpatialPointsDataFrame named ralu.site that comes with the package GeNetIt. Use data(ralu.site) to load it. This will create an object ralu.site. To demonstrate how to create a SpatialPointsDataFrame, we create a simple data frame Sites with the coordinates and site data. data(ralu.site) class(ralu.site) ## [1] &quot;SpatialPointsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; Sites &lt;- data.frame(ralu.site@coords, ralu.site@data) class(Sites) ## [1] &quot;data.frame&quot; head(Sites) ## coords.x1 coords.x2 SiteName Drainage Basin Substrate ## 1 688816.6 5003207 AirplaneLake ShipIslandCreek Sheepeater Silt ## 2 688494.4 4999093 BachelorMeadow WilsonCreek Skyhigh Silt ## 3 687938.4 5000223 BarkingFoxLake WaterfallCreek Terrace Silt ## 4 689732.8 5002522 BirdbillLake ClearCreek Birdbill Sand ## 5 690104.0 4999355 BobLake WilsonCreek Harbor Silt ## 6 688742.5 4997481 CacheLake WilsonCreek Skyhigh Silt ## NWI AREA_m2 PERI_m Depth_m TDS FISH ACB AUC ## 1 Lacustrine 62582.2 1142.8 21.64 2.5 1 0 0.411 ## 2 Riverine_Intermittent_Streambed 225.0 60.0 0.40 0.0 0 0 0.000 ## 3 Lacustrine 12000.0 435.0 5.00 13.8 1 0 0.300 ## 4 Lacustrine 12358.6 572.3 3.93 6.4 1 0 0.283 ## 5 Palustrine 4600.0 321.4 2.00 14.3 0 0 0.000 ## 6 Palustrine 2268.8 192.0 1.86 10.9 0 0 0.000 ## AUCV AUCC AUF AWOOD AUFV ## 1 0 0.411 0.063 0.063 0.464 ## 2 0 0.000 1.000 0.000 0.000 ## 3 0 0.300 0.700 0.000 0.000 ## 4 0 0.283 0.717 0.000 0.000 ## 5 0 0.000 0.500 0.000 0.500 ## 6 0 0.000 0.556 0.093 0.352 Question: What are the variable names for the spatial coordinates? To illustrate importing spatial data from Excel, here we export the data as a csv file, import it again as a data frame, then convert it to a SpatialPointsDataFrame. First we create a folder output if it does not yet exist. Note: to run the code, remove all the hashtags # at the beginning of the lines to uncomment them. This part assumes that you have writing permission on your computer. Alternatively, try setting up your R project folder on an external drive where you have writing permission. The second line exports the data in Sites as a .csv file. The third line re-imports the .csv file to re-create data frame Sites. #require(here) #if(!dir.exists(paste0(here(),&quot;/output&quot;))) dir.create(paste0(here(),&quot;/output&quot;)) #write.csv(Sites, file=paste0(here(),&quot;/output/ralu.site.csv&quot;), # quote=FALSE, row.names=FALSE) #Sites &lt;- read.csv(paste0(here(),&quot;/output/ralu.site.csv&quot;), header=TRUE) The dataset Sites contains two columns with spatial coordinates and 17 attribute variables. So far, R treats the spatial coordinates like any other quantitative variables. To let R know this is spatial information, we import it into a spatial object type, a SpatialPointsDataFrame from the sp package. The conversion is done with the function coordinates, which takes a data frame and converts it to a spatial object of the same name. The code is not very intuitive. Note: the tilde symbol ~ (here before the first coordinate) is often used in R formulas, we will see it again later. It roughly translates to is modeled as a function of. Sites.sp &lt;- Sites coordinates(Sites.sp) &lt;- ~coords.x1+coords.x2 Now R knows these are spatial data and knows how to handle them. It does not treat the coordinates as variables anymore, hence the first column is now SiteName. b. Add spatial reference data Before we can combine the sampling locations with other spatial datasets, such as raster data, we need to tell R where on earth these locations are (georeferencing). This is done by specifying the Coordinate Reference System (CRS) or a proj4 string. For more information on CRS, see: https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf We know that these coordinates are UTM zone 11 (Northern hemisphere) coordinates, hence we can use a helper function to find the correct proj4string, using function get_proj4 from the tmaptools package. (For the Southern hemisphere, you would add s after the zone: utm11s). Here we call the function and the package simultaneously (this is good practice, as it helps keep track of where the functions in your code come from). proj4string(Sites.sp) &lt;- tmaptools::get_proj4(&quot;utm11&quot;)$proj4string ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj ## = prefer_proj): Discarded datum Unknown based on WGS84 ellipsoid in Proj4 ## definition Note: this function only declares the existing projection, it does not change the coordinates to that projection. If we had longitude and latitude coordinates, we would modify the command like this: proj4string(Sites.sp) &lt;- tmaptools::get_proj4(longlat)$proj4string. c. Change projection In case we needed to transform the projection from UTM zone 11 to longitude/latitude, we could create a new sp object Sites.sp.longlat. We use the function spTransform to change the projection from the projection of the old object Sites.sp to the longlat coordinate system, which we define by the argument CRSobj. Sites.sp.longlat &lt;- sp::spTransform(Sites.sp, CRSobj = tmaptools::get_proj4(&quot;longlat&quot;)$proj4string) head(Sites.sp.longlat@coords) ## coords.x1 coords.x2 ## 1 -114.5977 45.15708 ## 2 -114.6034 45.12016 ## 3 -114.6100 45.13047 ## 4 -114.5864 45.15067 ## 5 -114.5828 45.12208 ## 6 -114.6008 45.10560 Question: Where on earth is this? You can enter the coordinates from the longlat projection in Google maps. Note that Google expects the Latitude (Y coordinate) first, then the Longitude (X coordinate). Here, coords.x1 is the longitude (X) and coords.x2 is the latitude (Y). Thus, to locate the first site in Google maps, you will need to enter 45.15708, -114.5977. Where is it located? d. Access data in SpatialPointsDataFrame As an S4 object, Sites.sp has predefined slots. These can be accessed with the @ symbol: @data: the attribute data @coords: the spatial coordinates @coords.nrs: the column numbers of the input data from which the coordinates were taken (filled automatically) @bbox: bounding box, i.e., the minimum and maximum of x and y coordinates (filled automatically) @proj4string: the georeferencing information slotNames(Sites.sp) ## [1] &quot;data&quot; &quot;coords.nrs&quot; &quot;coords&quot; &quot;bbox&quot; &quot;proj4string&quot; Here are the first few lines of the coordinates: head(Sites.sp@coords) ## coords.x1 coords.x2 ## 1 688816.6 5003207 ## 2 688494.4 4999093 ## 3 687938.4 5000223 ## 4 689732.8 5002522 ## 5 690104.0 4999355 ## 6 688742.5 4997481 And the proj4 string: Lets compare this to the proj4string of the original ralu.site dataset. Sites.sp@proj4string ## CRS arguments: ## +proj=utm +zone=11 +ellps=WGS84 +units=m +no_defs The default for get_proj4(\"utm11\") results in a slightly different proj4string than the ralu.site dataset. The difference is in the datum argument (WGS84 vs. NAD83): ralu.site@proj4string ## CRS arguments: ## +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs +ellps=GRS80 ## +towgs84=0,0,0 Lets go with the original information and copy it: Sites.sp@proj4string &lt;- ralu.site@proj4string 3. Display raster data and overlay sampling locations, extract data a. Display raster data The raster data for this project are already available in the package GeNetIt, under the name rasters, and we can load them with data(rasters). They are stored as a SpatialPixelsDataFrame, another S4 object type from the sp package. data(rasters) class(rasters) ## [1] &quot;SpatialPixelsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; However, raster data are better analyzed with the package raster, which has an object type raster. Lets convert the data to a RasterStack of RasterLayer objects (i.e. a set of raster layers with the same spatial reference information). RasterMaps &lt;- stack(rasters) class(RasterMaps) ## [1] &quot;RasterStack&quot; ## attr(,&quot;package&quot;) ## [1] &quot;raster&quot; Printing the name of the raster stack displays a summary. A few explanations: dimensions: number of rows (nrow), number of columns (ncol), number of cells (ncell), number of layers (nlayers). So we see there are 6 layers in the raster stack. resolution: cell size is 30 m both in x and y directions (typical for Landsat-derived remote sensing data) coord.ref: projected in UTM zone 11, though the datum (NAD83) is different than what we used for the sampling locations. RasterMaps ## class : RasterStack ## dimensions : 426, 358, 152508, 6 (nrow, ncol, ncell, nlayers) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## names : cti, err27, ffp, gsp, hli, nlcd ## min values : 8.429851e-01, 3.906551e-02, 0.000000e+00, 2.270000e+02, 1.014000e+03, 1.100000e+01 ## max values : 23.7147598, 0.7637643, 51.0000000, 338.0696716, 9263.0000000, 95.0000000 Now we can use plot, which knows what to do with a raster stack. Note: layer nlcd is a categorical map of land cover types. See this weeks bonus materials for how to better display a categorical map in R. plot(RasterMaps) Some layers seem to show a similar pattern. It is easy to calculate the correlation between quantitative raster layers. Here, the last layer ncld, is in fact categorical (land cover type), and its correlation here is meaningless. layerStats(RasterMaps, &#39;pearson&#39;, na.rm=T) ## $`pearson correlation coefficient` ## cti err27 ffp gsp hli nlcd ## cti 1.0000000 -0.25442672 0.12264734 -0.14029572 -0.30501483 -0.1807878 ## err27 -0.2544267 1.00000000 -0.23467075 0.21403415 0.07724426 0.1256296 ## ffp 0.1226473 -0.23467075 1.00000000 -0.95144256 -0.07567975 -0.3297561 ## gsp -0.1402957 0.21403415 -0.95144256 1.00000000 0.09520075 0.3765363 ## hli -0.3050148 0.07724426 -0.07567975 0.09520075 1.00000000 0.2465540 ## nlcd -0.1807878 0.12562961 -0.32975610 0.37653635 0.24655404 1.0000000 ## ## $mean ## cti err27 ffp gsp hli nlcd ## 5.3386441 0.4509513 11.2037444 277.2211529 1938.3644530 50.8191308 b. Change color ramp, add sampling locations We can specify a color ramp by setting the col argument. The default is terrain.colors(255). Here we change it to rainbow(9), a rainbow colorpalette with 9 color levels. Note: To learn about options for the plot function for raster objects, access the help file by typing ?plot and select Plot a Raster object. We can add the sampling locations (if we plot only a single raster layer). Here we use rev to reverse the color ramp for plotting raster layer ffp, and add the sites as white circles with black outlines. par(mar=c(3,3,1,2)) plot(raster(RasterMaps, layer=&quot;ffp&quot;), col=rev(rainbow(9))) points(Sites.sp, pch=21, col=&quot;black&quot;, bg=&quot;white&quot;) Question: Recall that ffp stands for frost free period (in days). What do you think is the average length of the frost free period at theses sampling sites? c. Extract raster values at sampling locations The following code adds six variables to the data slot of Sites.sp. Technically we combine the columns of the existing data frame Sites.sp with the new columns in a new data frame with the same name. R notices the difference in projection (CRS) between the sampling point data and the rasters and takes care of it, providing just a warning. Sites.sp@data &lt;- data.frame(Sites.sp@data, extract(RasterMaps, Sites.sp)) Lets calcualate the mean length of the frost free period for these sites: mean(Sites.sp@data$ffp) ## [1] 8.0963 What land cover type is assigned to the most sampling units? Lets tabulate them. table(Sites.sp@data$nlcd) ## ## 11 12 42 52 71 90 ## 3 1 21 1 4 1 Note: land cover types are coded by numbers. Check here what the numbers mean: https://www.mrlc.gov/data/legends/national-land-cover-database-2016-nlcd2016-legend Question: A total of 21 sites are classified as 42. What is this most frequent land cover type? 4. Calculate landscape metrics We are going to use the package landscapemetrics. It is an R package to calculate landscape metrics in a tidy workflow (for more information about tidy data see here). landscapemetrics is basically a reimplementation of FRAGSTATS, which allows an integration into larger workflows within the R environment. The core of the package are functions to calculate landscape metrics, but also several auxiliary functions exit. To facilitate an integration into larger workflows, landscapemetrics is based on the raster package. To check if a raster is suitable for landscapemetrics, run the check_landscape() function first. The function checks the coordinate reference system (and mainly if units are in meters) and if the raster values are discrete classes. If the check fails, the calculation of metrics is still possible, however, especially metrics that are based on area and distances must be used with caution. nlcd &lt;- raster(RasterMaps, layer = &quot;nlcd&quot;) landscapemetrics::check_landscape(nlcd) ## layer crs units class n_classes OK ## 1 1 projected m integer 8 v There are three different levels of landscape metrics. Firstly, metrics can be calculated for each single patch (a patch is defined as neighbouring cells of the same class). Secondly, metrics can be calculated for a certain class (i.e. all patches belonging to the same class) and lastly for the whole landscape. All these levels are implemented and easily accessible in landscapemetrics. All functions to calculate metrics start with lsm_ (for landscapemetrics). The second part of the name specifies the level (patch - p, class - c or landscape - l). Lastly, the final part of the function name is the abbreviation of the corresponding metric (e.g. enn for the Euclidean nearest-neighbor distance). To list all available metrics, you can use the list_lsm() function. The function also allows to show metrics filtered by level, type or metric name. For more information about the metrics, please see either the corresponding helpfile(s) or https://r-spatialecology.github.io/landscapemetrics. landscapemetrics::list_lsm(level = &quot;landscape&quot;, type = &quot;diversity metric&quot;) ## # A tibble: 9 x 5 ## metric name type level function_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 msidi modified simpson&#39;s diversity in~ diversity metr~ landsca~ lsm_l_msidi ## 2 msiei modified simpson&#39;s evenness ind~ diversity metr~ landsca~ lsm_l_msiei ## 3 pr patch richness diversity metr~ landsca~ lsm_l_pr ## 4 prd patch richness density diversity metr~ landsca~ lsm_l_prd ## 5 rpr relative patch richness diversity metr~ landsca~ lsm_l_rpr ## 6 shdi shannon&#39;s diversity index diversity metr~ landsca~ lsm_l_shdi ## 7 shei shannon&#39;s evenness index diversity metr~ landsca~ lsm_l_shei ## 8 sidi simpson&#39;s diversity index diversity metr~ landsca~ lsm_l_sidi ## 9 siei simspon&#39;s evenness index diversity metr~ landsca~ lsm_l_siei landscapemetrics::list_lsm(metric = &quot;area&quot;) ## # A tibble: 7 x 5 ## metric name type level function_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 area patch area area and edge metric patch lsm_p_area ## 2 area_cv patch area area and edge metric class lsm_c_area_cv ## 3 area_mn patch area area and edge metric class lsm_c_area_mn ## 4 area_sd patch area area and edge metric class lsm_c_area_sd ## 5 area_cv patch area area and edge metric landscape lsm_l_area_cv ## 6 area_mn patch area area and edge metric landscape lsm_l_area_mn ## 7 area_sd patch area area and edge metric landscape lsm_l_area_sd landscapemetrics::list_lsm(level = c(&quot;class&quot;, &quot;landscape&quot;), type = &quot;aggregation metric&quot;, simplify = TRUE) ## [1] &quot;lsm_c_ai&quot; &quot;lsm_c_clumpy&quot; &quot;lsm_c_cohesion&quot; &quot;lsm_c_division&quot; ## [5] &quot;lsm_c_enn_cv&quot; &quot;lsm_c_enn_mn&quot; &quot;lsm_c_enn_sd&quot; &quot;lsm_c_iji&quot; ## [9] &quot;lsm_c_lsi&quot; &quot;lsm_c_mesh&quot; &quot;lsm_c_nlsi&quot; &quot;lsm_c_np&quot; ## [13] &quot;lsm_c_pd&quot; &quot;lsm_c_pladj&quot; &quot;lsm_c_split&quot; &quot;lsm_l_ai&quot; ## [17] &quot;lsm_l_cohesion&quot; &quot;lsm_l_contag&quot; &quot;lsm_l_division&quot; &quot;lsm_l_enn_cv&quot; ## [21] &quot;lsm_l_enn_mn&quot; &quot;lsm_l_enn_sd&quot; &quot;lsm_l_iji&quot; &quot;lsm_l_lsi&quot; ## [25] &quot;lsm_l_mesh&quot; &quot;lsm_l_np&quot; &quot;lsm_l_pd&quot; &quot;lsm_l_pladj&quot; ## [29] &quot;lsm_l_split&quot; a. Calculate patch-, class- and landscape level landscape metrics Note: This section explains different ways of calculating a selection of landscape metrics from a raster map with landscapemetrics. If this seems too technical for a first go, you may jump to section 4b. To calculate a single metric, just use the corresponding function. The result of all landscape metric functions is always an identically structured tibble (i.e. an advanced data.frame). The first coloumn is the layer id (only interesting for e.g. a RasterStack). The second coloumn specifies the level (patch, class or landscape). The third coloumn is the class ID (NA on landscape level) and the fourth coloumn is the patch ID (NA on class- and landscape level). Lastly, The fith coloumn is the abbreviation of the metric and finally the corresponding value in the last coloumn. ## c.lculate percentage of landscape of class percentage_class &lt;- lsm_c_pland(landscape = nlcd) percentage_class ## # A tibble: 8 x 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 class 11 NA pland 0.948 ## 2 1 class 12 NA pland 0.441 ## 3 1 class 31 NA pland 0.394 ## 4 1 class 42 NA pland 59.1 ## 5 1 class 52 NA pland 11.0 ## 6 1 class 71 NA pland 28.1 ## 7 1 class 90 NA pland 0.0557 ## 8 1 class 95 NA pland 0.0210 Questions: What percentage of the overall landscape (total map) is evergreen forest (class 42)? What percentage of the landscape is classified as wetlands (classes 90 and 95)? Because the resulting tibble is type stable, you can easily row-bind (rbind) different metrics (even of different levels): metrics &lt;- rbind( landscapemetrics::lsm_c_pladj(nlcd), landscapemetrics::lsm_l_pr(nlcd), landscapemetrics::lsm_l_shdi(nlcd) ) metrics ## # A tibble: 10 x 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 class 11 NA pladj 77.9 ## 2 1 class 12 NA pladj 52.5 ## 3 1 class 31 NA pladj 48.8 ## 4 1 class 42 NA pladj 89.9 ## 5 1 class 52 NA pladj 58.6 ## 6 1 class 71 NA pladj 81.6 ## 7 1 class 90 NA pladj 41.8 ## 8 1 class 95 NA pladj 32.8 ## 9 1 landscape NA NA pr 8 ## 10 1 landscape NA NA shdi 1.01 To calculate a larger set of landscape metrics, you can just use the wrapper calculate_lsm(). The arguments are similar to list_lsm(), e.g. you can specify the level or the type of metrics to calculate. Alternatively, you can also provide a vector with the function names of metrics to calculate to the what argument. However, watch out, for large rasters and many metrics, this can be rather slow (set progress = TRUE to get an progress report on the console). Also, we suggest to not just calculate all available metrics, but rather think about which ones might be actually suitable for your research question. Calculate all patch-level metrics using wrapper: nlcd_patch &lt;- landscapemetrics::calculate_lsm(landscape = nlcd, level = &quot;patch&quot;) nlcd_patch ## # A tibble: 19,776 x 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 patch 11 1 area 0.45 ## 2 1 patch 11 2 area 0.45 ## 3 1 patch 11 3 area 41.7 ## 4 1 patch 11 4 area 0.72 ## 5 1 patch 11 5 area 6.12 ## 6 1 patch 11 6 area 0.9 ## 7 1 patch 11 7 area 0.9 ## 8 1 patch 11 8 area 1.89 ## 9 1 patch 11 9 area 0.09 ## 10 1 patch 11 10 area 1.17 ## # ... with 19,766 more rows Show abbreviation of all calculated metrics: unique(nlcd_patch$metric) ## [1] &quot;area&quot; &quot;cai&quot; &quot;circle&quot; &quot;contig&quot; &quot;core&quot; &quot;enn&quot; &quot;frac&quot; &quot;gyrate&quot; ## [9] &quot;ncore&quot; &quot;para&quot; &quot;perim&quot; &quot;shape&quot; Calculate all aggregation metrics on landscape level: nlcd_landscape_aggr &lt;- landscapemetrics::calculate_lsm(landscape = nlcd, level = &quot;landscape&quot;, type = &quot;aggregation metric&quot;) nlcd_landscape_aggr ## # A tibble: 14 x 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 landscape NA NA ai 84.1 ## 2 1 landscape NA NA cohesion 99.2 ## 3 1 landscape NA NA contag 62.7 ## 4 1 landscape NA NA division 0.804 ## 5 1 landscape NA NA enn_cv 218. ## 6 1 landscape NA NA enn_mn 129. ## 7 1 landscape NA NA enn_sd 281. ## 8 1 landscape NA NA iji 43.0 ## 9 1 landscape NA NA lsi 32.3 ## 10 1 landscape NA NA mesh 2694. ## 11 1 landscape NA NA np 1648 ## 12 1 landscape NA NA pd 12.0 ## 13 1 landscape NA NA pladj 83.7 ## 14 1 landscape NA NA split 5.09 Calculate specific metrics: nlcd_subset &lt;- landscapemetrics::calculate_lsm(landscape = nlcd, what = c(&quot;lsm_c_pladj&quot;, &quot;lsm_l_pr&quot;, &quot;lsm_l_shdi&quot;)) nlcd_subset ## # A tibble: 10 x 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 class 11 NA pladj 77.9 ## 2 1 class 12 NA pladj 52.5 ## 3 1 class 31 NA pladj 48.8 ## 4 1 class 42 NA pladj 89.9 ## 5 1 class 52 NA pladj 58.6 ## 6 1 class 71 NA pladj 81.6 ## 7 1 class 90 NA pladj 41.8 ## 8 1 class 95 NA pladj 32.8 ## 9 1 landscape NA NA pr 8 ## 10 1 landscape NA NA shdi 1.01 The resulting tibble is easy to integrate into a workflow. For example, to get the ordered patch IDs of the 5% largest patches, the following code could be used. The pipe operator %&gt;% from the dplyr package passes the resulting object automatically to the next function as first argument. Note: the last step (pulling the id variable only) could be done by adding this to the pipe: %&gt;% dplyr::pull(id). Due to some package inconsistencies, this sometimes created an error. Here we extract the id variable in a separate step as a work-around. id_largest &lt;- nlcd_patch %&gt;% # previously calculated patch metrics dplyr::filter(metric == &quot;area&quot;) %&gt;% # only patch area dplyr::arrange(-value) %&gt;% # order by decreasing size dplyr::filter(value &gt; quantile(value, probs = 0.95)) ## g.t only patches larger than 95% quantile id_largest &lt;- id_largest$id ## g.t only patch id id_largest ## [1] 206 166 1265 1459 1549 1558 1421 427 1434 1385 226 3 386 1205 589 ## [16] 1441 426 1059 1206 324 433 1195 315 1225 712 1266 1377 389 753 1528 ## [31] 894 1510 286 563 240 1411 478 1364 435 1336 812 640 1376 1523 786 ## [46] 467 485 1559 1244 284 1537 1574 718 814 499 864 955 1015 1430 443 ## [61] 393 930 1250 548 851 885 1481 928 1554 43 164 281 1608 716 514 ## [76] 802 1614 729 977 1488 37 357 1353 Because the metric names are only abbreviated, there is also a way to include the full name in the results. For the wrapper, just set full_name = TRUE. For the rowbinded tibble, you can use the provided tibble called lsm_abbreviations_names that comes with the package and use e.g. dplyr::left_join() to combine it with your results. Add full metrics name to result: nlcd_subset_full_a &lt;- landscapemetrics::calculate_lsm(nlcd, what = c(&quot;lsm_c_pladj&quot;, &quot;lsm_l_pr&quot;, &quot;lsm_l_shdi&quot;), full_name = TRUE) nlcd_subset_full_a ## # A tibble: 10 x 9 ## layer level class id metric value name type function_name ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 class 11 NA pladj 77.9 percentage of~ aggregat~ lsm_c_pladj ## 2 1 class 12 NA pladj 52.5 percentage of~ aggregat~ lsm_c_pladj ## 3 1 class 31 NA pladj 48.8 percentage of~ aggregat~ lsm_c_pladj ## 4 1 class 42 NA pladj 89.9 percentage of~ aggregat~ lsm_c_pladj ## 5 1 class 52 NA pladj 58.6 percentage of~ aggregat~ lsm_c_pladj ## 6 1 class 71 NA pladj 81.6 percentage of~ aggregat~ lsm_c_pladj ## 7 1 class 90 NA pladj 41.8 percentage of~ aggregat~ lsm_c_pladj ## 8 1 class 95 NA pladj 32.8 percentage of~ aggregat~ lsm_c_pladj ## 9 1 landsc~ NA NA pr 8 patch richness diversit~ lsm_l_pr ## 10 1 landsc~ NA NA shdi 1.01 shannon&#39;s div~ diversit~ lsm_l_shdi Add full metrics name to results calculated previously using left_join(): nlcd_subset_full_b &lt;- dplyr::left_join(x = nlcd_subset, y = lsm_abbreviations_names, by = c(&quot;metric&quot;, &quot;level&quot;)) nlcd_subset_full_b ## # A tibble: 10 x 9 ## layer level class id metric value name type function_name ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 class 11 NA pladj 77.9 percentage of~ aggregat~ lsm_c_pladj ## 2 1 class 12 NA pladj 52.5 percentage of~ aggregat~ lsm_c_pladj ## 3 1 class 31 NA pladj 48.8 percentage of~ aggregat~ lsm_c_pladj ## 4 1 class 42 NA pladj 89.9 percentage of~ aggregat~ lsm_c_pladj ## 5 1 class 52 NA pladj 58.6 percentage of~ aggregat~ lsm_c_pladj ## 6 1 class 71 NA pladj 81.6 percentage of~ aggregat~ lsm_c_pladj ## 7 1 class 90 NA pladj 41.8 percentage of~ aggregat~ lsm_c_pladj ## 8 1 class 95 NA pladj 32.8 percentage of~ aggregat~ lsm_c_pladj ## 9 1 landsc~ NA NA pr 8 patch richness diversit~ lsm_l_pr ## 10 1 landsc~ NA NA shdi 1.01 shannon&#39;s div~ diversit~ lsm_l_shdi b. Calculate patch-level landscape metrics for Evergreen Forest To only get the results for class 42 (evergreen forest), you can just dplyr::filter() the tibble (or use any other subset method you prefer). forest_patch_metrics &lt;- dplyr::filter(nlcd_patch, class == 42) All functions make heavy use of connected components labeling to delineate patches (neighbouring cells of the same class). To get all patches of every class you can just use get_patches(). To get only a certain class, just specify the class argument and the neighbourhood rule can be chosen between 8-neighbour rule or 4-neighbour rule with the argument directions. ## c.nnected components labeling of landscape cc_nlcd &lt;- landscapemetrics::get_patches(nlcd, directions = 8) # show name of each class sapply(cc_nlcd, function(x) names(x)) ## 11 12 31 42 52 71 90 95 ## &quot;layer&quot; &quot;layer&quot; &quot;layer&quot; &quot;layer&quot; &quot;layer&quot; &quot;layer&quot; &quot;layer&quot; &quot;layer&quot; # the fourth list entry is class forest cc_forest_a &lt;- cc_nlcd[4] cc_forest_b &lt;- landscapemetrics::get_patches(nlcd, class = 42) # watch out: result is list with one entry cc_forest_a ## $`42` ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : layer ## values : 1, 222 (min, max) cc_forest_b ## $`42` ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : layer ## values : 1, 222 (min, max) To plot the patches you can use the show_patches() function. Here we show patches of class 42 (forest) and class 52 (shrubland): show_patches(landscape = nlcd, class = c(42, 52), labels = FALSE) It is also possible to visualize only the core area of each patch using show_cores(). The core area is defined as all cells that are further away from the edge of each patch than a specified edge depth (e.g. 5 cells). Here we show core area with edge depth = 5 for class 42; try edge_depth = 1 for comparison: show_cores(landscape = nlcd, class = c(42), edge_depth = 5, labels = FALSE) Note: this may create a warning no non-missing arguments to min; returning Inf for each patch that does not have any core area. Here we suppressed the warnings for the chunk with the chunk option warning=FALSE. Lastly, you can plot the map and fill each patch with the corresponding metric value, e.g. patch size, using show_lsm(). Notice that there are two very large patches in class 42: show_lsm(landscape = nlcd, class = c(42, 52), what = &quot;lsm_p_area&quot;, labels = FALSE) c. Extract forest patch size at samplig locations Lets add forest patch size to the Sites.sp data. To extract landscape metrics of the patch in which each sampling point is located, use extract_lsm(). Which metrics are extracted can be specified by the what argument (similar to calculate_lsm()). However, only patch-level metrics are available. Please be aware, that the resulting tibble now has a new column, namely the ID of the sampling point (in the same order as the input points). ## e.tract patch area of all classes: patch_size_sp &lt;- extract_lsm(landscape = nlcd, y = Sites.sp, what = &quot;lsm_p_area&quot;) ## Warning: Only using &#39;what&#39; argument. ## b.cause we are only interested in the forest patch size, we set all area of class != 42 to 0: patch_size_sp_forest &lt;- dplyr::mutate(patch_size_sp, value = dplyr::case_when(class == 42 ~ value, class != 42 ~ 0)) ## a.d data to sp object: Sites.sp@data$ForestPatchSize &lt;- patch_size_sp_forest$value Sites.sp@data$ForestPatchSize ## [1] 3179.88 0.00 3179.88 0.00 6.66 4539.33 4539.33 4539.33 0.00 ## [10] 4539.33 4539.33 23.76 23.76 0.00 0.00 4539.33 0.00 4539.33 ## [19] 3.60 3179.88 0.00 4539.33 3.87 4539.33 0.00 0.00 23.76 ## [28] 3179.88 0.00 4539.33 0.09 d. Plot a bubble map of forest patch size at sampling locations par(mar = c(3,3,1,2)) bubble(Sites.sp, &quot;ForestPatchSize&quot;, fill = FALSE, key.entries = as.numeric(names(table(Sites.sp@data$ForestPatchSize)))) ## Warning in wkt(obj): CRS object has no comment ## Warning in wkt(obj): CRS object has no comment 5. Sample landscape metrics within buffer around sampling locations The package landscapemetrics has a built-in function sample_lsm to sample metrics in a buffer around sampling locations, which are provided with argument y. You can choose the shape of the buffer window (either a circle or a square) and, with the argument what, which metrics to sample (similar to calculate_lsm()). The argument size specifies the buffer size in map units (e.g., meters): radius for circles, half of the side length for squares. Here, the value size = 500 results in a square window of 1000 m x 1000 m centered at the sampling location. nlcd_sampled &lt;- landscapemetrics::sample_lsm(landscape = nlcd, what = c(&quot;lsm_l_ta&quot;, &quot;lsm_c_np&quot;, &quot;lsm_c_pland&quot;, &quot;lsm_c_ai&quot;), shape = &quot;square&quot;, y = Sites.sp, size = 500) nlcd_sampled ## # A tibble: 493 x 8 ## layer level class id metric value plot_id percentage_inside ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 class 11 NA ai 88.6 1 98.0 ## 2 1 class 31 NA ai NA 1 98.0 ## 3 1 class 42 NA ai 93.3 1 98.0 ## 4 1 class 52 NA ai 50 1 98.0 ## 5 1 class 71 NA ai 65.9 1 98.0 ## 6 1 class 11 NA np 2 1 98.0 ## 7 1 class 31 NA np 1 1 98.0 ## 8 1 class 42 NA np 2 1 98.0 ## 9 1 class 52 NA np 4 1 98.0 ## 10 1 class 71 NA np 8 1 98.0 ## # ... with 483 more rows The tibble now contains two additional columns. Firstly, the plot_id (in the same order as the input points) and secondly, the percentage_inside, i.e. what percentage of the buffer around the sampling location lies within the map. (In cases where the sampling location is on the edge of the landscape, the buffer around the sampling location could be only partly within the map). The value can also deviate from 100 % because the sampling locations are not necessarily in the cell center and the actually clipped cells lead to a slightly smaller or larger buffer area. A circular buffer shape increases this effect. It is also possible to get the clippings of the buffer around sampling locations as a RasterLayer. For this, just set return_raster = TRUE. # sample some metrics within buffer around sample location and returning sample # plots as raster nlcd_sampled_plots &lt;- landscapemetrics::sample_lsm(landscape = nlcd, what = c(&quot;lsm_l_ta&quot;, &quot;lsm_c_np&quot;, &quot;lsm_c_pland&quot;, &quot;lsm_c_ai&quot;), shape = &quot;square&quot;, y = Sites.sp, size = 500, return_raster = TRUE) nlcd_sampled_plots ## # A tibble: 493 x 9 ## layer level class id metric value plot_id percentage_inside ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 class 11 NA ai 88.6 1 98.0 ## 2 1 class 31 NA ai NA 1 98.0 ## 3 1 class 42 NA ai 93.3 1 98.0 ## 4 1 class 52 NA ai 50 1 98.0 ## 5 1 class 71 NA ai 65.9 1 98.0 ## 6 1 class 11 NA np 2 1 98.0 ## 7 1 class 31 NA np 1 1 98.0 ## 8 1 class 42 NA np 2 1 98.0 ## 9 1 class 52 NA np 4 1 98.0 ## 10 1 class 71 NA np 8 1 98.0 ## # ... with 483 more rows, and 1 more variable: raster_sample_plots &lt;list&gt; The result will be a nested tibble containing the plot_id, the metrics and a RasterLayer with the clipped buffers (as a list). Attention: Because several metrics on class- and landscape-level the clipped buffers will be repeated several times. Here we show results for the first four sampling locations: unique_plots &lt;- unique(nlcd_sampled_plots$raster_sample_plots)[1:4] par(mfrow = c(2,2)) plot(unique_plots[[1]], main = paste(Sites.sp$SiteName[1]), col = rev(rainbow(9))) plot(unique_plots[[2]], main = paste(Sites.sp$SiteName[2]), col = rev(rainbow(9))) plot(unique_plots[[3]], main = paste(Sites.sp$SiteName[3]), col = rev(rainbow(9))) plot(unique_plots[[4]], main = paste(Sites.sp$SiteName[4]), col = rev(rainbow(9))) par(mfrow = c(1,1)) b. Extract landscape metric of choice for a single cover type (as vector) To extract a metrics you can just dplyr::filter() the resulting tibble and pull the value column. Here we filter the results for class == 42 (forest) and metric pland (percentage of landscape) and pull the results as a vector: percentage_forest_500_a &lt;- dplyr::pull(dplyr::filter(nlcd_sampled, class == 42, metric == &quot;pland&quot;), value) percentage_forest_500_a ## [1] 77.50230 39.57759 38.01653 31.40138 40.40404 83.30450 69.16221 86.80927 ## [9] 10.19284 72.81910 92.19467 34.04635 34.84848 27.09447 31.37255 56.23886 ## [17] 59.77961 58.73440 37.64922 60.12111 46.16756 32.32323 23.35640 50.44563 ## [25] 66.93405 30.57851 33.70064 26.53811 45.18717 38.05704 62.47772 As an alternative, heres the same workflow again, but using a pipe: percentage_forest_500_b &lt;- nlcd_sampled %&gt;% dplyr::filter(class == 42, metric == &quot;pland&quot;) %&gt;% dplyr::pull(value) percentage_forest_500_b ## [1] 77.50230 39.57759 38.01653 31.40138 40.40404 83.30450 69.16221 86.80927 ## [9] 10.19284 72.81910 92.19467 34.04635 34.84848 27.09447 31.37255 56.23886 ## [17] 59.77961 58.73440 37.64922 60.12111 46.16756 32.32323 23.35640 50.44563 ## [25] 66.93405 30.57851 33.70064 26.53811 45.18717 38.05704 62.47772 c. Extract landscape metric of choice for all cover types (as data frame). To extract the landscape metric prop.landscape for all cover types as a tibble, just filter dplyr::filter() the tibble again, but only use the metric as filter. ## f.lter for percentage of landscape percentage_forest_500_df &lt;- dplyr::filter(nlcd_sampled, metric == &quot;pland&quot;) percentage_forest_500_df ## # A tibble: 154 x 8 ## layer level class id metric value plot_id percentage_inside ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 class 11 NA pland 7.25 1 98.0 ## 2 1 class 31 NA pland 0.0918 1 98.0 ## 3 1 class 42 NA pland 77.5 1 98.0 ## 4 1 class 52 NA pland 1.93 1 98.0 ## 5 1 class 71 NA pland 13.2 1 98.0 ## 6 1 class 11 NA pland 3.49 2 98.0 ## 7 1 class 12 NA pland 1.01 2 98.0 ## 8 1 class 31 NA pland 0.735 2 98.0 ## 9 1 class 42 NA pland 39.6 2 98.0 ## 10 1 class 52 NA pland 5.33 2 98.0 ## # ... with 144 more rows The percent cover of all cover types should add up to ~ 100% (i.e., 1) for each site. We can check this with the function dplyr::summarize(). First, we need to group the data using the plot_id, then sum all percentages. ## g.oup by plot_id and sum all percentages pland_sum_a &lt;- dplyr::summarize(dplyr::group_by(percentage_forest_500_df, by = plot_id), sum_pland = sum(value)) pland_sum_a ## # A tibble: 31 x 2 ## by sum_pland ## &lt;int&gt; &lt;dbl&gt; ## 1 1 100 ## 2 2 100 ## 3 3 100 ## 4 4 100 ## 5 5 100 ## 6 6 100 ## 7 7 100 ## 8 8 100 ## 9 9 100 ## 10 10 100 ## # ... with 21 more rows Same workflow, but using a pipe: pland_sum_b &lt;- percentage_forest_500_df %&gt;% dplyr::group_by(plot_id) %&gt;% dplyr::summarize(sum_pland = sum(value)) pland_sum_b ## # A tibble: 31 x 2 ## plot_id sum_pland ## &lt;int&gt; &lt;dbl&gt; ## 1 1 100 ## 2 2 100 ## 3 3 100 ## 4 4 100 ## 5 5 100 ## 6 6 100 ## 7 7 100 ## 8 8 100 ## 9 9 100 ## 10 10 100 ## # ... with 21 more rows d. Extract all landscape metrics for a single cover type (as data frame) Filterdplyr::filter() for class == 42 and add the sites names as coloumn to the resulting tibble. ## f.lter for class == 42 (forest) forest_500_df &lt;- dplyr::filter(nlcd_sampled, class == 42) ## d.ta.frame with id and name of site SiteName_df &lt;- data.frame(id = 1:length(Sites.sp$SiteName), site_name = Sites.sp$SiteName) ## a.d site_name to metrics using plot_id and id of sampling sites forest_500_df &lt;- dplyr::left_join(forest_500_df, SiteName_df, by = c(&quot;plot_id&quot; = &quot;id&quot;)) forest_500_df ## # A tibble: 93 x 9 ## layer level class id metric value plot_id percentage_inside site_name ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 class 42 NA ai 93.3 1 98.0 AirplaneLake ## 2 1 class 42 NA np 2 1 98.0 AirplaneLake ## 3 1 class 42 NA pland 77.5 1 98.0 AirplaneLake ## 4 1 class 42 NA ai 89.4 2 98.0 BachelorMeadow ## 5 1 class 42 NA np 7 2 98.0 BachelorMeadow ## 6 1 class 42 NA pland 39.6 2 98.0 BachelorMeadow ## 7 1 class 42 NA ai 79.4 3 98.0 BarkingFoxLake ## 8 1 class 42 NA np 10 3 98.0 BarkingFoxLake ## 9 1 class 42 NA pland 38.0 3 98.0 BarkingFoxLake ## 10 1 class 42 NA ai 81.2 4 104. BirdbillLake ## # ... with 83 more rows Done! Note: check this weeks bonus material if you want to see how to use the new sf library for spatial data, and how to export the site data to a shapefile that you can import into a GIS. "],["r-exercise-week-2.html", "R Exercise Week 2", " R Exercise Week 2 Task: Create a bubble plot of the number of genotyped individuals in the dataset pulsatilla_genotypes.csv, using Latitude/Longitude coordinates. Hints: Load libraries: Load libraries gstudio, dplyr, tibble and sp. Import data: Re-use your code from Week 1 exercise to import the dataset pulsatilla_genotypes.csv into gstudio. Recall that the resulting object is a data.frame. Check the variables with function str. Which variables contain the sites and the spatial coordinates? Summarize by site: Use the function group_by from library dplyr to group individuals (rows) by site (using pipe notation: %&gt;%), and add the function summarize to count the number of genotyped individuals per population (i.e., sampling site). Recall that this can be done with nesting the function n within summarize: summarize(nIndiv = n()). Write the result into a new object Pulsatilla. Add mean coordinates: You can nest multiple functions within summarize and separate them with a comma. E.g., to calculate both sample size and the mean of a variable myVar, you could write: summarize(nIndiv = n(), myMean = n(myVar)) Modify your code to calculate number of genotyped individuals for each site and their mean X and Y coordinates. Your object Pulsatilla should now have three columns, one with the number of individuals and two with the mean coordinates. Display the dataset with as_tibble to check. Convert to spatial object: Modify code from section 2.a to convert your dataframe Pulsatilla to a SpatialPointsDataFrame object (package sp). Make sure to adjust the correct variable names for the coordinates (i.e., the variable names that you assigned in the previous step for the mean X and Y coordinates). Specify known projection: Specify the proj4string as follows: proj4string(Pulsatilla) &lt;- CRS(\"+init=epsg:31468\") Transform projection: Adapt code from section 2.c to transform the projection to the longlat coordinate system, and write it into an object Pulsatilla.longlat. Display the coords slot of Pulsatilla.longlat and check on Google maps where the first site is located. Create bubble plot: Adapt code from section 4.c to create a bubble plot of the number of individuals per population. Note: you may drop the argument key.entries as it has a default. Save data as R object: Save the object Pulsatilla.longlat as an R object using the following code: saveRDS(Pulsatilla.longlat, file = paste0(here::here(), \"/output/Pulsatilla.longlat.rds\")). We will need it for a later R exercise. Question: Where on earth are the sites in the Pulsatilla dataset located? "],["bonus-2a.html", "Bonus: Using package sf", " Bonus: Using package sf "],["bonus-2b.html", "Bonus: Visualize LULC map", " Bonus: Visualize LULC map "],["Week3.html", "Week 3: Genetic Diversity ", " Week 3: Genetic Diversity "],["video-3.html", "View Course Video", " View Course Video Video, Part 1 Video, Part 2 Preview Slides Download "],["tutorial-3.html", "Interactive Tutorial 3", " Interactive Tutorial 3 List of R commands used Function Package table base range base split base length base nrow, ncol base sapply base matrix base dimnames base Reduce base rbind, cbind base list base t base [ , ] base is.na base != base seppop adegenet propTyped adegenet apply base mean base Instructions a) How to access tutorials: Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (Landscape_Genetics_R_Course) and tutorial (Weeks 1 - 8) b) How to complete tutorial: Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: MyName2) c) How to submit answers (participating institutions only): The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose yes, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use skip). If you used skip because you could not answer a question, please contact your instructor for advice. "],["WE-3.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals This worked example shows how to: Check markers and populations (polymorphism, HWE, linkage, null alleles). Assess genetic diversity. Aggregate genetic data at the population level. b. Data set This is the same data set as used in Weeks 1 &amp; 2. Microsatellite data for 181 individuals of Colombia spotted frogs (Rana luteiventris) from 12 populations. Site-level spatial coordinates and attributes. The data are a subsample of the full data set analyzed in Funk et al. (2005) and Murphy et al. (2010). Please see the separate introduction to the data set. ralu.loci: Data frame with populations and genetic data (181 rows x 9 columns). Included in package LandGenCourse. To load it, type: data(ralu.loci) ralu.site: Spatial points data frame with spatial coordinates and site variables Included in package GeNetIt. To load it, type: data(ralu.site) c. Required R libraries All required packages should have been installed already when you installed LandGenCourse. #require(adegenet) require(LandGenCourse) #require(pegas) #require(sp) #require(PopGenReport) require(dplyr) require(poppr) 2. Basic checking of markers and populations Before we do landscape genetic analysis, we need to perform a basic population genetic analysis of the genetic data, in order to better understand the nature and quality of the data and to check for underlying assumptions of population genetic models and corresponding methods. a. Re-create genind object Adapted from Week 1 tutorial: Note: we use the double colon notation package::function(argument) to indicate, for each function, which package it belongs to (see Week 2 video). data(ralu.loci, package=&quot;LandGenCourse&quot;) Frogs &lt;- data.frame(FrogID = paste(substr(ralu.loci$Pop, 1, 3), row.names(ralu.loci), sep=&quot;.&quot;), ralu.loci) Frogs.genind &lt;- adegenet::df2genind(X=Frogs[,c(4:11)], sep=&quot;:&quot;, ncode=NULL, ind.names= Frogs$FrogID, loc.names=NULL, pop=Frogs$Pop, NA.char=&quot;NA&quot;, ploidy=2, type=&quot;codom&quot;, strata=NULL, hierarchy=NULL) Frogs.genind ## /// GENIND OBJECT ///////// ## ## // 181 individuals; 8 loci; 39 alleles; size: 55.5 Kb ## ## // Basic content ## @tab: 181 x 39 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 3-9) ## @loc.fac: locus factor for the 39 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: adegenet::df2genind(X = Frogs[, c(4:11)], sep = &quot;:&quot;, ncode = NULL, ## ind.names = Frogs$FrogID, loc.names = NULL, pop = Frogs$Pop, ## NA.char = &quot;NA&quot;, ploidy = 2, type = &quot;codom&quot;, strata = NULL, ## hierarchy = NULL) ## ## // Optional content ## @pop: population of each individual (group size range: 7-23) b. Check that markers are polymorphic The genetic resolution depends on the number of markers and their polymorphism. The table above and the summary function for genind objects together provide this information. Now we run the summary function: summary(Frogs.genind) ## ## // Number of individuals: 181 ## // Group sizes: 21 8 14 13 7 17 9 20 19 13 17 23 ## // Number of alleles per locus: 3 4 4 4 9 3 4 8 ## // Number of alleles per group: 21 21 20 22 20 19 19 25 18 14 18 26 ## // Percentage of missing data: 10.64 % ## // Observed heterozygosity: 0.1 0.4 0.09 0.36 0.68 0.02 0.38 0.68 ## // Expected heterozygosity: 0.17 0.47 0.14 0.59 0.78 0.02 0.48 0.74 The output of the summary function shows us the following: 8 loci with 3 - 9 alleles (39 in total) Expected heterozygosity varies between 0.14 (locus C) and 0.78 (locus E) Theres a reasonable level of missing values (10.6%) c. Check for deviations from Hardy-Weinberg equilibrium (HWE) See also: http://dyerlab.github.io/applied_population_genetics/hardy-weinberg-equilibrium.html For a very large population (no drift) with random mating and non-overlapping generations (plus a few more assumptions about the mating system), and in the absence of mutation, migration (gene flow) and selection, we can predict offspring genotype frequencies from allele frequencies of the parent generation (Hardy-Weinberg equilibrium). In general, we dont expect all of these assumptions to be met (e.g., if we want to study gene flow or selection, we kind of expect that these processes are present). Note: plants often show higher levels of departure from HWE than animals. Here are p-values for two alternative tests of deviation from HWE for each locus. Columns: chi^2: value of the classical chi-squared test statistic df: degrees of freedom of the chi-squared test Pr(chi^2 &gt;): p-value of the chi-squared test (&gt; indicates that the alternative is greater, which is always the case for a chi-squared test) Pr.exact: p-value from an exact test based on Monte Carlo permutation of alleles (for diploids only). The default is B = 1000 permutations (set B = 0 to skip this test). Here we use the function round with argument digits = 3 to round all values to 3 decimals. round(pegas::hw.test(Frogs.genind, B = 1000), digits = 3) ## chi^2 df Pr(chi^2 &gt;) Pr.exact ## A 40.462 3 0.000 0.000 ## B 17.135 6 0.009 0.020 ## C 136.522 6 0.000 0.000 ## D 83.338 6 0.000 0.000 ## E 226.803 36 0.000 0.000 ## F 0.024 3 0.999 1.000 ## G 12.349 6 0.055 0.005 ## H 76.813 28 0.000 0.000 Both tests suggest that all loci except for locus F are out of HWE globally (across all 181 individuals). Next, we check for HWE of each locus in each population. Notes on the code: The curly brackets { } below are used to keep the output from multiple lines together in the html file. Function seppop splits the genind object by population. We use sapply to apply the function hw.test from package pegas to each population (see this weeks video and tutorial). We set B=0 to specify that we dont need any permutations right now. The function t takes the transpose of the resulting matrix, which means it flips rows and columns. This works on a matrix, not a data frame, hence we use data.matrix to temporarily interpret the data frame as a matrix. # Chi-squared test: p-value HWE.test &lt;- data.frame(sapply(seppop(Frogs.genind), function(ls) pegas::hw.test(ls, B=0)[,3])) HWE.test.chisq &lt;- t(data.matrix(HWE.test)) {cat(&quot;Chi-squared test (p-values):&quot;, &quot;\\n&quot;) round(HWE.test.chisq,3)} ## Chi-squared test (p-values): ## A B C D E F G H ## Airplane 0.092 0.359 1.000 0.427 0.680 1.000 0.178 0.051 ## Bachelor 1.000 0.557 0.576 0.686 0.716 1.000 0.414 0.609 ## BarkingFox 0.890 0.136 0.005 0.533 0.739 0.890 0.708 0.157 ## Bob 0.764 0.864 0.362 0.764 0.033 1.000 0.860 0.287 ## Cache 1.000 0.325 0.046 0.659 0.753 1.000 0.709 0.402 ## Egg 1.000 0.812 1.000 1.000 0.156 1.000 0.477 0.470 ## Frog 1.000 0.719 0.070 0.722 0.587 1.000 0.564 0.172 ## GentianL 0.809 0.059 1.000 0.028 0.560 0.717 0.474 0.108 ## ParagonL 1.000 0.054 0.885 0.709 0.868 1.000 0.291 0.000 ## Pothole 1.000 1.000 1.000 0.488 0.248 1.000 0.296 0.850 ## ShipIsland 0.807 0.497 1.000 0.521 0.006 1.000 0.498 0.403 ## Skyhigh 0.915 0.493 0.063 0.001 0.155 1.000 0.126 0.078 Lets repeat this with a Monte Carlo permutation test with B = 1000 replicates: # Monte Carlo: p-value HWE.test &lt;- data.frame(sapply(seppop(Frogs.genind), function(ls) pegas::hw.test(ls, B=1000)[,4])) HWE.test.MC &lt;- t(data.matrix(HWE.test)) {cat(&quot;MC permuation test (p-values):&quot;, &quot;\\n&quot;) round(HWE.test.MC,3)} ## MC permuation test (p-values): ## A B C D E F G H ## Airplane 0.016 1.000 1.000 0.421 0.607 1 0.255 0.007 ## Bachelor 1.000 0.423 1.000 1.000 0.852 1 0.465 0.606 ## BarkingFox 1.000 0.230 0.061 1.000 0.725 1 1.000 0.151 ## Bob 1.000 1.000 1.000 1.000 0.014 1 1.000 0.278 ## Cache 1.000 0.411 0.144 1.000 1.000 1 1.000 0.605 ## Egg 1.000 1.000 1.000 1.000 0.085 1 0.507 0.425 ## Frog 1.000 1.000 0.079 1.000 0.453 1 1.000 0.159 ## GentianL 1.000 0.066 1.000 0.066 0.646 1 0.637 0.142 ## ParagonL 1.000 0.166 1.000 1.000 1.000 1 0.317 0.074 ## Pothole 1.000 1.000 1.000 1.000 0.532 1 0.508 1.000 ## ShipIsland 1.000 0.622 1.000 0.718 0.152 1 0.555 0.431 ## Skyhigh 1.000 0.351 0.142 0.089 0.106 1 0.083 0.041 To summarize, lets calculate, for each locus, the proportion of populations where it was out of HWE. Here well use the conservative cut-off of alpha = 0.05 for each test. There are various ways of modifying this, including a simple Bonferroni correction, where we divide alpha by the number of tests, which you can activate here by removing the ## i. front of the line. We write the results into a data frame Prop.loci.out.of.HWE and use = to specify the name for each column. alpha=0.05 Prop.loci.out.of.HWE &lt;- data.frame(Chisq=apply(HWE.test.chisq&lt;alpha, 2, mean), MC=apply(HWE.test.MC&lt;alpha, 2, mean)) Prop.loci.out.of.HWE # Type this line again to see results table ## Chisq MC ## A 0.00000000 0.08333333 ## B 0.00000000 0.00000000 ## C 0.16666667 0.00000000 ## D 0.16666667 0.00000000 ## E 0.16666667 0.08333333 ## F 0.00000000 0.00000000 ## G 0.00000000 0.00000000 ## H 0.08333333 0.16666667 And similarly, for each population, the proportion of loci that were out of HWE: Prop.pops.out.of.HWE &lt;- data.frame(Chisq=apply(HWE.test.chisq&lt;alpha, 1, mean), MC=apply(HWE.test.MC&lt;alpha, 1, mean)) Prop.pops.out.of.HWE ## Chisq MC ## Airplane 0.000 0.250 ## Bachelor 0.000 0.000 ## BarkingFox 0.125 0.000 ## Bob 0.125 0.125 ## Cache 0.125 0.000 ## Egg 0.000 0.000 ## Frog 0.000 0.000 ## GentianL 0.125 0.000 ## ParagonL 0.125 0.000 ## Pothole 0.000 0.000 ## ShipIsland 0.125 0.000 ## Skyhigh 0.125 0.125 The results suggest that: While most loci are out of HWE globally, this is largely explained by subdivision (variation in allele frequencies among local populations indicating limited gene flow). No locus is consistently out of HWE across populations (loci probably not affected by selection). No population is consistently out of HWE across loci (probably no recent major bottlenecks/ founder effects). Lets repeat this with false discovery rate correction for the number of tests. Here we use the function p.adjust with the argument method=fdr to adjust the p-values from the previous tests. This returns a vector of length 96 (the number of p-values used), which we convert back into a matrix of 12 rows (pops) by 8 columns (loci). Then we procede as above. Chisq.fdr &lt;- matrix(p.adjust(HWE.test.chisq,method=&quot;fdr&quot;), nrow=nrow(HWE.test.chisq)) MC.fdr &lt;- matrix(p.adjust(HWE.test.MC, method=&quot;fdr&quot;), nrow=nrow(HWE.test.MC)) Prop.pops.out.of.HWE &lt;- data.frame(Chisq=apply(HWE.test.chisq&lt;alpha, 1, mean), MC=apply(HWE.test.MC&lt;alpha, 1, mean), Chisq.fdr=apply(Chisq.fdr&lt;alpha, 1, mean), MC.fdr=apply(MC.fdr&lt;alpha, 1, mean)) Prop.pops.out.of.HWE ## Chisq MC Chisq.fdr MC.fdr ## Airplane 0.000 0.250 0.000 0 ## Bachelor 0.000 0.000 0.000 0 ## BarkingFox 0.125 0.000 0.000 0 ## Bob 0.125 0.125 0.000 0 ## Cache 0.125 0.000 0.000 0 ## Egg 0.000 0.000 0.000 0 ## Frog 0.000 0.000 0.000 0 ## GentianL 0.125 0.000 0.000 0 ## ParagonL 0.125 0.000 0.125 0 ## Pothole 0.000 0.000 0.000 0 ## ShipIsland 0.125 0.000 0.000 0 ## Skyhigh 0.125 0.125 0.125 0 After using false discovery rate correction for the 8 * 12 = 96 tests performed, very few combinations of locus and population were out of HWE based on the chi-squared test, and none with the MC test. Note: exact results are likely to differ somewhat between runs due to the permutation tests. d. Check for linkage disequilibrium (LD) See also: https://grunwaldlab.github.io/Population_Genetics_in_R/Linkage_disequilibrium.html For microsatellite markers, we typically dont know where on the genome they are located. The closer together two markers are on a chromosome, the more likely they are inherited together, which means that they dont really provide independent information. Testing for linkage disequilibrium assesses this, for each pair of loci, by checking whether alleles of two loci are statistically associated. This step is especially important when developing a new set of markers. You may want to drop (the less informative) one marker of any pair of linked loci. Here, we start with performing an overall test of linkage disequilibrium (the null hypothesis is that there is no linkage among the set of markers). Two indices are calculated and tested: an index of association (Ia; Brown et al. 1980) and a measure of correlation (rbarD; Agapow and Burt 2001), which is less biased (see URL above). The number of permutations is specified by sample = 199. Overall, there is statistically significant association among the markers (p-value: prD = 0.005; also left figure). Recall that the power of a statistical test increases with sample size, and here we have n = 181, hence even a small effect may be statistically significant. Hence we look at effect size, i.e., the actual strength of the pairwise associations (right figure). poppr::ia(Frogs.genind, sample=199) ## Ia p.Ia rbarD p.rD ## 0.33744318 0.00500000 0.05366542 0.00500000 LD.pair &lt;- poppr::pair.ia(Frogs.genind) LD.pair ## Ia rbarD ## A:B 0.0485 0.0492 ## A:C -0.0314 -0.0335 ## A:D 0.1886 0.1966 ## A:E 0.0560 0.0569 ## A:F -0.0272 -0.0452 ## A:G 0.0931 0.0935 ## A:H 0.0294 0.0304 ## B:C -0.0329 -0.0375 ## B:D 0.0903 0.0911 ## B:E 0.0910 0.0910 ## B:F -0.0013 -0.0025 ## B:G 0.0451 0.0452 ## B:H 0.0621 0.0623 ## C:D -0.0859 -0.1049 ## C:E 0.0247 0.0284 ## C:F -0.0311 -0.0397 ## C:G -0.0107 -0.0118 ## C:H 0.0012 0.0015 ## D:E 0.0455 0.0458 ## D:F 0.0094 0.0199 ## D:G 0.0069 0.0070 ## D:H 0.0461 0.0462 ## E:F 0.0013 0.0025 ## E:G 0.0453 0.0454 ## E:H 0.2153 0.2159 ## F:G 0.0167 0.0299 ## F:H 0.0296 0.0606 ## G:H 0.0942 0.0953 The strongest correlation is around 0.2, for markers E and H. Effect size: If rbarD can be interpreted similarly to a linear correlation coefficient r, that would mean that less than 5% of the variation in one marker is shared with the other marker (recall from stats: the amount of variance explained in regression, Rsquared, is the square of the linear correlation coefficient). This is probably not large enough to worry about. e. Check for null alleles See also Dakin and Avise (2004): http://www.nature.com/articles/6800545 One potential drawback for microsatellites as molecular markers is the presence of null alleles that fail to amplify, thus they couldnt be detected in the PCR assays. The function null.all takes a genind object and returns a list with two components (homozygotes and null.allele.freq), and each of these is again a list. See ?null.all for details and choice of method. List homozygotes: homozygotes$observed: observed number of homozygotes for each allele at each locus homozygotes$bootstrap: distribution of the expected number of homozygotes homozygotes$probability.obs: probability of observing the number of homozygotes # Null alleles: depends on method! See help file. Null.alleles &lt;- PopGenReport::null.all(Frogs.genind) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ## Registered S3 method overwritten by &#39;genetics&#39;: ## method from ## [.haplotype pegas Null.alleles$homozygotes$probability.obs ## Allele-1 Allele-2 Allele-3 Allele-4 Allele-5 Allele-6 Allele-7 Allele-8 ## A 0.103 0.000 0.042 NA NA NA NA NA ## B 0.140 0.040 0.070 0.007 NA NA NA NA ## C 0.186 0.002 0.000 0.003 NA NA NA NA ## D 0.000 0.000 0.218 0.002 NA NA NA NA ## E 0.050 0.019 0.032 0.268 0.057 0.037 0.715 0.556 ## F 0.433 0.002 0.010 NA NA NA NA NA ## G 0.079 0.084 0.018 0.002 NA NA NA NA ## H 0.440 0.094 0.008 0.025 0.262 0.279 0.005 0.002 ## Allele-9 ## A NA ## B NA ## C NA ## D NA ## E 0 ## F NA ## G NA ## H NA List null.allele.freq: null.allele.freq$summary1: null allele frequency estimates based upon the forumulas of Chakraborty et al. (1994) null.allele.freq$summary2: null allele frequency estimates based upon the forumulas of Brookfield (1996) From the help file: Brookfield (1996) provides a brief discussion on which estimator should be used. In summary, it was recommended that Chakraborty et al. (1994)s method (e.g. summary1) be used if there are individuals with no bands at a locus seen, but they are discounted as possible artefacts. If all individuals have one or more bands at a locus then Brookfield (1996)s method (e.g. summary2) should be used. In this case, we have many individuals with missing values for both alleles, hence better use summary1. Each summary table contains a summary with observed, median, 2.5th percentile and 97.5the percentile. The percentiles form a 95% confidence interval. From the help file: If the 95% confidence interval includes zero, it indicates that the frequency of null alleles at a locus does not significantly differ from zero. {cat(&quot; summary1 (Chakraborty et al. 1994):&quot;, &quot;\\n&quot;) round(Null.alleles$null.allele.freq$summary1,2)} ## summary1 (Chakraborty et al. 1994): ## A B C D E F G H ## Observed frequency 0.24 0.08 0.23 0.25 0.06 0.00 0.11 0.04 ## Median frequency 0.24 0.08 0.22 0.24 0.06 0.00 0.11 0.04 ## 2.5th percentile 0.08 0.01 0.03 0.17 0.02 -0.01 0.02 0.00 ## 97.5th percentile 0.42 0.15 0.48 0.32 0.11 0.00 0.23 0.09 {cat(&quot;summary2 (Brookfield et al. 1996):&quot;, &quot;\\n&quot;) round(Null.alleles$null.allele.freq$summary2,2)} ## summary2 (Brookfield et al. 1996): ## A B C D E F G H ## Observed frequency 0.06 0.05 0.05 0.17 0.05 0 0.07 0.04 ## Median frequency 0.06 0.05 0.05 0.17 0.05 0 0.07 0.03 ## 2.5th percentile 0.02 0.01 0.01 0.12 0.01 0 0.01 0.00 ## 97.5th percentile 0.11 0.09 0.10 0.22 0.09 0 0.14 0.08 For this example, both methods suggest that there may be null alleles in most loci. However, the estimates of the frequency of null alleles differ a lot between the two methods. A different approach for estimating null alleles at microsatellite loci, based on the Estimation-Maximization algorithm, is implemented in FreeNA (outside of the R environment). FreeNA will directly provide Fst values and some other measurements using the corrected allele frequencies: https://www1.montpellier.inra.fr/CBGP/software/FreeNA/ Relevant papers for the Estimation-Maximization algorithm: Kalinowski et al. (2006), Conservation Genetics 7:991995, doi: 10.1007/s10592-006-9134-9. Chapuis and Estoup (2007), Mol. Biol. Evol. 24:621631, doi: 10.1093/molbev/msl191. f. Overall interpretation Spatial genetic structure: The Columbia spotted frog data used in this lab come from a study area with a great deal of genetic structure. If we use a population assignment test (see Week 9), each basin is a separate unit with significant substructure within basins. Testing for significant genetic distance, most pairs of ponds have a genetic distance that is significantly different from zero. HWE: Therefore, we expect global estimates (i.e., the whole dataset) of He to be out of HWE due to population substructure (HWE assumes panmictic populations). We would also expect data to be out of HWE when analyzing data by basin due to population substructure. We could, then, test HWE and linkage disequilibrium at the pond level (as shown here). However, some ponds have low sample sizes (which refleect a low number of individuals: based on repeated surveys of sites, most if not nearly all animals were captured). Linkage: These low samples sizes can result in deviations from HWE and in linkage disequilibrium as an artifact of sample size and/or breeding structure (if one male is responsible for all breeding, his genes would appear linked in offspring genotypes, even though they are not physically linked in the genome). In addition, each pond may not represent a population, but only a portion of a population. So, what do we do? We can look for patterns. Are there loci that are consistently out of HWE across samples sites while other loci are not out of HWE suggesting that there are null alleles or other data quality control issues? With the full data set, this was not the case. Are there loci that are consistently linked across different ponds (while others are not), suggesting that they are linked? With the full dataset, this was not the case. Null alleles: Missing data can imply null (non-amplifying) alleles. In this case, while there are loci that have higher drop-out rates than others, this is more likely due to some loci being more difficult to call (the authors were very strict in removing any questionable data; equivocal calls resulting in no data). In addition, some of the samples used in this study were toe clips which with low yields of DNA, resulting in incomplete genotypes. While presence of null alleles is a possibility, genetic structure, breeding patterns at low population size and aggressive quality control of genotypes can all explain the results. Finally, with all of the structure in these data, there are examples (at the basin level and pond level) of unique alleles that are fixed or nearly fixed. When the data are assessed globally, this will result in a similar pattern to the presence of null alleles and will result in positive null allele tests. 3. Assess genetic diversity These measures are typically quantified per population. a. Rarefied allelic richness Both nominal sample size (number of frogs sampled) and valid sample size (e.g., for each locus, the number of frogs with non-missing genetic data) vary between sites. We would expect the number of alleles found in a population to increase with the number of individuals genotyped. We can check this by plotting the number of alleles against sample size. Here we create an object Sum that contains the summary of the genind object, then we can access its elements by $ to plot what we need. The function names lists the names of the elements, which reduced the guesswork. Sum &lt;- summary(Frogs.genind) names(Sum) ## [1] &quot;n&quot; &quot;n.by.pop&quot; &quot;loc.n.all&quot; &quot;pop.n.all&quot; &quot;NA.perc&quot; &quot;Hobs&quot; ## [7] &quot;Hexp&quot; The site names are quite long, hence we print the labels vertically by setting las=3, and we modify the margins (mar). The four numbers give the size of each margin in the following order: bottom, left, top, right. We add a regression line to the scatterplot with the function abline, where we specify the linear regression model with the function lm. In this case, we model the response pop.n.all as a function of predictor n.by.pop. The barchart (left) shows that there is considerable variation among ponds in the number of alleles observed across all loci. The scatterplot (right) with the red regression line shows that the number of alleles increases with sample size. par(mar=c(5.5, 4.5,1,1)) barplot(Sum$pop.n.all, las=3, xlab = &quot;&quot;, ylab = &quot;Number of alleles&quot;) plot(Sum$n.by.pop, Sum$pop.n.all, xlab = &quot;Sample size&quot;, ylab = &quot;Number of alleles&quot;) abline(lm(Sum$pop.n.all ~ Sum$n.by.pop), col = &quot;red&quot;) Hence we should not compare the number of alleles directly. Instead, well use rarefied allelic richness (Ar). By default, the function allel.rich finds the lowest valid sample size across all populations and loci, and multiplies it by the ploidy level. The number is stored as Richness$alleles.sampled (here: 3 individuals * 2 alleles = 6 alleles). Alternatively, this number can be set with the min.alleles argument. Richness &lt;- PopGenReport::allel.rich(Frogs.genind, min.alleles = NULL) Richness$alleles.sampled ## [1] 6 Populations with more alleles are resampled to determine the average allelic richness among the minimum number of alleles. Here, this means that 6 alleles are sampled from each population, allelic richness is calculated, and the process is repeated many times to determine the average). Lets plot the results again. The barchart shows that there is considerable variation in genetic diversity among ponds. The scatterplot against sample size (here: for each population, the average number of valid alleles across loci) suggests that the variation is not related to sample size. The regression line (red) is almost horizontal. Here we plot the average Ar across loci, so that the result does not depend on the number of loci used. par(mar=c(5.5, 4.5,1,1)) barplot(Richness$mean.richness, las=3, ylab=&quot;Rarefied allelic richness (Ar)&quot;) plot(colMeans(Richness$pop.sizes), Richness$mean.richness, xlab=&quot;Valid sample size&quot;, ylab=&quot;Rarefied allelic richness (Ar)&quot;) abline(lm(Richness$mean.richness ~ colMeans(Richness$pop.sizes)), col=&quot;red&quot;) b. Observed and expected heterozygosity Note: Writing the genind summary into an object Sum allows accessing its attributes by name. Sum &lt;- summary(Frogs.genind) names(Sum) ## [1] &quot;n&quot; &quot;n.by.pop&quot; &quot;loc.n.all&quot; &quot;pop.n.all&quot; &quot;NA.perc&quot; &quot;Hobs&quot; ## [7] &quot;Hexp&quot; Expected heterozygosity (here: Hexp) is the heterozygosity expected in a population under HWE, and observed heterozygosity (here: Hobs) is the observed number of heterozygotes at a locus divided by the total number of genotyped individuals. Here are the global values (pooled across all populations): par(mar=c(3, 4.5,1,1)) barplot(Sum$Hexp, ylim=c(0,1), ylab=&quot;Expected heterozygosity&quot;) barplot(Sum$Hobs, ylim=c(0,1), ylab=&quot;Observed heterozygosity&quot;) By locus and population: Here we use seppop to split the genind object by population, then sapply to apply function summary to each population. Hobs &lt;- t(sapply(seppop(Frogs.genind), function(ls) summary(ls)$Hobs)) Hexp &lt;- t(sapply(seppop(Frogs.genind), function(ls) summary(ls)$Hexp)) {cat(&quot;Expected heterozygosity (Hexp):&quot;, &quot;\\n&quot;) round(Hexp, 2) cat(&quot;\\n&quot;, &quot;Observed heterozygosity (Hobs):&quot;, &quot;\\n&quot;) round(Hobs, 2)} ## Expected heterozygosity (Hexp): ## ## Observed heterozygosity (Hobs): ## A B C D E F G H ## Airplane 0.25 0.33 0.00 0.33 0.62 0.00 0.16 0.74 ## Bachelor 0.00 0.38 0.40 0.25 0.88 0.00 0.33 0.75 ## BarkingFox 0.07 0.14 0.00 0.57 0.79 0.07 0.22 0.23 ## Bob 0.15 0.38 0.42 0.15 0.92 0.00 0.11 0.67 ## Cache 0.00 0.83 0.00 0.29 1.00 0.00 0.40 0.86 ## Egg 0.00 0.41 0.00 0.00 0.87 0.00 0.36 0.87 ## Frog 0.00 0.38 0.14 0.56 0.86 0.00 0.33 1.00 ## GentianL 0.11 0.95 0.00 0.74 0.90 0.15 0.56 0.84 ## ParagonL 0.00 0.11 0.08 0.16 0.33 0.00 0.36 0.47 ## Pothole 0.00 0.00 0.00 0.33 0.50 0.00 0.57 0.73 ## ShipIsland 0.41 0.41 0.00 0.59 0.53 0.00 0.36 0.71 ## Skyhigh 0.04 0.57 0.11 0.30 0.52 0.00 0.77 0.59 ## 1.- Hobs/Hexp Locus F shows variation only in two populations (i.e., Hexp = 0 in 10 populations). Lets plot the average across all loci for each population: Here we use apply to apply the function mean to the rows (MARGIN = 1). For columns, use MARGIN = 2. par(mar=c(5.5, 4.5, 1, 1)) Hobs.pop &lt;- apply(Hobs, MARGIN = 1, FUN = mean) Hexp.pop &lt;- apply(Hexp, 1, mean) barplot(Hexp.pop, ylim=c(0,1), las=3, ylab=&quot;Expected heterozygosity&quot;) barplot(Hobs.pop, ylim=c(0,1), las=3, ylab=&quot;Observed heterozygosity&quot;) c. Create table with sitel-level genetic diversity measures Frogs.diversity &lt;- data.frame(Pop = names(Hobs.pop), n = Sum$n.by.pop, Hobs = Hobs.pop, Hexp = Hexp.pop, Ar = Richness$mean.richness) Frogs.diversity ## Pop n Hobs Hexp Ar ## Airplane Airplane 21 0.3038064 0.3433019 1.939629 ## Bachelor Bachelor 8 0.3729167 0.3651953 2.003673 ## BarkingFox BarkingFox 14 0.2619811 0.2818609 1.741904 ## Bob Bob 13 0.3504274 0.3171011 1.961791 ## Cache Cache 7 0.4220238 0.3923413 2.085115 ## Egg Egg 17 0.3135918 0.2951215 1.841382 ## Frog Frog 9 0.4079861 0.3659439 1.925469 ## GentianL GentianL 20 0.5296418 0.4529000 2.331599 ## ParagonL ParagonL 19 0.1880302 0.2200536 1.539923 ## Pothole Pothole 13 0.2665043 0.2328137 1.597687 ## ShipIsland ShipIsland 17 0.3755252 0.3918983 1.949255 ## Skyhigh Skyhigh 23 0.3632542 0.3552167 1.953610 You can save the R object Frogs.diversity with the code below (need to uncomment by removing the hashtags #): #require(here) #if(!dir.exists(paste0(here(),&quot;/output&quot;))) dir.create(paste0(here(),&quot;/output&quot;)) #save(Frogs.diversity, file = paste0(here(),&quot;/output/Frogs.diversity.RData&quot;)) #load(paste0(here(),&quot;/output/Frogs.diversity.RData&quot;)) 4. Aggregate genetic data at population level (allele frequencies) For some analyses, we will need to aggregate data from the individual to the population level, e.g. as a table of allele frequencies per population. Here we convert the genind object to a genpop object (NOT the same as a genepop object!). This is defined in the package adegenet to hold population-level genetic data. The function genind2genpop obviously converts from genind to genpop. Frogs.genpop &lt;- adegenet::genind2genpop(Frogs.genind) ## ## Converting data from a genind to a genpop object... ## ## ...done. The function makefreq extracts the table with allele frequencies from the genpop object. Well plot just a few lines and alleles. Freq &lt;- adegenet::makefreq(Frogs.genpop) ## ## Finding allelic frequencies from a genpop object... ## ## ...done. round(Freq[1:6,1:10], 2) ## A.1 A.2 A.3 B.1 B.3 B.2 B.4 C.1 C.2 C.4 ## Airplane 0.62 0.35 0.03 0.83 0.17 0.00 0.00 1.00 0.00 0 ## Bachelor 1.00 0.00 0.00 0.56 0.06 0.38 0.00 0.80 0.20 0 ## BarkingFox 0.96 0.00 0.04 0.86 0.04 0.11 0.00 0.88 0.12 0 ## Bob 0.92 0.00 0.08 0.81 0.00 0.12 0.08 0.79 0.21 0 ## Cache 1.00 0.00 0.00 0.42 0.00 0.50 0.08 0.75 0.25 0 ## Egg 1.00 0.00 0.00 0.74 0.00 0.26 0.00 1.00 0.00 0 The allele frequencies of all alleles from the same locus (e.g., A.1, A.2 and A.3) should sum to 1 for each population. With eight loci, the row sums should thus add to 8. apply(Freq, MARGIN = 1, FUN = sum) # Just checking ## Airplane Bachelor BarkingFox Bob Cache Egg Frog ## 8 8 8 8 8 8 8 ## GentianL ParagonL Pothole ShipIsland Skyhigh ## 8 8 8 8 8 "],["r-exercise-week-3.html", "R Exercise Week 3", " R Exercise Week 3 Task: Drop offspring (seeds) from dataset pulsatilla_genotypes.csv, check for HWE by site and locus and calculate Hexp for each site. Hints: Load packages: Make sure the packages gstudio, dplyr and adegenet are loaded. Import data: Re-use your code from Week 1 exercise to import the dataset pulsatilla_genotypes.csv into gstudio. Count genotyped individuals. Determine the number of rows (and thus genotyped individuals). The dataset contains adults (OffID == 0) and genotyped seeds (OffID != 0). Determine the number of adults in the dataset. You can achieve this either by subsetting with square brackets [ ], or as a pipe using the function filter from the dplyr package, followed by nrow(). Drop offspring from dataset: Subset the data to retain only the adults, and call it Pulsatilla.adults. Again, you can achieve this either by indexing with square brackets, or by using the function filter from the dplyr package. Check the number of rows (adults). Split dataset by site. Use function split to split the data by site (population) and create an object Adults.by.site. Determine the length of the resulting list, i.e., the number of sub-datasets, one for each site. Count adults per site with sapply: Use sapply to calculate the number of rows (and thus genotyped individuals) per site (population). What is the range of sample sizes for adults? Convert to genind object: adapt your code from Week 1 exercise to convert the dataset with all adults, Pulsatilla.adults, to a genind object. Print the object to check that the data have been correctly imported. Is the number of rows equal to the number of adults that you found above? Check polymorphism: Use function summary (section 2.b) to check whether markers are polymorphic: what is the range of expected heterozygosity among the loci? Test for HWE by site and locus: adapt the code from section 2.c to test for HWE deviations across by site and locus (using chi-square or Monte-Carlo test). How many tests were significant (p-value &lt; 0.05)? Is there a systematic pattern of deviations for a specific locus, or for a specific site? Calculate Hexp and Hobs by site: adapt code from section 3.b to calculate Hexp and Hobs by site and locus, then take the mean across all loci (Hexp.pop, Hobs.pop) and combine them into a dataframe H.pop. Include the population name as a variable. Save result as R object: Save the object H.pop as an R object using the following code: saveRDS(H.pop, file = paste0(here::here(), \"/output/H.pop.rds\")). We will need it for a later R exercise. Question: Which site had the lowest expected heterozygosity? "],["Week4.html", "Week 4: Metapopulations ", " Week 4: Metapopulations "],["video-4.html", "View Course Video", " View Course Video Video, Part 1 Video, Part 2 Preview Slides Download "],["tutorial-4.html", "Interactive Tutorial 4", " Interactive Tutorial 4 List of R commands used Function Package ecoslot.SlotName EcoGenetics sort base order base row.names base rank base sample base is.numeric base cor stats plot graphics with base lm stats summary base abline graphics points graphics qplot ggplot2 ggplot ggplot2 aes ggplot2 geom_point ggplot2 geom_smooth ggplot2 xlab, ylab ggplot2 Instructions a) How to access tutorials: Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (Landscape_Genetics_R_Course) and tutorial (Weeks 1 - 8) b) How to complete tutorial: Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: MyName2) c) How to submit answers (participating institutions only): The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose yes, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use skip). If you used skip because you could not answer a question, please contact your instructor for advice. "],["WE-4.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals This worked example shows how to: Assess the spatial distribution of genetic structure in a metapopulation using hierarchical AMOVA Relate site-specific Fst to patch connectivity and population size Relate site-specific genetic diversity to explanatory variables (node-level analysis) Assess temporal changes (between years for same site) and evidence for extinction events Perform power analysis and sample size calculation for the temporal study b. Data set Lamy et al. (2012) sampled the freshwater snail Drepanotrema depressissimum in a fragmented landscape of tropical ponds on the island of Guadeloupe in the French West Indies. They used a spatially and temporally stratified sampling design with a total of 25 sites, where 12 sites formed four well-separated clusters of three neighbouring sites each, to study spatial variability, and 12 sites spread across the island were sampled in multiple years to study temporal variability. For each site and year, 22 - 34 individuals were genotyped at ten microsatellite loci. The species is diploid, hermaphroditic, and outcrossed. A key characteristic of this system is the presence of a dry and a rainy season. In the dry season, many ponds can dry out, possibly causing extinction of the local snail populations. During the rainy season, ponds refill and can even overflow, thereby becoming connected through the hydrological network. During this rainy season, dispersal between ponds may occur. dd.ecogen: The dataset dd.ecogen with genetic data for 1270 snails from 42 populations is included in package LandGenCourse. To load it, type: data(dd.ecogen). dd.site: Population-level data from Tables 2 - 5 of Lamy et al. (2012) are available in dataset dd.site (with 25 variables) in package LandGenCourse. To load it, type: data(dd.site). Reference: Lamy, T., Pointier, J. P., Jarne, P. and David, P. (2012), Testing metapopulation dynamics using genetic, demographic and ecological data. Molecular Ecology, 21: 13941410. doi:10.1111/j.1365-294X.2012.05478.x c. Required R libraries Install some packages needed for this worked example. if(!requireNamespace(&quot;EcoGenetics&quot;, quietly = TRUE)) remotes::install_github(&quot;leandroroser/EcoGenetics-devel&quot;) library(LandGenCourse) library(EcoGenetics) library(methods) library(ggplot2) #require(tibble) #require(poppr) #require(ade4) #require(pwr) #require(effsize) #require(sp) #require(ggmap) #require(car) d. Import data Lets import the genetic data (spatial and temporal data sets combined, 42 combinations of site and year). Use ?dd.ecogen to check helpfile with data set desription. The ecogen object dd.ecogen contains individual-level data in the following slots: XY: Spatial coordinates (lat-long format) G: Microsatellite loci (columns = loci, rows = individuals) A: Table of allele frequencies (columns = alleles, rows = individuals) S: Structure variables (SiteID, SITE, YEAR, Cluster) data(dd.ecogen, package = &quot;LandGenCourse&quot;) dd.ecogen ## ## || ECOGEN CLASS OBJECT || ## ---------------------------------------------------------------------------- ## Access to slots: &lt;ecoslot.&gt; + &lt;name of the slot&gt; + &lt;(name of the object)&gt; ## See: help(&quot;EcoGenetics accessors&quot;) ## ---------------------------------------------------------------------------- ## ## | slot XY: | --&gt; 1270 x 2 coordinates ## | slot P: | --&gt; 0 x 0 phenotypic variables ## | slot G: | --&gt; 1270 x 10 loci &gt;&gt; ploidy: 2 || codominant ## | slot A: | --&gt; 1270 x 372 alleles ## | slot E: | --&gt; 0 x 0 environmental variables ## | slot S: | --&gt; 1270 x 4 structures &gt;&gt; 4 structures found ## | slot C: | --&gt; 0 x 0 variables ## | slot OUT: | --&gt; 0 results ## ---------------------------------------------------------------------------- ?dd.ecogen ## starting httpd help server ... done We also import site-level data from Tables 2 - 5 in Lamy et al. (2012). Use ?dd.site to check helpfile with data set desription of the variables. data(dd.site, package = &quot;LandGenCourse&quot;) tibble::as_tibble(dd.site) ## # A tibble: 42 x 24 ## SiteID SITE YEAR Spatial MultiYear APE Cluster n RA He f ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PICO2006 PICO 2006 FALSE TRUE FALSE &lt;NA&gt; 34 11.2 0.897 0.038 ## 2 PICO2007 PICO 2007 FALSE TRUE FALSE &lt;NA&gt; 24 11.4 0.911 0.022 ## 3 PICO2009 PICO 2009 TRUE TRUE FALSE &lt;NA&gt; 32 11.4 0.896 0.024 ## 4 ROC2006 ROC 2006 TRUE TRUE FALSE &lt;NA&gt; 32 11.2 0.831 0.005 ## 5 ROC2007 ROC 2007 FALSE TRUE FALSE &lt;NA&gt; 32 11.0 0.82 0.018 ## 6 SEN2006 SEN 2006 FALSE TRUE FALSE &lt;NA&gt; 32 11.2 0.876 -0.008 ## 7 SEN2007 SEN 2007 TRUE TRUE FALSE &lt;NA&gt; 30 11.6 0.88 0.046 ## 8 VEE2006 VEE 2006 FALSE TRUE FALSE &lt;NA&gt; 31 10.5 0.864 0.034 ## 9 VEE2007 VEE 2007 FALSE TRUE FALSE &lt;NA&gt; 32 11.3 0.854 0.044 ## 10 VEE2008 VEE 2008 TRUE TRUE FALSE &lt;NA&gt; 31 11.2 0.871 0.014 ## # ... with 32 more rows, and 13 more variables: s &lt;dbl&gt;, Type &lt;fct&gt;, ## # FST.GESTE &lt;dbl&gt;, Size &lt;dbl&gt;, V &lt;dbl&gt;, C &lt;dbl&gt;, Stab &lt;dbl&gt;, D &lt;int&gt;, ## # APA &lt;int&gt;, NLT &lt;dbl&gt;, Fst.temp &lt;dbl&gt;, Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt; ?dd.site Questions: with the help file for dd.site, check the meaning of the following explanatory variables: What does APE refer to, and how is it different from APA? What does NLT represent, and is it calculated independently from Size? What does Type mean, and what about V and D? To understand how connectivity C and stability Stab were calculated, youll need to consult Lamy et al. (2012). Your hypothesis: which explanatory variables would you expect to affect: Genetic diversity within local populations? Genetic differentiation among local populations? Both? In the following, well perform three types of analyses: Compare 25 populations in space, across the island of Guadeloupe. Compare 12 populations in 4 clusters: differentiation within vs. among clusters? Compare 12 sites over time, some of which experienced a local extinction event. 2. Spatial distribution of genetic structure How similar are populations from nearby habitat patches compared to populations across the island? To answer this question, we perform a hiearchical AMOVA (analysis of molecular variance) with individuals from 12 populations that form 4 clusters with 3 populations each. a. Creating a genind object with the hierarchical data set First, we need to extract the samples that belong to the hierarchical data set. There are four clusters: North, East, Center and South. We are looking for the observations where the variable Cluster has one of these four values, all other observations will have a missing value for Cluster. We can use !is.na to extract all rows with non-missing values. Then we convert to a genind object. dd.ecogen.Cluster &lt;- dd.ecogen[!is.na(dd.ecogen[[&quot;S&quot;]]$Cluster),] dd.genind.Cluster &lt;- EcoGenetics::ecogen2genind(dd.ecogen.Cluster) ## Loading required package: adegenet ## Loading required package: ade4 ## Registered S3 method overwritten by &#39;spdep&#39;: ## method from ## plot.mst ape ## ## /// adegenet 2.1.3 is loaded //////////// ## ## &gt; overview: &#39;?adegenet&#39; ## &gt; tutorials/doc/questions: &#39;adegenetWeb()&#39; ## &gt; bug reports/feature requests: adegenetIssues() b. Hierarchical AMOVA There are several implementations of AMOVA in R, e.g. in pacakges ade4, pegas and vegan. The ade4 implementation is closest to the original implementation in Arlequin. Package poppr has a wrapper function poppr.amova that makes it easy to perform AMOVA with the ade4 or with the pegas implementation (see ?poppr.amova for a discussion of their pros and cons). Here well use ade4. The first argument is the genind object. The argument hier defines the hierarchy, with the top level first (i.e., here SITE is nested within Cluster). The variables are expected to be found in the @strata slot of the genind object. The argument within=FALSE specifies that within-individual variance (i.e., observed heterozygosity) should not be tested. Setting this to TRUE can lead to problems with missing values. First we run the AMOVA and estimate the percent of molecular variance at each hierarchical level. amova.result &lt;- poppr::poppr.amova(dd.genind.Cluster, hier = ~ Cluster/SITE, within=FALSE, method = &quot;ade4&quot;) ## Registered S3 method overwritten by &#39;pegas&#39;: ## method from ## print.amova ade4 ## ## No missing values detected. amova.result ## $call ## ade4::amova(samples = xtab, distances = xdist, structures = xstruct) ## ## $results ## Df Sum Sq Mean Sq ## Between Cluster 3 67.27429 22.424764 ## Between samples Within Cluster 8 158.37917 19.797396 ## Within samples 353 1563.08216 4.427995 ## Total 364 1788.73562 4.914109 ## ## $componentsofcovariance ## Sigma % ## Variations Between Cluster 0.02770227 0.5582719 ## Variations Between samples Within Cluster 0.50644882 10.2062461 ## Variations Within samples 4.42799478 89.2354820 ## Total variations 4.96214587 100.0000000 ## ## $statphi ## Phi ## Phi-samples-total 0.107645180 ## Phi-samples-Cluster 0.102635446 ## Phi-Cluster-total 0.005582719 Then we test whether each variance component is statistically significant (i.e., significantly larger than zero). amova.test &lt;- ade4::randtest(amova.result, nrepet = 999) amova.test ## class: krandtest lightkrandtest ## Monte-Carlo tests ## Call: randtest.amova(xtest = amova.result, nrepet = 999) ## ## Number of tests: 3 ## ## Adjustment method for multiple comparisons: none ## Permutation number: 999 ## Test Obs Std.Obs Alter Pvalue ## 1 Variations within samples 4.42799478 -84.386816 less 0.001 ## 2 Variations between samples 0.50644882 57.303043 greater 0.001 ## 3 Variations between Cluster 0.02770227 1.693551 greater 0.051 Questions: At what level is there stronger differentiation, within or among clusters? What does this mean biologically? Are both levels statistically significant? 3. What determines genetic differentiation among sites? What factors explain site-specific Fst? Lets consider the key micro-evolutionary processes: Genetic drift: the smaller the population, the higher the rate of drift, hence we expect higher differention for small populations. Predictor: long-term population size NLT. Gene flow: gene flow homogenizes allele frequencies, hence we expect less differentiation for well connected patches. Predictors: connectivity C, density of favorable habitat D (within 2 km radius). First, we create a new SpatialPointsDataFrame with the subset of data for the spatial analysis (25 ponds, one year each). dd.spatial &lt;- dd.site[dd.site@data$Spatial==TRUE, ] a. Correlation matrix Lets start with a correlation matrix. cor(dd.spatial@data[ , c(&quot;FST.GESTE&quot;, &quot;NLT&quot;, &quot;C&quot;, &quot;D&quot;)], use=&quot;pairwise.complete&quot;) ## FST.GESTE NLT C D ## FST.GESTE 1.00000000 -0.48565247 -0.43104600 0.02768978 ## NLT -0.48565247 1.00000000 -0.08369541 0.10242973 ## C -0.43104600 -0.08369541 1.00000000 -0.32078070 ## D 0.02768978 0.10242973 -0.32078070 1.00000000 Questions: Is there genetic evidence for higher drift in small populations? Is there genetic evidence for higher gene flow among well connected patches? Are the two factors confounded for this data set? Would you prefer C or D to quantify patch connectivity? Does it matter? What does this mean biologically? b. Scatterplots Lets plot the response variable FST.GESTE against each of the two predictors NLT and C. Here, we use functions from the package ggplot2 (already loaded) to define two ggplot objects NLT.plot and C.plot, then we plot them side-by-side with the function cowplot::plot_grid. For NLT.plot, we define the dataset as dd.spatial@data, the x-axis as variable NLT, the y-axis as variable FST.GESTE, and the labels as variable SITE. We add points with geom_point. We add a regression line (geom_smooth), make it linear (method = lm) and add a shaded area for plus/minus 1 SE of the mean for a given value of x (se = TRUE). We add backfilled labels (geom_label), define their size (size), and move them up a little along the y-axis (nudge_y) NLT.plot &lt;- ggplot(dd.spatial@data, aes(x=NLT, y=FST.GESTE, label=SITE)) + geom_point() + geom_smooth(method = lm, se = TRUE) + geom_text(size=2, nudge_x=0, nudge_y=0.01, check_overlap=TRUE) C.plot &lt;- ggplot(dd.spatial@data, aes(x=C, y=FST.GESTE, label=SITE)) + geom_point() + geom_smooth(method = lm, se = TRUE) + geom_text(size=2.5, nudge_x=0, nudge_y=0.01, check_overlap=TRUE) cowplot::plot_grid(NLT.plot, C.plot) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; c. Regression model The two predictors NLT and C are not strongly correlated. Well fit a regression model with both predictors. Here we use function scale to standardize each variable, so that we can interpret the regression slope coefficients as partial correlation coefficients (beta coefficients). mod.diff &lt;- lm(scale(FST.GESTE) ~ scale(NLT) + scale(C), data=dd.spatial) summary(mod.diff) ## ## Call: ## lm(formula = scale(FST.GESTE) ~ scale(NLT) + scale(C), data = dd.spatial) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0576 -0.4142 -0.1361 0.2653 2.2394 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.576e-17 1.535e-01 0.000 1.00000 ## scale(NLT) -5.254e-01 1.572e-01 -3.342 0.00296 ** ## scale(C) -4.750e-01 1.572e-01 -3.021 0.00628 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7676 on 22 degrees of freedom ## Multiple R-squared: 0.4599, Adjusted R-squared: 0.4108 ## F-statistic: 9.367 on 2 and 22 DF, p-value: 0.00114 Is the model valid? Lets check the residual plots: par(mfrow=c(2,2)) plot(mod.diff, labels.id = dd.spatial@data$SITE) par(mfrow=c(1,1)) If we had more than two predictors, it would be a good idea to calculate variance inflation factors. The package car has a function vif that takes as argument a fitted model. Here, both predictors have VIF = 1.007, which indicates no collinearity. car::vif(mod.diff) ## scale(NLT) scale(C) ## 1.007054 1.007054 d. Which populations dont fit the general pattern? Lets plot the residuals in space. The function bubble from the package sp evaluates the projection information of the SpatialPointsDataFrame dd.spatial. dd.spatial@data$Residuals &lt;- mod.diff$residuals sp::bubble(dd.spatial, zcol = &quot;Residuals&quot;, col = c(&quot;red&quot;, &quot;blue&quot;)) ## Warning in wkt(obj): CRS object has no comment ## Warning in wkt(obj): CRS object has no comment Or on a map from the internet, using qmplot from the ggmap package. It expects lat-lon coordinates that are stored in a data frame, not a SpatialPointsDataFrame. If we convert dd.spatial with the function as.data.frame, R will return a data frame with the site variables and with the coordinates appended as additional columns. The code below does the following (see Week 4 video, part 2): Creates an index a that lists identifies the two potential outliers by their SITE names (logical), Creates a vectors a2 that contains the row numbers of the potential outliers, Grabs a grayscale map from the internet, plots all ponds with size and color according to their residuals, and stores the resulting map in the object myMap. Plots the map and adds labels for the two potential outliers.The argument size affects font size and label.padding determines the size of the textbox for the label. a &lt;- is.element(dd.spatial@data$SITE, c(&quot;DESB&quot;, &quot;PTC&quot;)) a2 &lt;- c(1:nrow(dd.spatial@data))[a] myMap &lt;- ggmap::qmplot(Longitude, Latitude, data = as.data.frame(dd.spatial), source = &quot;stamen&quot;, maptype = &quot;toner-lite&quot;, force=TRUE, col = sign(Residuals), size = abs(Residuals)) myMap + ggplot2::geom_label(data = as.data.frame(dd.spatial[a2,]), mapping = ggplot2::aes(Longitude, Latitude, label = SITE), size = 4, label.padding = unit(0.2, &quot;lines&quot;), col = &quot;black&quot;, vjust = 0, nudge_x = 0, nudge_y = -0.015) Exports the last plot as a png (Portable Network Graphics) file, specifying canvas size and resolution. To run the code, uncomment by removing # at the beginning of each line. #require(here) #if(!dir.exists(paste0(here(),&quot;/output&quot;))) dir.create(paste0(here(),&quot;/output&quot;)) #ggplot2::ggsave(paste0(here(),&quot;/output/ResidualMap.png&quot;), # width = 7, height = 5.5, units = &quot;in&quot;, dpi = 300) What might explain the large residuals for the two sites PTC and DESB? Site PTC lies on the tip of a peninsula and thus is very isolated geographically. Site DESB is a very instable site that can frequently dry out during the dry season, as it is shallow and lies in the comparatively dry northern part of the island. In addition, although DESB is surrounded by many ponds, these ponds never get connected to DESB hydrologically during the rainy season. Therefore, immigration can only occur via cattle or birds, which are much less important drivers of gene flow than immigration by hydrological connectivity during the rainy season. e. Regression model without outliers We can use the same index a to exclude the potential outliers from the regression model: mod.diff.minus2 &lt;- lm(scale(FST.GESTE) ~ scale(NLT) + scale(C), data=dd.spatial[-a2,]) summary(mod.diff.minus2) ## ## Call: ## lm(formula = scale(FST.GESTE) ~ scale(NLT) + scale(C), data = dd.spatial[-a2, ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2815 -0.5002 -0.1009 0.4732 1.7142 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.490e-17 1.562e-01 0.000 1.00000 ## scale(NLT) -5.096e-01 1.622e-01 -3.142 0.00514 ** ## scale(C) -5.761e-01 1.622e-01 -3.552 0.00200 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7493 on 20 degrees of freedom ## Multiple R-squared: 0.4896, Adjusted R-squared: 0.4386 ## F-statistic: 9.593 on 2 and 20 DF, p-value: 0.001199 Did omitting the two sites improve model fit? Did it change the nature of the results? Does this affect the biologial interpretation? par(mfrow=c(2,2)) plot(mod.diff.minus2, labels.id = dd.spatial@data$SITE[-a2]) par(mfrow=c(1,1)) 4. What determines genetic diversity? Can the same predictors (population size and connectivity) explain genetic diversity? Is patch size (Size) a good proxy for population size (as often used in ecological studies)? Which measure of genetic diversity shows the stronger response, allelic richness (rarefied) or expected heterozygosity? a. Correlation matrix cor(dd.spatial@data[, c(&quot;RA&quot;, &quot;He&quot;, &quot;Size&quot;, &quot;NLT&quot;, &quot;C&quot;, &quot;D&quot;)], use=&quot;pairwise.complete&quot;) ## RA He Size NLT C D ## RA 1.00000000 0.95151016 0.3482453 0.51343861 0.46576979 -0.09856668 ## He 0.95151016 1.00000000 0.3057086 0.49851503 0.45206733 -0.09593604 ## Size 0.34824532 0.30570855 1.0000000 0.76128271 -0.17765815 0.30002700 ## NLT 0.51343861 0.49851503 0.7612827 1.00000000 -0.08369541 0.10242973 ## C 0.46576979 0.45206733 -0.1776581 -0.08369541 1.00000000 -0.32078070 ## D -0.09856668 -0.09593604 0.3000270 0.10242973 -0.32078070 1.00000000 Questions: How strongly are the two diversity measures RA and He correlated? Are NLT and C strongly correlated with the diversity measures RA and He? Is the correlation with Size similarly strong as the correlation with NLT? How strongly are Size and NLT correlated with each other? Does D show a stronger correlation with diversity than with differentiation? b. Regression models For allelic richness: mod.RA &lt;- lm(scale(RA) ~ scale(NLT) + scale(C), data = dd.spatial) summary(mod.RA) ## ## Call: ## lm(formula = scale(RA) ~ scale(NLT) + scale(C), data = dd.spatial) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.5115 -0.5140 0.2356 0.4995 1.2462 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.238e-16 1.441e-01 0.000 1.00000 ## scale(NLT) 5.563e-01 1.476e-01 3.770 0.00106 ** ## scale(C) 5.123e-01 1.476e-01 3.472 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7204 on 22 degrees of freedom ## Multiple R-squared: 0.5243, Adjusted R-squared: 0.481 ## F-statistic: 12.12 on 2 and 22 DF, p-value: 0.0002825 par(mfrow=c(2,2)) plot(mod.RA, labels.id = dd.spatial@data$SITE) par(mfrow=c(1,1)) For gene diversity (expected heterozygosity): mod.He &lt;- lm(scale(He) ~ scale(NLT) + scale(C), data = dd.spatial) summary(mod.He) ## ## Call: ## lm(formula = scale(He) ~ scale(NLT) + scale(C), data = dd.spatial) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.7545 -0.3619 0.1211 0.4458 1.2550 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.381e-16 1.486e-01 0.000 1.00000 ## scale(NLT) 5.401e-01 1.522e-01 3.549 0.00180 ** ## scale(C) 4.973e-01 1.522e-01 3.268 0.00352 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7429 on 22 degrees of freedom ## Multiple R-squared: 0.4941, Adjusted R-squared: 0.4481 ## F-statistic: 10.74 on 2 and 22 DF, p-value: 0.0005559 par(mfrow=c(2,2)) plot(mod.He, labels.id = dd.spatial@data$SITE) par(mfrow=c(1,1)) 5. Are genetic differentiation and diversity related? Would you expect a relationship between genetic diversity and genetic differentiation of individual patches? Lets examine the correlation between gene diversity (He) and site-specific Fst: cor(dd.site$He, dd.site$FST.GESTE, use = &quot;pairwise.complete&quot;) ## [1] -0.9567184 There are a number of possible reasons for such a correlation. Can you put forward some hypotheses to explain this relationship? See Lamy et al. (2012) for their interpretation. 6. Effect of recent extinction events Several patches fell dry between observation years, which is assumed to signify extinction of the local population. Does genetic evidence support this interpretation, i.e., is there genetic evidence of bottlenecks or founder effects in D. depressissimum? a. Effect of patch extinction event (temporal data set) dd.temporal &lt;- dd.site[dd.site@data$MultiYear==TRUE,] cor(dd.temporal@data[, c(&quot;Fst.temp&quot;, &quot;APE&quot;, &quot;NLT&quot;, &quot;C&quot;)], use=&quot;pairwise.complete&quot;) ## Fst.temp APE NLT C ## Fst.temp 1.0000000 0.29379662 -0.10494809 -0.3356817 ## APE 0.2937966 1.00000000 -0.07420821 -0.4927664 ## NLT -0.1049481 -0.07420821 1.00000000 0.3103541 ## C -0.3356817 -0.49276638 0.31035409 1.0000000 We can compare a number of competing models using the Akaike Information Criterion (AIC). Models with lower AIC are better (see Week 12). mod.Fst.temp &lt;- lm(scale(Fst.temp) ~ scale(APE), data=dd.temporal) summary(mod.Fst.temp) ## ## Call: ## lm(formula = scale(Fst.temp) ~ scale(APE), data = dd.temporal) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0290 -0.5094 -0.3566 0.2140 2.0886 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.04099 0.29246 -0.140 0.891 ## scale(APE) 0.27599 0.28396 0.972 0.354 ## ## Residual standard error: 1.003 on 10 degrees of freedom ## (17 observations deleted due to missingness) ## Multiple R-squared: 0.08632, Adjusted R-squared: -0.005052 ## F-statistic: 0.9447 on 1 and 10 DF, p-value: 0.354 mod.Fst.temp.C &lt;- lm(scale(Fst.temp) ~ scale(APE) + scale(C), data=dd.temporal) mod.Fst.temp.NLT &lt;- lm(scale(Fst.temp) ~ scale(APE) + scale(NLT), data=dd.temporal) mod.Fst.temp.both &lt;- lm(scale(Fst.temp) ~ scale(APE) + scale(NLT) + scale(C), data=dd.temporal) AIC(mod.Fst.temp, mod.Fst.temp.C, mod.Fst.temp.NLT, mod.Fst.temp.both) ## df AIC ## mod.Fst.temp 3 37.92714 ## mod.Fst.temp.C 4 39.27770 ## mod.Fst.temp.NLT 4 39.83549 ## mod.Fst.temp.both 5 41.27462 The best model includes neither C nor NLT. Note that APE is a binary variable, so in essence were performing a t-test here. res.Fst.temp &lt;- t.test(Fst.temp ~ APE, data=dd.temporal, alternative = &quot;less&quot;) res.Fst.temp ## ## Welch Two Sample t-test ## ## data: Fst.temp by APE ## t = -0.94604, df = 7.893, p-value = 0.1861 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.01085372 ## sample estimates: ## mean in group FALSE mean in group TRUE ## 0.0080 0.0192 b. Power analysis The effect is not statistically significant. Does that mean that we found no effect of apparent population extinctions on temporal Fst? Lets check effect size. For means, Cohens effect size is measured by d (which is measured in units of standard deviations): small effect: d &gt; 0.2 (means at least 0.2 standard deviations apart) medium effect: d &gt; 0.5 large effect: d &gt; 0.8 We can let R calculate effect size for us: effsize::cohen.d(Fst.temp ~ factor(APE), data=dd.temporal) ## ## Cohen&#39;s d ## ## d estimate: -0.5691221 (medium) ## 95 percent confidence interval: ## lower upper ## -1.8992165 0.7609722 So, we actually found a medium effect (more than 0.5 standard deviations difference between group means). Maybe sample size was too small to have sufficient power? Lets check sample size: table(dd.temporal$APE[!is.na(dd.temporal$Fst.temp)]) ## ## FALSE TRUE ## 7 5 Ah, that explains a lot. There were only 5 sites with apparent extinction, and 7 without. Given that sample size, what was the statistical power of our test to detect at least a large effect (d = - 0.8), i.e., be able to reject the null hypothesis if such an effect is present in the population from which we sampled? pwr::pwr.t2n.test(n1=7, n2=5, d=-0.8, alternative = &quot;less&quot;) ## ## t test power calculation ## ## n1 = 7 ## n2 = 5 ## d = -0.8 ## sig.level = 0.05 ## power = 0.3552962 ## alternative = less So the power to detect at least a large effect, if it exists in the population, was only 0.355, way below the 0.8 (or even 0.95) that we would want to see. For a medium effect, the power is even smaller. c. Sample size calculation How large a sample would we have needed in each group to achieve a power of 0.8 to detect a large effect? And for a medium effect? pwr::pwr.t.test(power = 0.8, d = -0.8, alternative = &quot;less&quot;) ## ## Two-sample t test power calculation ## ## n = 20.03277 ## d = -0.8 ## sig.level = 0.05 ## power = 0.8 ## alternative = less ## ## NOTE: n is number in *each* group pwr::pwr.t.test(power = 0.8, d = -0.5, alternative = &quot;less&quot;) ## ## Two-sample t test power calculation ## ## n = 50.1508 ## d = -0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = less ## ## NOTE: n is number in *each* group More than 20 sites in each group would have been needed to detect a large effect, or more than 50 per group to detect a medium effect, with a power of 80%. Hence, these particular results are inconclusive. There was a trend showing a large effect size but power was very low. This aspect of the study should ideally be repeated with a larger sample size before reaching any conclusions. Note however that using additional evidence (e.g., population assignment tests), Lamy et al. (2012) concluded that extinctions were in fact less common in this system than previously assumed  in many cases of apparent extinction, individuals may still be present but just not detected. "],["r-exercise-week-4.html", "R Exercise Week 4", " R Exercise Week 4 Task: Build on your previous exercises and plot the sites on a map downloaded from the internet. Explore the relationships between Hexp, census population size and percent forest cover within 500 m of the site (forest may act as a barrier for grassland plants). Hints: Load packages: You may want to load the packages dplyr and ggplot2. Alternatively, you can use :: to call functions from packages. Import your datasets from Weeks 2 &amp; 3 R Exercises. Heres an example for your code, adapt it as needed to import the R objects Pulsatilla.longlat.rds (SpatialPointsDataFrame, Week 2) and H.pop.rds (Week 3) that you saved previously: Pulsatilla.longlat &lt;- readRDS(paste0(here::here(), \"/output/Pulsatilla.longlat.rds\")) Plot sites on map from internet: To get started, you may use this code (note the names of the variables with the spatial coordinates): myMap &lt;- ggmap::qmplot(x=meanX, y=meanY, data = as.data.frame(Pulsatilla.longlat), source = \"stamen\", maptype = \"toner-lite\") Next, modify code from section 3.d to add labels for all sites and plot the map. Combine data: Use the function dplyr::left_join to add the variables from the dataset H.pop to the data frame in the @data slot of Pulsatilla.longlat. Notes: This is important, as the order of populations may not be the same in the two datasets. Remember to check the structure of the datasets (variable names and types) first so that you know which are the ID variables that you can use to match sites. If the two ID variables are not of the same type (e.g., one if a factor, the other is character), it is best to change the format of one (e.g., with as.character) before doing the left-join. Scatterplot with regression line: Create a scatterplot of Hexp (y axis) plotted against nIndiv (x axis). Add a regression line and, if possible, label points. You may modify code from section 3.b using the ggplot2 package, or use base R functions. Regression analysis: Adapt code from section 3.c to perform a regression of Hexp (response variable) on the predictor nIndiv. Create residual plots and inspect them. What is the main issue here? Questions: There is one influential point in the regression analysis: Which site is it? Where is it located (north / east / south / west)? What makes it an influential point (large residual, leverage, or both)? What would happen to the regression line and the R-squared if this point was omitted? "],["Week5.html", "Week 5: Spatial Statistics ", " Week 5: Spatial Statistics "],["video-5.html", "View Course Video", " View Course Video Preview Slides Download "],["tutorial-5.html", "Interactive Tutorial 5", " Interactive Tutorial 5 List of R commands used Function Package plot(asp = 1) base dist stats as.matrix base lower.tri base diag base dnearneigh spdep data.matrix base nb2listw spdep listw2mat spdep nbdists spdep Instructions a) How to access tutorials: Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (Landscape_Genetics_R_Course) and tutorial (Weeks 1 - 8) b) How to complete tutorial: Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: MyName2) c) How to submit answers (participating institutions only): The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose yes, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use skip). If you used skip because you could not answer a question, please contact your instructor for advice. "],["WE-5.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals This worked example shows how to: Calculate genetic distance at population level. Perform a Mantel test to test for IBD. Create Mantel correlograms for the genetic data. Calculate and test Morans I for population-level genetic diversity data. b. Data set This is a larger version of the frog data set used in Weeks 1 - 3. Here, we will analyze microsatellite data from Funk et al. (2005) and Murphy et al. (2010) for 418 individuals of Colombia spotted frogs (Rana luteiventris) from 30 populations, together with site-level spatial coordinates. Please see the introductory video on the DGS course website, R lab page. ralu_loci_allpops.csv: Data frame with populations and genetic data (406 rows x 9 columns). Included as external file in package LandGenCourse. Frogs_diversity_allpops.csv: Table with genetic diversity variables similar to Week 3. ralu_coords_allpops.csv: Spatial coordinates (both in UTM and LongLat format), and basin. c. Required R packages Install some packages (not on CRAN) that are needed for this worked example. if(!requireNamespace(&quot;EcoGenetics&quot;, quietly = TRUE)) remotes::install_github(&quot;leandroroser/EcoGenetics-devel&quot;) if(!requireNamespace(&quot;GeNetIt&quot;, quietly = TRUE)) remotes::install_github(&quot;jeffreyevans/GeNetIt&quot;) if(!requireNamespace(&quot;popgraph&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/popgraph&quot;) if(!requireNamespace(&quot;gstudio&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/gstudio&quot;) library(LandGenCourse) library(GeNetIt) library(dplyr) library(EcoGenetics) #require(adegenet) #require(tibble) #require(gstudio) #require(hierfstat) #require(PopGenReport) #require(mmod) #require(spdep) 2. Data import and manipulation a. Create an ecogen object Import the site data that we used before in Week 2. Frogs.coords &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;ralu_coords_allpops.csv&quot;, package = &quot;LandGenCourse&quot;)) In Week 3, we calculated population-level genetic data as a table Frogs.diversity. Here we import a table with the same diversity measures for all 29 sites from a system file in LandGenCourse. Frogs.diversity &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;Frogs_diversity_allpops.csv&quot;, package = &quot;LandGenCourse&quot;)) Import the genetic data for 29 sites: Frogs.loci &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;ralu_loci_allpops.csv&quot;, package = &quot;LandGenCourse&quot;)) We start building an ecogen object by assigning the genetic data to the G slot, and the structure (i.e., hierarchical sampling information, here site names) to the S slot. For the genetic data, we need to specify the type of data and coding. Note the tweak using data.frame when specifying data.frame(ralu.loci[,1:2]). This is necessary to import the multiple columns correctly and with their original names. Frogs.ecogen &lt;- ecogen(G = Frogs.loci[,-c(1:2)], ploidy = 2, type = &quot;codominant&quot;, sep = &quot;:&quot;, S = data.frame(Frogs.loci[,1:2])) Frogs.ecogen ## ## || ECOGEN CLASS OBJECT || ## ---------------------------------------------------------------------------- ## Access to slots: &lt;ecoslot.&gt; + &lt;name of the slot&gt; + &lt;(name of the object)&gt; ## See: help(&quot;EcoGenetics accessors&quot;) ## ---------------------------------------------------------------------------- ## ## | slot XY: | --&gt; 0 x 0 coordinates ## | slot P: | --&gt; 0 x 0 phenotypic variables ## | slot G: | --&gt; 413 x 8 loci &gt;&gt; ploidy: 2 || codominant ## | slot A: | --&gt; 413 x 43 alleles ## | slot E: | --&gt; 0 x 0 environmental variables ## | slot S: | --&gt; 413 x 2 structures &gt;&gt; 2 structures found ## | slot C: | --&gt; 0 x 0 variables ## | slot OUT: | --&gt; 0 results ## ---------------------------------------------------------------------------- The summary confirms that we now have data in the S and G slots. In addition, EcoGenetics created a table of absolute frequencies (i.e. counts) of alleles for each individual in slot A. b. Aggregate to ecopop Most of our analysis for this lab will be at the population level. The function ecogen2ecopop aggregates from individual to population-level data. Frogs.ecopop &lt;- ecogen2ecopop(Frogs.ecogen, hier = &quot;SiteName&quot;) Frogs.ecopop ## ## &lt;&lt; ECOPOP CLASS OBJECT &gt;&gt; ## **************************************************************************** ## Access to slots: &lt;ecoslot.&gt; + &lt;name of the slot&gt; + &lt;(name of the object)&gt; ## See: help(&quot;EcoGenetics accessors&quot;) ## **************************************************************************** ## ## # slot XY: # =&gt; 0 x 0 coordinates ## # slot P: # =&gt; 0 x 0 phenotypic variables ## # slot AF: # =&gt; 30 x 43 loci &gt;&gt; ploidy: 2 || codominant ## # slot E: # =&gt; 0 x 0 environmental variables ## # slot S: # =&gt; 30 x 1 population found ## # slot C: # =&gt; 0 x 0 variables ## **************************************************************************** Instead of 413 individuals, we now have data for 30 populations. Slot AF contains the population-level absolute frequencies (counts) of alleles. b. Add site-level data Before importing the genetic diversity, spatial coordinates and site data into the ecopop object, we need to match the rows and extract the data for the sampled populations. Unfortunately, the datasets use different versions of the site name, both were include in the file ralu_loci_allpops.csv and thus in the S slot of Frogs.ecogen. Here, we use group_by and summarize to get the unique set of site names in both versions. We save it as a data frame Subset. Then we join the genetic diversity data. Because the two data frames share a common variable, we dont need to specify the argument by. R will confirm what ID variable it used to join. Subset &lt;- ecoslot.S(Frogs.ecogen) %&gt;% group_by(SiteName, Pop) %&gt;% summarize() ## `summarise()` has grouped output by &#39;SiteName&#39;. You can override using the `.groups` argument. Subset &lt;- left_join(Subset, Frogs.diversity) ## Joining, by = &quot;Pop&quot; R tells us that it used the shared ID variable Pop to join the data. Now we can join the site data. Note: with as.data.frame, we combine the @data and @coords slots of the SpatialPointsDataFrame ralu.site to a single data frame. Subset &lt;- left_join(Subset, as.data.frame(Frogs.coords)) ## Joining, by = &quot;SiteName&quot; This time, R used the shared ID variable SiteName to join the data. Now we have all site-level data in the data frame Subset that has the same row names as Frogs.ecopop. We check the names of the variables to decide which ones to put where. Then we assign them to their respective slots ((C?) for the custom data, here genetic diversity, @E for the environmental data, and @XY for the coordinates). The argument pop specifies the matching ID variable in the @S slot. When we aggregated from Frogs.ecogen to Frogs.ecopop, unfortunately EcoGenetics renamed the variable SiteName to pop. The argument pop_levels identifies the corresponding ID variable in Subset. names(Subset) ## [1] &quot;SiteName&quot; &quot;Pop&quot; &quot;n&quot; &quot;Hobs&quot; &quot;Hexp&quot; &quot;Ar&quot; ## [7] &quot;UTM83_E&quot; &quot;UTM83_N&quot; &quot;Longitude&quot; &quot;Latitude&quot; &quot;Basin&quot; Frogs.ecopop &lt;- EcoGenetics::eco.fill_ecogen_with_df(Frogs.ecopop, pop =&quot;pop&quot;, pop_levels = Subset$SiteName, C = Subset[,3:6], XY = Subset[,7:10]) Frogs.ecopop ## ## &lt;&lt; ECOPOP CLASS OBJECT &gt;&gt; ## **************************************************************************** ## Access to slots: &lt;ecoslot.&gt; + &lt;name of the slot&gt; + &lt;(name of the object)&gt; ## See: help(&quot;EcoGenetics accessors&quot;) ## **************************************************************************** ## ## # slot XY: # =&gt; 30 x 4 coordinates ## # slot P: # =&gt; 0 x 0 phenotypic variables ## # slot AF: # =&gt; 30 x 43 loci &gt;&gt; ploidy: 2 || codominant ## # slot E: # =&gt; 0 x 0 environmental variables ## # slot S: # =&gt; 30 x 1 population found ## # slot C: # =&gt; 30 x 4 variables ## **************************************************************************** c. Export to adegenet and gstudio EcoGenetics provides convenient functions for converting genetic data to and from other packages. Import into genind object (package adegenet): there is a dedicated function, but we need to separately declare the variable that represents the populations and write it into the @pop slot of the genind object. Frogs.genind &lt;- EcoGenetics::ecogen2genind(Frogs.ecogen) ## Loading required package: adegenet ## Loading required package: ade4 ## Registered S3 method overwritten by &#39;spdep&#39;: ## method from ## plot.mst ape ## ## /// adegenet 2.1.3 is loaded //////////// ## ## &gt; overview: &#39;?adegenet&#39; ## &gt; tutorials/doc/questions: &#39;adegenetWeb()&#39; ## &gt; bug reports/feature requests: adegenetIssues() Frogs.genind@pop &lt;- ecoslot.S(Frogs.ecogen)$SiteName For calculating population-level genetic distances, we aggregate the individual-level data to a genpop object with population-level allele frequencies. Frogs.genpop &lt;- adegenet::genind2genpop(Frogs.genind) ## ## Converting data from a genind to a genpop object... ## ## ...done. Frogs.genpop ## /// GENPOP OBJECT ///////// ## ## // 30 populations; 8 loci; 43 alleles; size: 18.2 Kb ## ## // Basic content ## @tab: 30 x 43 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 3-9) ## @loc.fac: locus factor for the 43 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: adegenet::genind2genpop(x = Frogs.genind) ## ## // Optional content ## - empty - Note: Alternatively, we could directly import the ecopop object into a genpop object (adegenet) with EcoGenetics::ecopop2genpop(Frogs.ecopop). However, this can create warnings later on when calculating genetic distances. The object Frogs.genpop has 30 rows, each representing a population. We will also use some functions from the package gstudio, hence we import the individual-level genetic data into gstudio. This should be easy with the function EcoGenetics::ecogen2gstudio. However, there seems to be an issue. The following chunk contains code adapted from within the ecogen2gstudio function, tweaked to work for our data. (You can view the original function with: findMethods(ecogen2gstudio).) #Frogs.gstudio &lt;- EcoGenetics::ecogen2gstudio(Frogs.ecogen, type=&quot;codominant&quot;) dat &lt;- eco.convert(Frogs.ecogen@G, &quot;matrix&quot;, sep.in = &quot;:&quot;, sep.out = &quot;:&quot;) dat &lt;- as.data.frame(dat, stringsAsFactors = FALSE) for (i in 1:ncol(dat)) { class(dat[, i]) &lt;- &quot;locus&quot; } dat[is.na(dat)] &lt;- gstudio::locus(NA) colnames(dat) &lt;- colnames(Frogs.ecogen@G) Frogs.gstudio &lt;- data.frame(ecoslot.S(Frogs.ecogen), dat) tibble::as_tibble(Frogs.gstudio) ## # A tibble: 413 x 10 ## SiteName Pop A B C D E F G H ## &lt;fct&gt; &lt;fct&gt; &lt;locus&gt; &lt;locus&gt; &lt;locu&gt; &lt;locu&gt; &lt;locu&gt; &lt;locu&gt; &lt;locu&gt; &lt;locu&gt; ## 1 AirplaneLa~ Airpla~ 1:1 1:1 1:1 1:1 1:2 1:1 1:1 4:5 ## 2 AirplaneLa~ Airpla~ 2:2 1:1 1:1 1:1 2:2 ## 3 AirplaneLa~ Airpla~ 1:1 1:1 1:1 1:1 3:3 1:1 1:1 3:3 ## 4 AirplaneLa~ Airpla~ 1:1 1:1 2:2 1:2 ## 5 AirplaneLa~ Airpla~ 1:2 1:3 1:1 1:1 1:2 1:1 1:2 4:5 ## 6 AirplaneLa~ Airpla~ 1:2 1:1 1:1 1:3 1:1 1:1 1:2 4:5 ## 7 AirplaneLa~ Airpla~ 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 8 AirplaneLa~ Airpla~ 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 9 AirplaneLa~ Airpla~ 1:3 1:1 1:1 1:1 1:7 1:1 1:1 3:5 ## 10 AirplaneLa~ Airpla~ 2:2 1:3 1:1 1:1 3:7 1:1 1:1 3:3 ## # ... with 403 more rows 3. Calculate genetic distances Here, well calculate a number of different measures of genetic distance, using functions from several packages. Adding the package name to each distance matrix name helps keeping track of methods used. Normally you would not calculate all of them for your own data, and some are redundant, as we will see. Note: Some functions provide an option linearized = TRUE to linearize distances d by calculating d/(1-d). This should result in more linear relationships when plotted or correlated against geographic distance.Here we dont linearize, we can do so later manually. a. Genetic distances calculated from genind object Pairwise Fst with package hierfstat: GD.pop.PairwiseFst.hierfstat &lt;- as.dist(hierfstat::pairwise.neifst(hierfstat::genind2hierfstat(Frogs.genind))) Proportion of shared alleles with package PopGenReport: GD.pop.propShared &lt;- PopGenReport::pairwise.propShared(Frogs.genind) ## Registered S3 method overwritten by &#39;pegas&#39;: ## method from ## print.amova ade4 ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ## Registered S3 methods overwritten by &#39;genetics&#39;: ## method from ## print.locus gstudio ## [.haplotype pegas Several distance matrices with package adegenet: GD.pop.Nei &lt;- adegenet::dist.genpop(Frogs.genpop, method=1) GD.pop.Edwards &lt;- adegenet::dist.genpop(Frogs.genpop, method=2) GD.pop.Reynolds &lt;- adegenet::dist.genpop(Frogs.genpop, method=3) GD.pop.Rogers &lt;- adegenet::dist.genpop(Frogs.genpop, method=4) GD.pop.Provesti &lt;- adegenet::dist.genpop(Frogs.genpop, method=5) Additional distance matrices with package mmod: GD.pop.Joost &lt;- mmod::pairwise_D(Frogs.genind, linearized = FALSE) GD.pop.Hedrick &lt;- mmod::pairwise_Gst_Hedrick(Frogs.genind, linearized = FALSE) GD.pop.NeiGst &lt;- mmod::pairwise_Gst_Nei(Frogs.genind, linearized = FALSE) b. More distance matrices with gstudio GD.pop.Euclidean.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;Euclidean&quot;, stratum=&quot;SiteName&quot;) ## Multilous estimates of Euclidean distance are assumed to be additive. GD.pop.cGD.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;cGD&quot;, stratum=&quot;SiteName&quot;) GD.pop.Nei.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;Nei&quot;, stratum=&quot;SiteName&quot;) GD.pop.Dps.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;Dps&quot;, stratum=&quot;SiteName&quot;) ## Bray distance will be assumed to be entirely additive across loci. GD.pop.Jaccard.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;Jaccard&quot;, stratum=&quot;SiteName&quot;) ## Jaccard distance will be assumed to be entirely additive across loci. c. Assemble distance matrices Well store the population-level genetic distances in a list GD.pop. Note: a few measures return similarities (scaled between 0 and 1) instead of distances. For instance, proporition of shared alleles is 1 if the alleles are identical, and zero of no alleles are shared. We convert these values to distances by subtracting them from 1. GD.pop &lt;- list(pairwiseFst.hierfstat = GD.pop.PairwiseFst.hierfstat, propShared.PopGenReport = 1 - GD.pop.propShared, Nei.adegenet = GD.pop.Nei, Edwards.adegenet = GD.pop.Edwards, Reynolds.adegenet = GD.pop.Reynolds, Rogers.adegenet = GD.pop.Rogers, Provesti.adegenet = GD.pop.Provesti, Joost.mmod = GD.pop.Joost, Hedrick.mmod = GD.pop.Hedrick, Nei.mmod = GD.pop.NeiGst, Euclidean.gstudio = as.dist(GD.pop.Euclidean.gstudio), cGD.gstudio = as.dist(GD.pop.cGD.gstudio), Nei.gstudio = as.dist(GD.pop.Nei.gstudio), Dps.gstudio = as.dist(1 - GD.pop.Dps.gstudio), Jaccard.gstudio = as.dist(1 - GD.pop.Jaccard.gstudio)) round(cor(sapply(GD.pop, function(ls) as.vector(ls))),2)[,1:2] ## pairwiseFst.hierfstat propShared.PopGenReport ## pairwiseFst.hierfstat 1.00 0.79 ## propShared.PopGenReport 0.79 1.00 ## Nei.adegenet 0.85 0.96 ## Edwards.adegenet 0.83 0.93 ## Reynolds.adegenet 0.97 0.82 ## Rogers.adegenet 0.83 0.99 ## Provesti.adegenet 0.79 1.00 ## Joost.mmod 0.90 0.95 ## Hedrick.mmod 0.98 0.88 ## Nei.mmod 1.00 0.77 ## Euclidean.gstudio 0.93 0.95 ## cGD.gstudio 0.50 0.62 ## Nei.gstudio 0.85 0.96 ## Dps.gstudio 0.71 0.90 ## Jaccard.gstudio 0.71 0.90 Consider the correlations printed above (only the first two columns of the correlation matrix are shown). Correlations are high in general, except for conditional genetic distance (cGD) (see Week 13). There are some duplicate measures (with correlation = 1). Note: the following functions calculate distance matrices at the individual level: PopGenReport::gd.smouse() adegenet::propShared() gstudio::genetic_distance(mode = AMOVA) d. Export genetic distance matrices Optional: You can use save to save an R object to your file system, and load to read it in again. Note: the default setting is that save will overwrite existing files with the same name. The code is commented out with #. To run it, remove the #. The first part creates a folder output in your project folder if it does not yet exist. The function save writes the list GD.pop into a file GD.pop.RData, and the function load imports it again. #require(here) #if(!dir.exists(paste0(here::here(),&quot;/output&quot;))) dir.create(paste0(here::here(),&quot;/output&quot;)) #save(GD.pop, file = paste0(here::here(),&quot;/output/GD.pop.RData&quot;)) #load(paste0(here::here(),&quot;/output/GD.pop.RData&quot;)) 4. Perform a Mantel test to test for IBD First, we calculate geographic (Euclidean) distances Dgeo with the dist function, using the spatial coordinates that we imported from ralu.site. These are UTM coordinates and thus metric. Dgeo &lt;- as.vector(dist(ecoslot.XY(Frogs.ecopop)[,1:2])) a. Visually check linearity Before we quantify the linear relationship between genetic and geographic distances, lets check visually whether the relationship is indeed linear. To start, we will define genetic distance Dgen based on proportion of shared alleles. par(mar=c(4,4,0,0)) Dgen &lt;- as.vector(GD.pop$propShared.PopGenReport) dens &lt;- MASS::kde2d(Dgeo, Dgen, n=300) myPal &lt;- colorRampPalette(c(&quot;white&quot;,&quot;blue&quot;,&quot;gold&quot;,&quot;orange&quot;,&quot;red&quot;)) plot(Dgeo, Dgen, pch=20, cex=0.5, xlab=&quot;Geographic Distance&quot;, ylab=&quot;Genetic Distance&quot;) image(dens, col=transp(myPal(300), 0.7), add=TRUE) abline(lm(Dgen ~ Dgeo)) lines(loess.smooth(Dgeo, Dgen), col=&quot;red&quot;) There is clearly an increase of genetic distance with geographic distance. However, the red line, which is a smoothed local mean, indicates that the relationship is not linear. Lets take the natural logarithm of geographic distance: par(mar=c(4,4,0,0)) dens &lt;- MASS::kde2d(log(Dgeo), Dgen, n=300) plot(log(Dgeo), Dgen, pch=20, cex=0.5, xlab=&quot;Geographic Distance&quot;, ylab=&quot;Genetic Distance&quot;) image(dens, col=transp(myPal(300), 0.7), add=TRUE) abline(lm(Dgen ~ log(Dgeo))) lines(loess.smooth(log(Dgeo), Dgen), col=&quot;red&quot;) Questions: Is the relationship between Dgen and Dgeo more linear than before the transformation? What happened to the spread of points along the x axis (geographic distance)? What do the units of the x-axis represent, after the transformation? b. Perform Mantel test Next, we perform a Mantel test with the function mantel from the vegan package. We define Dgen and Dgeo anew, as we need them in dist format this time, not as vectors. Dgen &lt;- GD.pop$propShared.PopGenReport Dgeo &lt;- dist(ecoslot.XY(Frogs.ecopop)[,1:2]) IBD &lt;- vegan::mantel(Dgen,Dgeo, method=&quot;pearson&quot;) IBD ## ## Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## vegan::mantel(xdis = Dgen, ydis = Dgeo, method = &quot;pearson&quot;) ## ## Mantel statistic r: 0.6211 ## Significance: 0.001 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.0852 0.1096 0.1353 0.1597 ## Permutation: free ## Number of permutations: 999 What happens if we take the log of geographic distance, which we saw above helps linearize the relationship for these data? IBD &lt;- vegan::mantel(Dgen,log(Dgeo), method=&quot;pearson&quot;) IBD ## ## Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## vegan::mantel(xdis = Dgen, ydis = log(Dgeo), method = &quot;pearson&quot;) ## ## Mantel statistic r: 0.6839 ## Significance: 0.001 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.0694 0.0926 0.1135 0.1552 ## Permutation: free ## Number of permutations: 999 The statistical significance (p-value) didnt really change, but the strength of the Mantel correlation increased from r = 0.62 to r = 0.68. Instead of transforming variables, we could use Spearman rank correlation to quantify the strength of the association. Rank correlations can be used to quantify and test the strength of curvilinear relationships. IBD &lt;- vegan::mantel(Dgen,Dgeo, method=&quot;spearman&quot;) IBD ## ## Mantel statistic based on Spearman&#39;s rank correlation rho ## ## Call: ## vegan::mantel(xdis = Dgen, ydis = Dgeo, method = &quot;spearman&quot;) ## ## Mantel statistic r: 0.6512 ## Significance: 0.001 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.0735 0.0965 0.1219 0.1576 ## Permutation: free ## Number of permutations: 999 For this measure of genetic diversity, the Mantel correlation was significant, quite strong, and the relationship not linear, hence the transformed data or the rank correlation performed better. What about the other measures? Here we use lapply to apply the function mantel to each genetic distance matrix in GD.pop. Then we use sapply to extract two values for each distance matrix: statistic is the Mantel r statistic (here: Pearson linear correlation), and signif is the p-value. We can find these names with the function attributes (see above). attributes(IBD) ## $names ## [1] &quot;call&quot; &quot;method&quot; &quot;statistic&quot; &quot;signif&quot; &quot;perm&quot; ## [6] &quot;permutations&quot; &quot;control&quot; ## ## $class ## [1] &quot;mantel&quot; Mantel.test &lt;- lapply(GD.pop, function(x) vegan::mantel(x,Dgeo, method=&quot;pearson&quot;)) data.frame(Mantel.r = sapply(Mantel.test, function(x) x$statistic), p.value = sapply(Mantel.test, function(x) x$signif)) ## Mantel.r p.value ## pairwiseFst.hierfstat 0.6238025 0.001 ## propShared.PopGenReport 0.6211335 0.001 ## Nei.adegenet 0.6316224 0.001 ## Edwards.adegenet 0.6639963 0.001 ## Reynolds.adegenet 0.6162305 0.001 ## Rogers.adegenet 0.6188092 0.001 ## Provesti.adegenet 0.6211335 0.001 ## Joost.mmod 0.6556870 0.001 ## Hedrick.mmod 0.6520614 0.001 ## Nei.mmod 0.6147866 0.001 ## Euclidean.gstudio 0.6591863 0.001 ## cGD.gstudio 0.5667087 0.001 ## Nei.gstudio 0.6317948 0.001 ## Dps.gstudio 0.5371591 0.001 ## Jaccard.gstudio 0.5357096 0.001 The nature of the result did not depend on the measure of genetic diversity used. Lets repeat the analysis with log(Dgeo). Mantel.test &lt;- lapply(GD.pop, function(x) ade4::mantel.randtest(x,log(Dgeo))) data.frame(Mantel.r = sapply(Mantel.test, function(x) x$obs), p.value = sapply(Mantel.test, function(x) x$pvalue)) ## Mantel.r p.value ## pairwiseFst.hierfstat 0.6218519 0.001 ## propShared.PopGenReport 0.6839092 0.001 ## Nei.adegenet 0.6547320 0.001 ## Edwards.adegenet 0.7122499 0.001 ## Reynolds.adegenet 0.6437788 0.001 ## Rogers.adegenet 0.6718605 0.001 ## Provesti.adegenet 0.6839092 0.001 ## Joost.mmod 0.6748485 0.001 ## Hedrick.mmod 0.6673508 0.001 ## Nei.mmod 0.6034555 0.001 ## Euclidean.gstudio 0.6967457 0.001 ## cGD.gstudio 0.6008983 0.001 ## Nei.gstudio 0.6548413 0.001 ## Dps.gstudio 0.5911350 0.001 ## Jaccard.gstudio 0.5880305 0.001 Questions: - Did the transformation lead to higher Mantel correlations for all measures of genetic distance used here? - Why was it higher? 5. Create Mantel correlogram for genetic data Lets look at the relationship between genetic distance and geographic distance in a different way, with a Mantel correlogram. Note that this method does not make the assumption of linearity. a. Create a first Mantel correlogram Here, well create a population-level Mantel correlogram with the proportion of shared alleles. Note: The function eco.cormantel has an option latlon=TRUE that takes care of the distance calculation from lat-lon coordinates. To use this option, the coordinates must be in a matrix or data frame with the longitude in the first column and the latitude in the second. Here, we can set we can set latlon=FALSE because the spatial coordinates are in UTM projection. The biological hypothesis of isolation-by-distance postulates that genetic distance increases with geographic distance. Hence it makes sense to use a one-sided alternative. Somewhat counter-intutitively, we use alternative=less to test for positive spatial autocorrelation. corm &lt;- EcoGenetics::eco.cormantel(M = GD.pop$propShared.PopGenReport, XY = ecoslot.XY(Frogs.ecopop)[,1:2], nsim = 199, latlon=FALSE, alternative=&quot;less&quot;, method = &quot;pearson&quot;) ## interval 0 / 10 completed interval 1 / 10 completed interval 2 / 10 completed interval 3 / 10 completed interval 4 / 10 completed interval 5 / 10 completed interval 6 / 10 completed interval 7 / 10 completed interval 8 / 10 completed interval 9 / 10 completed interval 10 / 10 completed corm ## ## ############################ ## Mantel statistic ## ############################ ## ## &gt; Correlation coefficient used --&gt; Pearson ## &gt; Number of simulations --&gt; 199 ## &gt; Random test --&gt; permutation ## &gt; P-adjust method --&gt; holm -sequential: TRUE ## ## &gt; ecoslot.OUT(x): results --&gt; ## ## [[1]] ## d.mean obs exp p.val cardinal ## d= 0 - 1055.243 697.776 0.4815 -0.0058 0.005 45 ## d= 1055.243 - 2110.485 1583.490 0.2708 -0.0029 0.010 67 ## d= 2110.485 - 3165.728 2622.371 0.2043 -0.0043 0.015 64 ## d= 3165.728 - 4220.97 3710.591 -0.1302 0.0040 0.995 64 ## d= 4220.97 - 5276.213 4716.029 -0.1719 0.0086 1.000 54 ## d= 5276.213 - 6331.455 5824.450 -0.2021 -0.0059 1.000 46 ## d= 6331.455 - 7386.698 6856.457 -0.1496 0.0053 1.000 27 ## d= 7386.698 - 8441.941 7883.679 -0.2297 0.0038 1.000 33 ## d= 8441.941 - 9497.183 8882.536 -0.1909 -0.0053 1.000 25 ## d= 9497.183 - 10552.426 9847.000 -0.0807 0.0008 1.000 9 ## ## ## Results table(s) in ecoslot.OUT(x) ## -------------------------------------------------------------------------- ## Access to slots: &lt;ecoslot.&gt; + &lt;name of the slot&gt; + &lt;(name of the object)&gt; ## See: help(&quot;EcoGenetics accessors&quot;) The table shows: Breaks of distance lag d: here in meters (default for lag definition: Sturges rule) Mean distance d.mean: mean distance of pairs in each lag. obs: observed value of the statistic Expected value exp: expected if there is no autocorrelation. P-value p.val: default uses a two-sided permutation test with sequential Holm-Bonferroni adjustement of p-values. Number of pairs cardinal: number of unique pairs per lag. The result corm is an object of class eco.correlog (package: EcoGenetics). A safe way to access thet table is ecoslot.OUT(corm). Lets plot the correlogram: EcoGenetics::eco.plotCorrelog(corm) You can hover over individual points of the correlogram to see the test statistic and mean distance. b. Vary distance class definition Under IBD, at least the first distance lag should show positive spatial autocorrelation. Here, it is the first two classes, as indicated by the red symbols. To what degree does this result depend on the following: The distance lag definition? The measure of genetic distance? Non-linear relationship between genetic and geographic distances? There are several options of the eco.cormantel function to modify the definition of distance classes: int: distance interval in the units of XY smin: minimum class distance in the units of XY smax: maximum class distance in the units of XY nclass: number of classes seqvec: vector with breaks in the unites of XY size: number of individuals per class bin: rule for constructing intervals if no other parameters provided (default: Sturges rule) The easiest ones to modify are either nclass or size. Here we use size to specify that there should be at least 50 or 100 pairs in each distance class. (Note: for a reliable analysis, this should be 100 or more). corm.50 &lt;- EcoGenetics::eco.cormantel(M = GD.pop$propShared.PopGenReport, XY = ecoslot.XY(Frogs.ecopop)[,1:2], nsim = 199, latlon=FALSE, alternative=&quot;less&quot;, size=50) ## interval 0 / 8 completed interval 1 / 8 completed interval 2 / 8 completed interval 3 / 8 completed interval 4 / 8 completed interval 5 / 8 completed interval 6 / 8 completed interval 7 / 8 completed interval 8 / 8 completed EcoGenetics::ecoslot.OUT(corm.50) ## [[1]] ## d.mean obs exp p.val cardinal ## d= 0 - 1152.337 739.966 0.4872 -0.0053 0.005 50 ## d= 1152.337 - 1821.843 1536.052 0.2256 -0.0074 0.010 50 ## d= 1821.843 - 2769.341 2286.134 0.2307 -0.0094 0.015 50 ## d= 2769.341 - 3526.318 3169.188 0.0411 -0.0032 0.195 50 ## d= 3526.318 - 4408.262 3992.072 -0.1714 0.0006 1.000 50 ## d= 4408.262 - 5339.102 4863.176 -0.1579 -0.0010 1.000 50 ## d= 5339.102 - 6668.986 6011.873 -0.2404 0.0024 1.000 50 ## d= 6668.986 - 8434.883 7615.126 -0.2429 -0.0049 1.000 50 corm.100 &lt;- EcoGenetics::eco.cormantel(M = GD.pop$propShared.PopGenReport, XY = ecoslot.XY(Frogs.ecopop)[,1:2], nsim = 199, latlon=FALSE, alternative=&quot;less&quot;, size=100) ## interval 0 / 4 completed interval 1 / 4 completed interval 2 / 4 completed interval 3 / 4 completed interval 4 / 4 completed EcoGenetics::ecoslot.OUT(corm.100) ## [[1]] ## d.mean obs exp p.val cardinal ## d= 0 - 1821.843 1138.009 0.5403 -0.0022 0.005 100 ## d= 1821.843 - 3526.318 2727.661 0.2060 0.0070 0.010 100 ## d= 3526.318 - 5339.102 4427.624 -0.2496 0.0094 1.000 100 ## d= 5339.102 - 8434.883 6813.499 -0.3664 0.0004 1.000 100 EcoGenetics::eco.plotCorrelog(corm.50) EcoGenetics::eco.plotCorrelog(corm.100) Lets compare the observed Mantel r statistic, p-value, number of pairs in the first distance class and their mean distance, as well as the definition of the first lag interval. We can get all of this by extracting the first line from each object. The lag intervals are stored only in the row names, and we need to extract those separately and add them as a colum. Lag1.def &lt;- data.frame(rbind(Sturge = EcoGenetics::ecoslot.OUT(corm)[[1]][1,], size.50 = EcoGenetics::ecoslot.OUT(corm.50)[[1]][1,], size.100 = EcoGenetics::ecoslot.OUT(corm.100)[[1]][1,])) Lag1.def$bin &lt;- c(row.names(EcoGenetics::ecoslot.OUT(corm)[[1]])[1], row.names(EcoGenetics::ecoslot.OUT(corm.50)[[1]])[1], row.names(EcoGenetics::ecoslot.OUT(corm.100)[[1]])[1]) Lag1.def ## d.mean obs exp p.val cardinal bin ## Sturge 697.776 0.4815 -0.0058 0.005 45 d= 0 - 1055.243 ## size.50 739.966 0.4872 -0.0053 0.005 50 d= 0 - 1152.337 ## size.100 1138.009 0.5403 -0.0022 0.005 100 d= 0 - 1821.843 All three distance class definitions (Sturges rule, size = 50, size = 100) resulted in statistically significant p-values. The Mantel correlation in the first distance lag was strongest for size.100. For the first distance class, Sturges rule resulted in 45 pairs and a mean distance similar to size.50 (for this specific dataset). Overall, Sturges rule to define distance classes seems to be a good compromise. What is the trade-off, i.e., what happens if distance lags are defined too narrowly or too widely? It can be really helpful to plot the distribution of distances among the pairs and compare it to the distance intervals: par(mfrow=c(3,1)) hist(Dgeo, nclass=50, main=&quot;Sturge&#39;s rule&quot;, axes=F, xlab=&quot;&quot;, ylab=&quot;&quot;) for(i in 1:length(EcoGenetics::ecoslot.BREAKS(corm))){ lines(rep(EcoGenetics::ecoslot.BREAKS(corm)[i], 2), c(0,50), col=&quot;blue&quot;)} hist(Dgeo, nclass=50, main = &quot;50 pairs per lag&quot;, axes=F) for(i in 1:length(EcoGenetics::ecoslot.BREAKS(corm.50))){ lines(rep(EcoGenetics::ecoslot.BREAKS(corm.50)[i], 2), c(0,50), col=&quot;blue&quot;)} hist(Dgeo, nclass=50, main = &quot;100 pairs per lag&quot;, axes=F) for(i in 1:length(EcoGenetics::ecoslot.BREAKS(corm.100))){ lines(rep(EcoGenetics::ecoslot.BREAKS(corm.100)[i], 2), c(0,50), col=&quot;blue&quot;)} Question: Compare what happens at larger distances. Do you think Sturges rule does a good job for these as well? Unlike a Mantel test, where all pairs are considered, in geostatistics we typically interpret only values for distances up to a certain threshold, e.g. half the maximum distance, for two reasons: There are few pairs in of these bins, making estimates highly variable. Not all pairs contribute (equally) to the largest distance classes (those in the center of the study area are not involved in very large distances). c. Alternative measures of genetic distances Which measure of genetic distance would provide the strongest Mantel correlation in the first distance class for this data set? Here we will cycle through all genetic distance matrices in GD.pop and calculate a Mantel correlogram with Sturges rule (not linearized, method=pearson). This may take a while. Note: the code that calculates corm.GD.pop is included here twice, first commented out, then with the option include=FALSE. This avoids printing out a lot of unnecessary output while still showing the (commented out) code in the .html version of the file. #corm.GD.pop &lt;- lapply(GD.pop, function(x) EcoGenetics::eco.cormantel(M = x, # XY = ecoslot.XY(Frogs.ecopop)[,1:2], nsim = 199, latlon=FALSE, # alternative=&quot;less&quot;)) Next, we extract for each genetic distance matrix the observed value of the Mantel correlation for the first distance class and its p-value. t(sapply(corm.GD.pop, function(x) EcoGenetics::ecoslot.OUT(x)[[1]][1,c(2,4)])) ## obs p.val ## pairwiseFst.hierfstat 0.3908 0.005 ## propShared.PopGenReport 0.4815 0.005 ## Nei.adegenet 0.4175 0.005 ## Edwards.adegenet 0.4782 0.005 ## Reynolds.adegenet 0.4337 0.005 ## Rogers.adegenet 0.4704 0.005 ## Provesti.adegenet 0.4815 0.005 ## Joost.mmod 0.4331 0.005 ## Hedrick.mmod 0.4331 0.005 ## Nei.mmod 0.3698 0.005 ## Euclidean.gstudio 0.4721 0.005 ## cGD.gstudio 0.3647 0.005 ## Nei.gstudio 0.4175 0.005 ## Dps.gstudio 0.4074 0.005 ## Jaccard.gstudio 0.4031 0.005 Compare the p-values: for this dataset, all genetic distance measures resulted in significant spatial autocorrelation (indicating IBD). Lets plot the Mantel correlogram for Nei.adegenet. Statistically significant lags are shown in a different color than non-significant ones. EcoGenetics::eco.plotCorrelog(corm.GD.pop$Nei.adegenet) Questions: Starting with the first distance class, how many consecutive distance classes showed significant spatial autocorrelation? At what distance (range), roughly, did the positive autocorrelation disappear? 6. Specify spatial weights and calculate Morans I In this part, well quantify and test Morans I for the genetic diversity data as calculated in Week 3 lab. Note: Above, we used a distance lag approach from geostatistics, here we use spatial neighbours and weights (neighbor topology). Either approach could be used with either type of data. For a detailed tutorial on defining spatial neighbors and weights, see: https://cran.r-project.org/web/packages/adespatial/vignettes/tutorial.html#irregular-samplings a. Defining spatial neighbors The function chooseCN (package: adegenet) provides an interface for choosing a connection network, i.e., for defining spatial neighbors. The underlying functions are defined in package spdep (for defining spatial dependences). It can return the following graph types: Delaunay triangulation (type 1) Gabriel graph (type 2) Relative neighbours (type 3) Minimum spanning tree (type 4) Neighbourhood by distance (type 5) K nearests neighbours (type 6) Inverse distances (type 7) Here we use types 1 - 6 to define neighbors in different ways. Then we plot each graph in geographic space. Lines indicate pairs of sites classified as neighbors. Note: this function expects metric spatial coordinates (e.g., UTM). nb.del &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 1) nb.gab &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 2) nb.rel &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 3) nb.mst &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 4) nb.nbd &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 5, d1=100, d2=15000) nb.4nn &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 6, k = 4) par(mfrow=c(2,3), mai=c(0.1,0.1,0.1, 0.1)) plot(nb.del, coords=ecoslot.XY(Frogs.ecopop)); title(main=&quot;Delaunay&quot;) plot(nb.gab, coords=ecoslot.XY(Frogs.ecopop)); title(main=&quot;Gabriel&quot;) plot(nb.rel, coords=ecoslot.XY(Frogs.ecopop)); title(main= &quot;Rel. neighbors&quot;) plot(nb.mst, coords=ecoslot.XY(Frogs.ecopop)); title(main= &quot;Min spanning tree&quot;) plot(nb.nbd, coords=ecoslot.XY(Frogs.ecopop)); title(main = &quot;Neighbor distance&quot;) plot(nb.4nn, coords=ecoslot.XY(Frogs.ecopop)); title(main = &quot;4 nearest neighbors&quot;) par(mfrow=c(1,1)) For spatial statistics, spatial neighbors are used to calculate a local mean. We want each site to have multiple neighbors, but they should be nearby only. Gabriel graph (type = 2) is often a good option, and well use it for the rest of this worked example. b. Defining spatial weights By default, chooseCN returns row-standardized weights, so that for each site, the weights of its neighbors sum to 1. This means that a local mean can be calculated as a weighted mean of the other sites (non-neighboring sites have a weight of 0). With the function nb2mat we can convert the neighbor object to a matrix of spatial weights. Lets look at the first five lines and columns: spdep::nb2mat(nb.gab)[1:5,1:5] ## [,1] [,2] [,3] [,4] [,5] ## 1 0 0.0000000 0.0000000 0.0000000 0 ## 2 0 0.0000000 0.1666667 0.1666667 0 ## 3 0 0.2500000 0.0000000 0.0000000 0 ## 4 0 0.3333333 0.0000000 0.0000000 0 ## 5 0 0.0000000 0.0000000 0.0000000 0 Each row contains the weights for the neighbors of one site. We see that the third site is a neighbor of the second site and vice versa. However, the weights are not the same. It seems that site 2 has six neighbors, so each has a weight of 0.167, whereas site 3 has four neighbors, each with a weight of 0.25. c. Calculating and testing Morans I Lets see whether rarefied allelic richness, Ar, shows spatial autocorrelation among these sites. spdep::moran.test(ecoslot.C(Frogs.ecopop)$Ar, spdep::nb2listw(nb.gab), alternative=&quot;greater&quot;) ## ## Moran I test under randomisation ## ## data: ecoslot.C(Frogs.ecopop)$Ar ## weights: spdep::nb2listw(nb.gab) ## ## Moran I statistic standard deviate = 4.648, p-value = 1.676e-06 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.60990344 -0.03448276 0.01921995 The test statistic is 0.61, and the p-value for a one-sided alternative greater (i.e., positive spatial autocorrelation) is p &lt; 0.0001. Thus, Ar showed strong and statistially significant spatial autocorrelation. Lets do this for all genetic diversity variables and extract the value of the Moran I statistics (for first neighbors) and its p-value. Frogs.moran &lt;- lapply(ecoslot.C(Frogs.ecopop), function(x) spdep::moran.test(x, spdep::nb2listw(nb.gab), alternative=&quot;two.sided&quot;)) round(data.frame(obs = sapply(Frogs.moran, function(x) as.vector(x$estimate[1])), p.value = sapply(Frogs.moran, function(x) x$p.value)),3) ## obs p.value ## n -0.109 0.606 ## Hobs 0.507 0.000 ## Hexp 0.551 0.000 ## Ar 0.610 0.000 Questions: Which measure of genetic diversity showed the strongest autocorrelation? Sample size n had a negative value of Morans I (obs). What does this mean? Did all four variables show statistically significant spatial autocorrelation? "],["r-exercise-week-5.html", "R Exercise Week 5", " R Exercise Week 5 Task: Assess fine-scale spatial genetic structure in Pulsatilla vulgaris within a single patch, A25: Test for IBD with Mantel rank correlation, and use a Mantel correlogram to assess the range of spatial autocorrelation. Hints: Load packages: You may want to load the packages dplyr, EcoGenetics and adegenet. Alternatively, you can use :: to call functions from packages. Import data, extract adults from A25. Use the code below to import the data into gstudio. Inspect a few rows of the data. Filter the dataset to retain adults (OffID == 0) from site A25 (Population == A25). Pulsatilla.gstudio &lt;- gstudio::read_population(path=system.file(&quot;extdata&quot;, &quot;pulsatilla_genotypes.csv&quot;, package = &quot;LandGenCourse&quot;), type=&quot;column&quot;, locus.columns=c(6:19), phased=FALSE, sep=&quot;,&quot;, header=TRUE) Plot locations of individuals: use plot with the argument asp = 1 to plot the locations of the sampled individuals from site A25 to scale. Convert to ecogen and genind: use EcoGenetics::gstudio2ecogen to convert to an ecogen object. From there, use EcoGenetics::ecogen2genind to convert to a genind object. Calculate genetic and Euclidean distance: Use adegenet::propShared to calculate the proportion of shared alleles at the individual level. (Do not convert to genpop). Convert to a distance measure by calculating 1 - propShared. Check object class. If it is not dist, use as.dist to convert to a dist object. Use dist to calculate Euclidean distance between individuals. Mantel test: Adapt code from section 4.a to plot pairwise genetic distance against Euclidean distance. Do you notice something unusial in the plot? Why are there so few different values of genetic distance? Do you think there is spatial autocorrelation? If so, up to what distance? Adapt code from section 4.b to test for IBD with a Mantel test, using Spearman rank correlation. Mantel correlogram: adapt code from section 5.a to create and plot a Mantel correlogram. Interpret the results. Questions: What is the range of spatial autocorrelation in P. vulgaris in site A25? Based on a plot of genetic distance against Euclidean distance? Based on where the Mantel correlogram reaches 0? Based on statistical significance tests for the Mantel correlogram (with default settings: one-sided alternative less, Holms adjustment)? "],["Week6.html", "Week 6: Quantitative Genetics ", " Week 6: Quantitative Genetics "],["video-6.html", "View Course Video", " View Course Video Video, Part 1 Video, Part 2 Preview Slides Download "],["tutorial-6.html", "Interactive Tutorial 6", " Interactive Tutorial 6 List of R commands used Function Package str utils lmer lme4 geom_boxplot ggplot2 facet_wrap ggplot2 geom_line ggplot2 stat_summary ggplot2 geom_errorbar ggplot2 theme_bw ggplot2 Instructions a) How to access tutorials: Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (Landscape_Genetics_R_Course) and tutorial (Weeks 1 - 8) b) How to complete tutorial: Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: MyName2) c) How to submit answers (participating institutions only): The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose yes, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use skip). If you used skip because you could not answer a question, please contact your instructor for advice. "],["WE-6.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals Justification: Natural selection acts on the heritable component of the total phenotypic variation. Traditionally, the heritable proportion of trait variation is estimated using information on the degree of relatedness between individuals. This approach - extensively employed in breeding and evolutionary quantitative genetics  ignores the molecular underpinnings of traits. An alternative approach is scanning genomes using molecular markers to search for so-called outlier loci (see week 11), where selection invoked a change in allele frequencies that is detectable as unusual differentiation between populations in different environments. This approach  advocated in population genetics - neglects information about the trait variation, but assumes that the markers are in or close to genes that code for adaptive traits. Genetic outliers often hard to interpret because generally we lack information what phenotype they affect and if this phenotype results in fitness differences between populations. Learning Objectives: This lab was constructed to give you experience in working with basic quantitative and population genetic analyses to test hypotheses about the presence of local adaptation. Similar to the genomic revolution, phenotyping is also undergoing a revolution thanks to developments e.g. in imagery that allow for a highly automated recording of traits; a task that has been extremely time consuming in the past. These developments will certainly advance the quantitative genetic approach in the future in evolutionary biology let alone breeding. By the end of the laboratory, you should be able to do the following: Construct, fit, and assess linear mixed effects models (LMMs) to estimate genetic values and variance components of a phenotypic trait measured in families, e.g. in a common garden experiments. Use LMMs to estimate the heritability of a trait. Test for the presence of unusual trait differentiation among populations relative to differentiation expected in the absence of adaptation Assess if trait differentiation is correlated with differentiation in environmental variables to try identifying the selective component of the environment. b. Data set The data come from a study of western white pine (Pinus monticola Dougl. ex D. Don) sampled around the Lake Tahoe Basin of California and Nevada. These data consist of 157 trees sampled from 10 populations (n = 9 to 22 trees/population). Within each population, trees were sampled within three plots. For each plot, GPS coordinates were collected (i.e. each plot in each population has its own GPS coordinates) and used to generate a set of 7 environmental variables. From these trees, needle tissue was collected from which total DNA was extracted and genotyped for 164 single nucleotide polymorphisms (SNPs). Seeds were also collected and planted in a common garden. The seedlings (n = 5 to 35/tree) were measured for several phenotypic traits. The phenotypic trait we will be working with today is known as the carbon isotope ratio (\\(^{13}C\\)). It is the ratio of two isotopes of carbon (\\(^{13}C\\) and \\(^{12}C\\)) relative to an experimental control, and it is strongly correlated with intrinsic water-use efficiency in plants. Plants need water to live, so it is not a stretch of the imagination to believe that this phenotypic trait has a link with plant fitness. We will thus have the following data: WWP_phenotype_data.txt: Phenotypic measurements for 5 seedlings per tree made in a common garden. WWP.ecogen: an ecogen object (package EcoGenetics) with SNP genotypes for all trees sampled in the field, and with environmental data collected from each plot within each population. c. Required R packages Install some packages needed for this worked example. if(!requireNamespace(&quot;EcoGenetics&quot;, quietly = TRUE)) remotes::install_github(&quot;leandroroser/EcoGenetics-devel&quot;) if(!requireNamespace(&quot;QstFstComp&quot;, quietly = TRUE)) remotes::install_github(&quot;kjgilbert/QstFstComp&quot;) library(LandGenCourse) library(lme4) #require(EcoGenetics) library(methods) library(tibble) #require(lattice) #require(MuMIn) #require(predictmeans) #require(nlme) #require(QstFstComp) #require(car) Package ggeffects not automatically installed with LandGenCourse: if(!require(ggeffects)) install.packages(&quot;ggeffects&quot;, repos=&#39;http://cran.us.r-project.org&#39;) Source two files with additional functions: source(system.file(&quot;extdata&quot;, &quot;supplemental_R_functions.R&quot;, package = &quot;LandGenCourse&quot;)) source(system.file(&quot;extdata&quot;, &quot;panel.cor.r&quot;, package = &quot;LandGenCourse&quot;)) 2. Estimate genetic and non-genetic variance components from a common garden experiment Motivation: A lot of genetics can be carried out without use of any molecular markers. Practitioners of empirical population genetics forget this quite often. A common garden allows us to create a standardized environment in which we minimize the influence of environment on the expression of a particular phenotypic trait. Phenotypic variation can be partitioned to genetic , environmental and residual components based on quantitative genetic theory. Further, we may also test for the presence of interaction between the genetic and environmental variation. The rationale of a common garden is to standardize the environment, thus phenotypic variation we observe is mainly due to genetic variation, even though environmental heterogeneity can never be completely eliminated and we still have to additionally control for it, e.g. using a block design (see below). Goals &amp; Background: The goal for this part of the laboratory is to construct, fit, and assess LMMs for \\(^{13}C\\). We will be using the data in the file named WWP_phenotypic_data.txt. These data are organized in a tab-delimited text file with seedlings grouped by maternal tree (i.e. its mother tree), plot, and population. Also included is an experimental treatment known as block. In a common garden, seedlings from the same maternal tree are randomized among blocks to avoid the influence of micro-environmental variation on expression of phenotypic traits. a. Import phenytypic data phen &lt;- read.delim(system.file(&quot;extdata&quot;, &quot;WWP_phenotype_data.txt&quot;, package = &quot;LandGenCourse&quot;), sep = &quot;\\t&quot;, header = T) tibble::as_tibble(phen) ## # A tibble: 779 x 5 ## population plot family block d13c ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 blk cyn BC1 59 5 -30.2 ## 2 blk cyn BC1 59 2 -29.7 ## 3 blk cyn BC1 59 4 -29.6 ## 4 blk cyn BC1 59 3 -29.2 ## 5 blk cyn BC1 59 1 -29.0 ## 6 blk cyn BC1 60 4 -31.2 ## 7 blk cyn BC1 60 3 -30.7 ## 8 blk cyn BC1 60 1 -30.5 ## 9 blk cyn BC1 60 5 -30.1 ## 10 blk cyn BC1 60 2 -29.3 ## # ... with 769 more rows Check the variable types: family and block have been imported as type integer, and we need to convert them to factor first. phen$family &lt;- as.factor(phen$family) phen$block &lt;- as.factor(phen$block) sapply(phen, class) ## population plot family block d13c ## &quot;character&quot; &quot;character&quot; &quot;factor&quot; &quot;factor&quot; &quot;numeric&quot; b. Fit linear mixed effects models (LMM) to trait data observed on families Now,we are ready to fit a series of linear models. Here we will fit four models in total, though the last two are equivalent for our data: - mod1: a model with only fixed effects (intercept and block), - mod2:a LMM with an fixed effects and a random effect due to family, and - mod3:a LMM where the random effect due to family is nested within population. We will thus be ignoring the variable plot. - mod4:a LMM where the nesting of family within population is not explicitly specified, it is implied by the data. Note: d13c ~ 1 + block is equivalent to d13c ~ block (see video, Part 1). mod1 &lt;- lm(d13c ~ block, data = phen) mod2 &lt;- lme4::lmer(d13c ~ 1 + (1|family) + block,data = phen, REML = TRUE) mod3 &lt;- lme4::lmer(d13c ~ 1 + (1|population/family) + block, data = phen, REML = TRUE) mod4 &lt;- lme4::lmer(d13c ~ 1 + (1|population) + (1|family) + block, data = phen, REML = TRUE) Models mod3 and mod4 produce identical results, as long as each family has its own unique ID. In the following, we will use mod4 as it simplifies interpretation (as commonly used in quantitative genetics). Notice that we are using REML=TRUE, which stands for restricted maximum likelihood. This method is advised for quantitative genetic analysis, because it provides unbiased estimates of the variance components. The other approach would be to use ML or maximum likelihood, however, it generally underestimates the residual variance, thus leading to inflated estimates of the family variance and thus the heritability. ML would be needed e.g. to test fixed effects. c. Compare model fit Now, lets explore which model best fits our data. To do this we will use the Akaike Information Criterion (AIC). This statistic scores model fit while giving a penalty for the number of parameters in a model. The model with the lowest score is the preferred model. How do we ensure that the AIC values of the different models are comparable? For mod1 that was fitted with lm, we use the function AIC. For the other models that were fitted with lmer, we use extractAIC. For model comparison purposes, we have to use the ML fit, as AIC is no longer valid for REML. The function extractAIC refits the models with REML=FALSE to obtain AIC values that are comparable between models with different fixed effects (though this does not apply here because the fixed effects were the same), or between models fitted with lm and lmer. It returns two values, the first is the equivalent degrees of freedom of the fitted model, and the second is the AIC value. Here we only extract the second value. aic_vals &lt;- c(AIC(mod1), extractAIC(mod2)[2], extractAIC(mod3)[2], extractAIC(mod4)[2]) names(aic_vals) &lt;- c(&quot;mod1&quot;,&quot;mod2&quot;,&quot;mod3&quot;, &quot;mod4&quot;) aic_vals ## mod1 mod2 mod3 mod4 ## 2120.987 2080.266 2051.207 2051.207 Hence, model mod3 (and its equivalent mod4) appears to be the best model. This suggests that there are important differences among populations, and among trees within populations. We will learn more about model selection later in the course (Week 12). d. Check model validity Is the model mod4 valid? Lets check the residuals. The function residplot from package predictmeans produces four diagnostic plots for a model fitted with lmer. The option level indicates which random factor should be plotted (here: 1 = family, 2 = population). This function uses the conditional residuals, which represent observed - fitted(fixed) - fitted(random). predictmeans::residplot(mod4, group=&quot;population&quot;, level=1) Hint: You may need to use the arrow symbols to scroll through the plots in the Plot tab in RStudio. The plots are: A normal probability plot of the random effect family: points should follow line. A normal probability plot of (conditional) residuals: points should follow line. A scatterplot of the (conditional) residuals against fitted values: the plot should not thicken (which would indicate differences in variances between groups such as blocks = heteroscedasticity). Due to the option group = population, residuals are colored by population. A plot of fitted values against observed: the scatterplot shows the overall model fit, where points would fall on a line if the model explained 100% of the variation in the response (you can ignore the dashed line). In addition, we may want to look for outliers and influential points. For this, we create two additional plots. Here we use marginal residuals, which are calculated without accounting for random effects: observed - fitted(fixed). A plot of marginal residuals against the fixed effect block: there should not be any large outliers. A plot of Cooks distances, where D &gt; 1 indicates influential points (more relevant when using quantitative predictors (covariates). Note: this may take a while to calculate. First we calculate the marginal residuals by predicting values with fixed effects only (re.form=NA) and subtract these fitted values from the observed values. marginal.residuals &lt;- mod4@frame$d13c - predict(mod4, re.form=NA) plot(mod4@frame$block, marginal.residuals) #predictmeans::CookD(mod4) plot(cooks.distance(mod4)) The residual plots did not indicate any major problem. Hence we can proceed with interpreting the results. e. Estimate variance components How much of the variation in \\(d^{13}C\\) is explained by the fixed and random effects? In models fitted with lm, we use the \\(R^2\\) statistics to describe the total variance in the response explained by all the predictors in the model. In a mixed effects modeling context, there is no longer an obvious definition for \\(R^2\\). Two approximate \\(R^2\\) statistics have been implemented in the function r.squaredGLMM from the package MuMIn (which stands for multi-model inference): R2m: marginal \\(R^2\\). This is the variance explained by fixed factors. R2c: conditional \\(R^2\\). This is interpreted as the variance explained by both fixed and random factors, i.e., the entire model. In both statistics, the denominator is a sum of the fixed-effects variance, the random effect variance, and the residual variance. MuMIn::r.squaredGLMM(mod4) ## Warning: &#39;r.squaredGLMM&#39; now calculates a revised statistic. See the help page. ## R2m R2c ## [1,] 0.02466321 0.2297893 The fixed effect block had a small effect of about 2.5%. The total model explained about 23%, most of which was due to the random effects. How important are the random factors population and family? The summary for mod4 lists the variance components under Random effects. summary(mod4) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: d13c ~ 1 + (1 | population) + (1 | family) + block ## Data: phen ## ## REML criterion at convergence: 2050.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5436 -0.7485 0.0151 0.6028 3.7867 ## ## Random effects: ## Groups Name Variance Std.Dev. ## family (Intercept) 0.08164 0.2857 ## population (Intercept) 0.10859 0.3295 ## Residual 0.71429 0.8452 ## Number of obs: 779, groups: family, 157; population, 10 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -30.62635 0.12666 -241.792 ## block2 -0.13833 0.09667 -1.431 ## block3 -0.35071 0.09520 -3.684 ## block4 -0.10060 0.09538 -1.055 ## block5 -0.39443 0.09651 -4.087 ## ## Correlation of Fixed Effects: ## (Intr) block2 block3 block4 ## block2 -0.374 ## block3 -0.379 0.498 ## block4 -0.378 0.497 0.505 ## block5 -0.374 0.492 0.499 0.496 Here we extract these variance components from the summary and divide by their sum. fam.var &lt;- nlme::VarCorr(mod4)$&quot;family&quot;[1] prov.var &lt;- nlme::VarCorr(mod4)$&quot;population&quot;[1] res.var &lt;- summary(mod4)$sigma^2 Components &lt;- data.frame(fam.var, prov.var, res.var) Components / sum(Components) ## fam.var prov.var res.var ## 1 0.09026004 0.1200531 0.7896869 Compared to Cohens (1988) effect sizes, population and family each had a medium-size effect (&gt; 9% variance explained) on d13c values. f. Significance testing In quantiative genetics, we are more interested in estimating variance components and effects than hypothesis testing. Lets do some testing anyways to see how it works. The confusing part here is that we need to fit the model differentely to test fixed and random effects. For random effects, we can use model mod4 that was fitted with REML=TRUE. The simplest way to test the significance of a random effect is to calculate the model with and without it and use anova() to test whether the more complex model (listed first) explains significantly more than the simpler model. This implements a likelihood ratio test (LR). By default, when used for a model fitted with lmer, anova will refit the models with ML. Here we use the optionrefit=FALSE to prevent this. #mod.noPop &lt;- update(mod4, .~. -(1 | population)) mod.noPop &lt;- lmer(d13c ~ 1 + (1 | family) + block, data=phen, REML=TRUE) mod.noFam &lt;- lmer(d13c ~ 1 + (1 | population) + block, data=phen, REML=TRUE) anova(mod4, mod.noPop, refit=FALSE) ## Data: phen ## Models: ## mod.noPop: d13c ~ 1 + (1 | family) + block ## mod4: d13c ~ 1 + (1 | population) + (1 | family) + block ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## mod.noPop 7 2097.2 2129.8 -1041.6 2083.2 ## mod4 8 2066.4 2103.7 -1025.2 2050.4 32.819 1 1.012e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod4, mod.noFam, refit=FALSE) ## Data: phen ## Models: ## mod.noFam: d13c ~ 1 + (1 | population) + block ## mod4: d13c ~ 1 + (1 | population) + (1 | family) + block ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## mod.noFam 7 2077.4 2110.0 -1031.7 2063.4 ## mod4 8 2066.4 2103.7 -1025.2 2050.4 12.996 1 0.0003122 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can do a similar test for the fixed effecs. Here we have only one effect. mod.noBlock &lt;- lmer(d13c ~ 1 + (1 | population) + (1 | family), data=phen, REML=TRUE) mod.noFam &lt;- lmer(d13c ~ 1 + (1 | population) + block, data=phen, REML=TRUE) anova(mod4, mod.noBlock, refit=TRUE, REML=FALSE) ## refitting model(s) with ML (instead of REML) ## Data: phen ## Models: ## mod.noBlock: d13c ~ 1 + (1 | population) + (1 | family) ## mod4: d13c ~ 1 + (1 | population) + (1 | family) + block ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## mod.noBlock 4 2067.7 2086.3 -1029.8 2059.7 ## mod4 8 2051.2 2088.5 -1017.6 2035.2 24.454 4 6.475e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If we have several fixed effects, it may be more convenient to use function Anova from the car package to perform a Wald chi-square test. However, the model must be fitted with ML. Choose between type II and type II sums of squares (see video). mod4.ML &lt;- lmer(d13c ~ 1 + (1 | population) + (1 | family) + block, data=phen, REML=FALSE) car::Anova(mod4.ML, type=&quot;II&quot;, test.statistic=&quot;Chisq&quot;) ## Registered S3 methods overwritten by &#39;car&#39;: ## method from ## influence.merMod lme4 ## cooks.distance.influence.merMod lme4 ## dfbeta.influence.merMod lme4 ## dfbetas.influence.merMod lme4 ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: d13c ## Chisq Df Pr(&gt;Chisq) ## block 24.905 4 5.257e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3. Estimate trait heritability Motivation: Now that we have learned how to estimate genetic values for \\(^{13}C\\), lets learn how to estimate what fraction of the total variation in trait values is due to genetic effects. More precisely, we shall estimate the heritable proportion of the trait variation. Not all genetic effects are transmitted from one generation to the next, but the so-called additive genetic effects (see Week 6 lecture). Here we have data on half-siblings because seeds come from the same mother tree, but their father is most likely different because our species is wind pollinated. In such family structure, the estimation of the additive genetic variance is straightforward because these analyses provide key information about whether or not local adaptation should even be considered. Remember that local adaptation is about genetically determined phenotypes that vary across environments in responses to differing selective pressures. This step allows us to assess how genetic variation for a phenotypic trait is distributed across the landscape. Goals &amp; Background: The goal for this part of the laboratory is to estimate heritability, trait differentiation, and correlation with environment for trait values determined in Part 1. To do this, we will be using the output from the previous part of the laboratory and the environmental data contained in the file named WWP_environmental_data.txt. As with the phenotype file this is a tab-delimited text file. a. Estimate heritability Lets start with estimating the heritability of \\(^{13}C\\). If you remember from your undergraduate evolution course, heritability refers generally to the proportion of phenotypic variance due to genetic variance. It comes in at least two different versions. The version we are interested in is narrow-sense heritability (\\(h^2\\)), which is defined as the ratio of additive genetic variance to total phenotypic variance: \\[h^{2} = \\frac{\\sigma^{2}_{additive}}{\\sigma^{2}_{total}}\\] We need to extract the variance components from mod3 for all model terms. We do this visually by printing mod3 to screen or using a set of functions applied to mod3. Here, we will do both. mod4 ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: d13c ~ 1 + (1 | population) + (1 | family) + block ## Data: phen ## REML criterion at convergence: 2050.381 ## Random effects: ## Groups Name Std.Dev. ## family (Intercept) 0.2857 ## population (Intercept) 0.3295 ## Residual 0.8452 ## Number of obs: 779, groups: family, 157; population, 10 ## Fixed Effects: ## (Intercept) block2 block3 block4 block5 ## -30.6263 -0.1383 -0.3507 -0.1006 -0.3944 Using the results from above, lets calculate \\(h^2\\). If we assume that the seedlings from each maternal tree are half-siblings (i.e. same mom, but each with a different father) then \\(^2_A = 4 ^2_{family}\\) (so variance due to family). If the seedlings were all full-siblings, then the 4 would be replaced with 2. Lets assume half-siblings. We can then do the following: Copying the values visually from the output above (note that we need to square the standard deviations to get the variances): add_var &lt;- 4*(0.2857^2) total_wp_var &lt;- (0.2857^2) + (0.3295^2) + (0.8452^2) h2 &lt;- add_var/total_wp_var h2 ## [1] 0.3609476 And the same using the variance components from above: one.over.relatedness &lt;- 1/0.25 h2 &lt;- (one.over.relatedness*Components$fam.var) / sum(Components) h2 ## [1] 0.3610402 Question: Inspect your value of \\(h^2\\). What does it mean? We have generated a point estimate for \\(h^2\\). It represents the average \\(h^2\\) across populations after removing the genetic effects due to population differences. To take this further and also get a SE (so that you can construct a confidence interval), you could check out this R package: https://bioconductor.org/packages/release/bioc/html/GeneticsPed.html b. Estimate trait differentiation Great, we have shown that within population genetic variation is statistically greater than zero. What about among population genetic variation? Lets get to that right now. To measure among population genetic variation we will use a statistic known as \\(Q_{ST}\\). It is similar in concept to \\(F_{ST}\\) from population genetics. To estimate \\(Q_{ST}\\), we will use our LMM output again. If we assume that all seedlings are again half-siblings, then: \\[Q_{ST} = \\frac{\\sigma^{2}_{population}} {\\sigma^{2}_{population}+8\\sigma^{2}_{family}}\\] Again, we can do this by copying the values visually from the output: num_qst &lt;- 0.3295^2 dem_qst &lt;- 0.3295^2 + (8*(0.2857^2)) qst &lt;- num_qst/dem_qst qst ## [1] 0.1425618 Or using the variance components from above: num_qst &lt;- Components$prov.var dem_qst &lt;- Components$prov.var + (8*Components$fam.var) qst &lt;- num_qst/dem_qst qst ## [1] 0.1425583 Question: Inspect your qst value: What does it mean? Look at the quantities in the equation above, what is the denominator equal to? Is it the total phenotypic variance or the total genetic variance? 4. Compare \\(Q_{st}\\) to \\(F_{st}\\) measured from SNP data If you want to learn more about analyzing SNP data in R, here are some resources: adegenet SNP tutorial: http://adegenet.r-forge.r-project.org/files/tutorial-genomics.pdf popgen.nescent.org (click on Users tab): http://popgen.nescent.org/ a. Import ecogen object Load the data set WWP.ecogen. data(WWP.ecogen, package=&quot;LandGenCourse&quot;) This ecogen object has information in the following slots: XY: data frame with spatial coordinates (here: longitude, latitude) P: data frame with phenotypic traits (here: intercept, family effects and population effects from mod4) G: data frame with genotypic data (here: 164 SNPs) A: generated automatically from G: matrix of allele counts (codominant markers only) E: data frame with environmental data (here: 7 bioclimatic site variables) S: data frame with structure (hierarchical sampling levels) b. Compare \\(Q_{ST}\\) to \\(F_{ST}\\) See Week 6 Bonus Material for calculating Fst from the SNP data. Here, we will use the QstFstComp library to formally test whether or not \\(Q_{ST} &gt; F_{ST}\\) for \\(^{13}C\\). The function QstFstComp from the package with the same name calculates both \\(Q_{ST}\\) and \\(F_{ST}\\) itself, then tests whethere they are different from each other. Note: as this is a permutation test, if you run it several times, the results may change slightly from run to run. First, however, we need to re-format the data to the format that QstFstComp expects. This is easy with the function ecogen2hierfstat. WWP.hierfstat &lt;- EcoGenetics::ecogen2hierfstat(WWP.ecogen, pop=&#39;population&#39;, to_numeric=TRUE, nout=1) phen_mod &lt;- phen[,-c(2,4)] QstFst_out &lt;- QstFstComp::QstFstComp(fst.dat = WWP.hierfstat, qst.dat = phen_mod, numpops = nlevels(WWP@S$population), nsim = 10000, breeding.design = &quot;half.sib.dam&quot;, dam.offspring.relatedness = 0.25, output = &quot;concise_nowrite&quot;) ## [1] &quot;No output file of Q minus F values written.&quot; QstFst_out ## [[1]] ## Calculated Qst-Fst Lower Bound crit. value.2.5% ## 0.11917632 -0.02709886 ## Upper bound crit. value.97.5% ## 0.06398914 ## ## [[2]] ## [1] &quot;Qst-Fst values output to file: C:/Users/wagnerh1/Desktop/LandGenCourse_book/vignettes/QminusFvalues_2021-05-04_16-27-25.txt&quot; ## ## [[3]] ## Lower one-tailed p value Upper one-tailed p value Two-tailed p value ## 0.9951 0.0049 0.0098 ## ## [[4]] ## Estimated Fst Lower Bound CI.2.5% Upper bound CI.97.5% ## 0.01419871 0.01114991 0.01750789 ## ## [[5]] ## Estimated Qst Lower Bound CI.2.5% Upper bound CI.97.5% ## 0.13337503 0.02526873 0.37597791 ## ## [[6]] ## Va Lower bound CI.2.5% Upper bound CI.97.5% ## 0.3180848 0.1035385 0.5518678 The output contains the following elements: [[1]]: The calculated difference between Qst and Fst, with 95% critical values of the distribution of the distribution under the null hypothesis of no difference. [[2]]: an output file name (though we suppressed the writing of this file with the option output=consise_nowrite) [[3]]: p-values for a hypothesis test with H0: \\(Q_{ST} = F_{ST}\\), with three different alternatives (less, greater, two.sided) [[4]]: the Fst value estimated from the data with 95% confidence intervals. [[5]]: the Qst value estimated from the data with 95% confidence intervals. [[6]]: the additive genetic variance for the trait with 95% confidence intervals. Note: the values are somewhat different from what we calculated, most likely because the function QstFstComp did not account for the block effect. Questions: Inspect the first and third elements of the list QstFst_out: For the observed values, is \\(Q_{ST} &gt; F_{ST}\\)? Which alternative is most appropriate here? Can the null hypothesis be rejected with alpha = 0.05? What does this mean biologically? 5. Assess correlation between trait and environment Motivation: Similar to the fixed effects, estimates of the random effects can be also extracted from a mixed effects model and used in further analysis. Such estimates are called Best Linear Unbiased Predictions or BLUPs. They are technically predictions because when you fit a mixed effects model, initially only the mean and variance of the random effects are estimated. Then, the random effects or BLUPs for a given experimental unit (e.g., population or family) can be predicted from the estimated mean and variance (and the data). Goals and background: Here we will extract block, population and family effects from the model. Then we will correlate the plot-level bioclimatic data with the population effects to see if population differences in seedling d13C trait can be explained by the environment at the site where the mother tree stands. The underlying idea is to look if selection has made the population different. a. Describe and extract effects Which blocks, populations or trees had higher mean d13c values than others? To answer this question, we extract and plot the fixed and random effects. Well start with the fixed effect for block. The effects are not simply the mean of all seedlings from the same block, but the marginal effect of block, i.e., estimates of the mean per block after accounting for all other effects (i.e., keeping population and family constant). There are two ways to extract the block effects. The first is with function fixef. This treats the first level of block as a reference level and uses it to estimate the global intercept, then expresses the effect of the other blocks as deviations from the first block. lme4::fixef(mod4) ## (Intercept) block2 block3 block4 block5 ## -30.6263478 -0.1383311 -0.3507053 -0.1006031 -0.3944314 A more convenient way to extract the effects is with function ggeffect of the package ggeffects. Note: The package ggeffects is not automatically imported with LandGenCourse to avoid incompatibility issues. You can install by runing install.packages(ggeffects) below without #. ## i.stall.packages(&quot;ggeffects&quot;) ggeffects::ggeffect(mod4, terms=c(&quot;block&quot;)) ## Package `effects` is not available, but needed for `ggeffect()`. Either install package `effects`, or use `ggpredict()`. Calling `ggpredict()` now.FALSE ## # Predicted values of d13c ## ## block | Predicted | 95% CI ## ------------------------------------ ## 1 | -30.63 | [-30.87, -30.38] ## 2 | -30.76 | [-31.01, -30.51] ## 3 | -30.98 | [-31.22, -30.73] ## 4 | -30.73 | [-30.97, -30.48] ## 5 | -31.02 | [-31.27, -30.77] ## ## Adjusted for: ## * population = 0 (population-level) ## * family = 0 (population-level) The table lists the predicted d13c value for each block (accounting for population and family) and a 95% confidence interval for each predicted value. Now lets plot the effects (mean with 95% confidence interval) for each random factor, accounting for all other factors. Again, these are not simply means of observed traits values but marginal effect, accounting for all other factors in the model. Note: If the plots appear in the Plot tab in RStudio, use the arrows to scroll between plots. lattice::dotplot(ranef(mod4,condVar=TRUE)) ## $family ## ## $population We can see that there are considerable differences between populations, where blk cyn has the highest, and mt watson the lowest mean. We can extract the values with ranef. Again, they are deviations from the global intercept (block 1). We are now ready to correlate the population effect to the environmental data. prov.eff &lt;- lme4::ranef(mod4)$population fam.eff &lt;- lme4::ranef(mod4)$family prov.eff ## (Intercept) ## armstrong -0.16372457 ## blk cyn 0.56424812 ## echo lk -0.10480714 ## flume 0.37270696 ## hvn -0.25055204 ## incline lk 0.21873458 ## jakes 0.03776884 ## meiss mdw -0.18557714 ## montreal cyn -0.02673127 ## mt watson -0.46206634 head(fam.eff) ## (Intercept) ## 59 0.269660618 ## 60 -0.036688490 ## 61 -0.007035082 ## 63 -0.259031610 ## 64 0.087029419 ## 65 -0.168188771 b. Correlation of population effect with environment We have population effects estimated for 10 populations, and bioclimatic data for 30 plots (3 plots x 10 populations). If we used all 157 rows of the site data set, this would inflate our sample size. To avoid this, we first extract 30 unique rows (one per plot) from the site data WWP.ecogen@E. Then we use match to find for each plot the corresponding population. This returns the row number (index) and we use it to extract the corresponding population effect. Plot.data &lt;- dplyr::distinct(data.frame(WWP.ecogen@E, population=WWP.ecogen@S$population)) index &lt;- match(Plot.data$population, row.names(prov.eff)) dat2 &lt;- data.frame(prov.eff=prov.eff[index,1], Plot.data[,-8]) Now we estimate the correlation between the population effect and each bioclimatic variable. We summarize the results in a correlation matrix plot. pairs(dat2, lower.panel=panel.smooth, upper.panel=panel.cor, diag.panel=panel.hist) How to read this plot: Each diagonal cell shows the name and histogram of one variable. Each cell in the upper triangle shows the correlation coefficient r, and the p-value for the correlation, between the two variables that define the corresponding row and column. (A larger font indicates a stronger correlation, though this seems to be inconsistent). Each cell in the lower triangle shows a scatterplot of the two variables that define the corresponding row and column. A smooth regression line is shown in red. Questions: Which site variables show the strongest correlation with the population effect? Which site variables are significantly correlated with the population effect? OVerall, does the population effect appear to be related to the environment? "],["r-exercise-week-6.html", "R Exercise Week 6", " R Exercise Week 6 Task: Test whether observed heterozygosity of Pulsatilla vulgaris adults depends on census population size. Fit a model at the individual level where you include a random effect for population. Hints: Load packages: Please install the package inbreedR (to calculate individual measures of heterozygosity) from CRAN, if it is not yet installed. You may want to load the packages dplyr and ggplot2. Alternatively, you can use :: to call functions from packages. Import data and extract adults. Use the code below to import the data. Use dplyr::filter to extract adults with OffID == 0. Pulsatilla &lt;- read.csv(system.file(\"extdata\",\"pulsatilla_genotypes.csv\", package = \"LandGenCourse\")) Calculate multilocus heterozygosity: Use package inbreedR to calculate multilocus heterozygosity for each adult. Use the function inbreedR::convert_raw(x), where x is the matrix of genotypes only (no ID or other non-genetic data), with two columns per locus. Check the help file of the function convert_raw. Use the function inbreedR::MLH to calculate observed heterozygosity for each individual. Add the result as a variable het to the adults dataset. Example code from inbreedR::MLH help file: data(mouse_msats) genotypes &lt;- convert_raw(mouse_msats) het &lt;- MLH(genotypes) Add population-level data: Import the file pulsatilla_population.csv with the code below. Do a left-join to add the data to your adults dataset. Check the dataset. Pop.data &lt;- read.csv(system.file(\"extdata\", \"pulsatilla_population.csv\", package = \"LandGenCourse\")) Scatterplot with regression line: Use ggplot2 to create a scatterplot of adult heterozygosity against census population size (population.size), with a regression line. Fit linear mixed model: Adapt code from section 3.c to perform a regression of individual-level observed heterozygosity (response variable) on census population size (predictor), including population as a random effect. Fit the model with REML and print a summary. Test fixed effect: Adapt code from section 2.f to test the fixed effect with function car::Anova. Check residual plots: Adapt code from section 2.d to create residual plots. Questions: There is one influential point in the regression analysis: What was the direction of the relationship, did heterozygosity increase or decrease with census population size? Was the fixed effect statistically significant? Was the model valid, or was there a problem with the residual plots? What would be the main issue, and what remedy could you suggest? "],["Week7.html", "Week 7: Spatial Linear Models ", " Week 7: Spatial Linear Models "],["video-7.html", "View Course Video", " View Course Video Video, Part 1 Video, Part 2 Preview Slides Download "],["tutorial-7.html", "Interactive Tutorial 7", " Interactive Tutorial 7 List of R commands used Function Package gabrielneigh spdep graph2nb spdep log base coord_trans ggplot2 par(mfrow) graphics par(mar) graphics residuals stats fitted stats geom_histogram ggplot2 lm.morantest spdep errorsarlm spdep as.data.frame base gls nlme Variogram nlme corExp nlme update stats Instructions a) How to access tutorials: Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (Landscape_Genetics_R_Course) and tutorial (Weeks 1 - 8) b) How to complete tutorial: Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: MyName2) c) How to submit answers (participating institutions only): The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose yes, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use skip). If you used skip because you could not answer a question, please contact your instructor for advice. "],["WE-7.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals Justification: Natural selection acts on the heritable component of the total phenotypic variation. Traditionally, the heritable proportion of trait variation is estimated using information on the degree of relatedness between individuals. This approach - extensively employed in breeding and evolutionary quantitative genetics  ignores the molecular underpinnings of traits. An alternative approach is scanning genomes using molecular markers to search for so-called outlier loci (see week 11), where selection invoked a change in allele frequencies that is detectable as unusual differentiation between populations in different environments. This approach  advocated in population genetics - neglects information about the trait variation, but assumes that the markers are in or close to genes that code for adaptive traits. Genetic outliers often hard to interpret because generally we lack information what phenotype they affect and if this phenotype results in fitness differences between populations. Learning Objectives: This lab was constructed to give you experience in working with basic quantitative and population genetic analyses to test hypotheses about the presence of local adaptation. Similar to the genomic revolution, phenotyping is also undergoing a revolution thanks to developments e.g. in imagery that allow for a highly automated recording of traits; a task that has been extremely time consuming in the past. These developments will certainly advance the quantitative genetic approach in the future in evolutionary biology let alone breeding. By the end of the laboratory, you should be able to do the following: Construct, fit, and assess linear mixed effects models (LMMs) to estimate genetic values and variance components of a phenotypic trait measured in families, e.g. in a common garden experiments. Use LMMs to estimate the heritability of a trait. Test for the presence of unusual trait differentiation among populations relative to differentiation expected in the absence of adaptation Assess if trait differentiation is correlated with differentiation in environmental variables to try identifying the selective component of the environment. b. Data set The data come from a study of western white pine (Pinus monticola Dougl. ex D. Don) sampled around the Lake Tahoe Basin of California and Nevada. These data consist of 157 trees sampled from 10 populations (n = 9 to 22 trees/population). Within each population, trees were sampled within three plots. For each plot, GPS coordinates were collected (i.e. each plot in each population has its own GPS coordinates) and used to generate a set of 7 environmental variables. From these trees, needle tissue was collected from which total DNA was extracted and genotyped for 164 single nucleotide polymorphisms (SNPs). Seeds were also collected and planted in a common garden. The seedlings (n = 5 to 35/tree) were measured for several phenotypic traits. The phenotypic trait we will be working with today is known as the carbon isotope ratio (\\(^{13}C\\)). It is the ratio of two isotopes of carbon (\\(^{13}C\\) and \\(^{12}C\\)) relative to an experimental control, and it is strongly correlated with intrinsic water-use efficiency in plants. Plants need water to live, so it is not a stretch of the imagination to believe that this phenotypic trait has a link with plant fitness. We will thus have the following data: WWP_phenotype_data.txt: Phenotypic measurements for 5 seedlings per tree made in a common garden. WWP.ecogen: an ecogen object (package EcoGenetics) with SNP genotypes for all trees sampled in the field, and with environmental data collected from each plot within each population. c. Required R packages Install some packages needed for this worked example. if(!requireNamespace(&quot;EcoGenetics&quot;, quietly = TRUE)) remotes::install_github(&quot;leandroroser/EcoGenetics-devel&quot;) if(!requireNamespace(&quot;QstFstComp&quot;, quietly = TRUE)) remotes::install_github(&quot;kjgilbert/QstFstComp&quot;) library(LandGenCourse) library(lme4) #require(EcoGenetics) library(methods) library(tibble) #require(lattice) #require(MuMIn) #require(predictmeans) #require(nlme) #require(QstFstComp) #require(car) Package ggeffects not automatically installed with LandGenCourse: if(!require(ggeffects)) install.packages(&quot;ggeffects&quot;, repos=&#39;http://cran.us.r-project.org&#39;) Source two files with additional functions: source(system.file(&quot;extdata&quot;, &quot;supplemental_R_functions.R&quot;, package = &quot;LandGenCourse&quot;)) source(system.file(&quot;extdata&quot;, &quot;panel.cor.r&quot;, package = &quot;LandGenCourse&quot;)) 2. Estimate genetic and non-genetic variance components from a common garden experiment Motivation: A lot of genetics can be carried out without use of any molecular markers. Practitioners of empirical population genetics forget this quite often. A common garden allows us to create a standardized environment in which we minimize the influence of environment on the expression of a particular phenotypic trait. Phenotypic variation can be partitioned to genetic , environmental and residual components based on quantitative genetic theory. Further, we may also test for the presence of interaction between the genetic and environmental variation. The rationale of a common garden is to standardize the environment, thus phenotypic variation we observe is mainly due to genetic variation, even though environmental heterogeneity can never be completely eliminated and we still have to additionally control for it, e.g. using a block design (see below). Goals &amp; Background: The goal for this part of the laboratory is to construct, fit, and assess LMMs for \\(^{13}C\\). We will be using the data in the file named WWP_phenotypic_data.txt. These data are organized in a tab-delimited text file with seedlings grouped by maternal tree (i.e. its mother tree), plot, and population. Also included is an experimental treatment known as block. In a common garden, seedlings from the same maternal tree are randomized among blocks to avoid the influence of micro-environmental variation on expression of phenotypic traits. a. Import phenytypic data phen &lt;- read.delim(system.file(&quot;extdata&quot;, &quot;WWP_phenotype_data.txt&quot;, package = &quot;LandGenCourse&quot;), sep = &quot;\\t&quot;, header = T) tibble::as_tibble(phen) ## # A tibble: 779 x 5 ## population plot family block d13c ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 blk cyn BC1 59 5 -30.2 ## 2 blk cyn BC1 59 2 -29.7 ## 3 blk cyn BC1 59 4 -29.6 ## 4 blk cyn BC1 59 3 -29.2 ## 5 blk cyn BC1 59 1 -29.0 ## 6 blk cyn BC1 60 4 -31.2 ## 7 blk cyn BC1 60 3 -30.7 ## 8 blk cyn BC1 60 1 -30.5 ## 9 blk cyn BC1 60 5 -30.1 ## 10 blk cyn BC1 60 2 -29.3 ## # ... with 769 more rows Check the variable types: family and block have been imported as type integer, and we need to convert them to factor first. phen$family &lt;- as.factor(phen$family) phen$block &lt;- as.factor(phen$block) sapply(phen, class) ## population plot family block d13c ## &quot;character&quot; &quot;character&quot; &quot;factor&quot; &quot;factor&quot; &quot;numeric&quot; b. Fit linear mixed effects models (LMM) to trait data observed on families Now,we are ready to fit a series of linear models. Here we will fit four models in total, though the last two are equivalent for our data: - mod1: a model with only fixed effects (intercept and block), - mod2:a LMM with an fixed effects and a random effect due to family, and - mod3:a LMM where the random effect due to family is nested within population. We will thus be ignoring the variable plot. - mod4:a LMM where the nesting of family within population is not explicitly specified, it is implied by the data. Note: d13c ~ 1 + block is equivalent to d13c ~ block (see video, Part 1). mod1 &lt;- lm(d13c ~ block, data = phen) mod2 &lt;- lme4::lmer(d13c ~ 1 + (1|family) + block,data = phen, REML = TRUE) mod3 &lt;- lme4::lmer(d13c ~ 1 + (1|population/family) + block, data = phen, REML = TRUE) mod4 &lt;- lme4::lmer(d13c ~ 1 + (1|population) + (1|family) + block, data = phen, REML = TRUE) Models mod3 and mod4 produce identical results, as long as each family has its own unique ID. In the following, we will use mod4 as it simplifies interpretation (as commonly used in quantitative genetics). Notice that we are using REML=TRUE, which stands for restricted maximum likelihood. This method is advised for quantitative genetic analysis, because it provides unbiased estimates of the variance components. The other approach would be to use ML or maximum likelihood, however, it generally underestimates the residual variance, thus leading to inflated estimates of the family variance and thus the heritability. ML would be needed e.g. to test fixed effects. c. Compare model fit Now, lets explore which model best fits our data. To do this we will use the Akaike Information Criterion (AIC). This statistic scores model fit while giving a penalty for the number of parameters in a model. The model with the lowest score is the preferred model. How do we ensure that the AIC values of the different models are comparable? For mod1 that was fitted with lm, we use the function AIC. For the other models that were fitted with lmer, we use extractAIC. For model comparison purposes, we have to use the ML fit, as AIC is no longer valid for REML. The function extractAIC refits the models with REML=FALSE to obtain AIC values that are comparable between models with different fixed effects (though this does not apply here because the fixed effects were the same), or between models fitted with lm and lmer. It returns two values, the first is the equivalent degrees of freedom of the fitted model, and the second is the AIC value. Here we only extract the second value. aic_vals &lt;- c(AIC(mod1), extractAIC(mod2)[2], extractAIC(mod3)[2], extractAIC(mod4)[2]) names(aic_vals) &lt;- c(&quot;mod1&quot;,&quot;mod2&quot;,&quot;mod3&quot;, &quot;mod4&quot;) aic_vals ## mod1 mod2 mod3 mod4 ## 2120.987 2080.266 2051.207 2051.207 Hence, model mod3 (and its equivalent mod4) appears to be the best model. This suggests that there are important differences among populations, and among trees within populations. We will learn more about model selection later in the course (Week 12). d. Check model validity Is the model mod4 valid? Lets check the residuals. The function residplot from package predictmeans produces four diagnostic plots for a model fitted with lmer. The option level indicates which random factor should be plotted (here: 1 = family, 2 = population). This function uses the conditional residuals, which represent observed - fitted(fixed) - fitted(random). predictmeans::residplot(mod4, group=&quot;population&quot;, level=1) Hint: You may need to use the arrow symbols to scroll through the plots in the Plot tab in RStudio. The plots are: A normal probability plot of the random effect family: points should follow line. A normal probability plot of (conditional) residuals: points should follow line. A scatterplot of the (conditional) residuals against fitted values: the plot should not thicken (which would indicate differences in variances between groups such as blocks = heteroscedasticity). Due to the option group = population, residuals are colored by population. A plot of fitted values against observed: the scatterplot shows the overall model fit, where points would fall on a line if the model explained 100% of the variation in the response (you can ignore the dashed line). In addition, we may want to look for outliers and influential points. For this, we create two additional plots. Here we use marginal residuals, which are calculated without accounting for random effects: observed - fitted(fixed). A plot of marginal residuals against the fixed effect block: there should not be any large outliers. A plot of Cooks distances, where D &gt; 1 indicates influential points (more relevant when using quantitative predictors (covariates). Note: this may take a while to calculate. First we calculate the marginal residuals by predicting values with fixed effects only (re.form=NA) and subtract these fitted values from the observed values. marginal.residuals &lt;- mod4@frame$d13c - predict(mod4, re.form=NA) plot(mod4@frame$block, marginal.residuals) #predictmeans::CookD(mod4) plot(cooks.distance(mod4)) The residual plots did not indicate any major problem. Hence we can proceed with interpreting the results. e. Estimate variance components How much of the variation in \\(d^{13}C\\) is explained by the fixed and random effects? In models fitted with lm, we use the \\(R^2\\) statistics to describe the total variance in the response explained by all the predictors in the model. In a mixed effects modeling context, there is no longer an obvious definition for \\(R^2\\). Two approximate \\(R^2\\) statistics have been implemented in the function r.squaredGLMM from the package MuMIn (which stands for multi-model inference): R2m: marginal \\(R^2\\). This is the variance explained by fixed factors. R2c: conditional \\(R^2\\). This is interpreted as the variance explained by both fixed and random factors, i.e., the entire model. In both statistics, the denominator is a sum of the fixed-effects variance, the random effect variance, and the residual variance. MuMIn::r.squaredGLMM(mod4) ## Warning: &#39;r.squaredGLMM&#39; now calculates a revised statistic. See the help page. ## R2m R2c ## [1,] 0.02466321 0.2297893 The fixed effect block had a small effect of about 2.5%. The total model explained about 23%, most of which was due to the random effects. How important are the random factors population and family? The summary for mod4 lists the variance components under Random effects. summary(mod4) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: d13c ~ 1 + (1 | population) + (1 | family) + block ## Data: phen ## ## REML criterion at convergence: 2050.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5436 -0.7485 0.0151 0.6028 3.7867 ## ## Random effects: ## Groups Name Variance Std.Dev. ## family (Intercept) 0.08164 0.2857 ## population (Intercept) 0.10859 0.3295 ## Residual 0.71429 0.8452 ## Number of obs: 779, groups: family, 157; population, 10 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -30.62635 0.12666 -241.792 ## block2 -0.13833 0.09667 -1.431 ## block3 -0.35071 0.09520 -3.684 ## block4 -0.10060 0.09538 -1.055 ## block5 -0.39443 0.09651 -4.087 ## ## Correlation of Fixed Effects: ## (Intr) block2 block3 block4 ## block2 -0.374 ## block3 -0.379 0.498 ## block4 -0.378 0.497 0.505 ## block5 -0.374 0.492 0.499 0.496 Here we extract these variance components from the summary and divide by their sum. fam.var &lt;- nlme::VarCorr(mod4)$&quot;family&quot;[1] prov.var &lt;- nlme::VarCorr(mod4)$&quot;population&quot;[1] res.var &lt;- summary(mod4)$sigma^2 Components &lt;- data.frame(fam.var, prov.var, res.var) Components / sum(Components) ## fam.var prov.var res.var ## 1 0.09026004 0.1200531 0.7896869 Compared to Cohens (1988) effect sizes, population and family each had a medium-size effect (&gt; 9% variance explained) on d13c values. f. Significance testing In quantiative genetics, we are more interested in estimating variance components and effects than hypothesis testing. Lets do some testing anyways to see how it works. The confusing part here is that we need to fit the model differentely to test fixed and random effects. For random effects, we can use model mod4 that was fitted with REML=TRUE. The simplest way to test the significance of a random effect is to calculate the model with and without it and use anova() to test whether the more complex model (listed first) explains significantly more than the simpler model. This implements a likelihood ratio test (LR). By default, when used for a model fitted with lmer, anova will refit the models with ML. Here we use the optionrefit=FALSE to prevent this. #mod.noPop &lt;- update(mod4, .~. -(1 | population)) mod.noPop &lt;- lmer(d13c ~ 1 + (1 | family) + block, data=phen, REML=TRUE) mod.noFam &lt;- lmer(d13c ~ 1 + (1 | population) + block, data=phen, REML=TRUE) anova(mod4, mod.noPop, refit=FALSE) ## Data: phen ## Models: ## mod.noPop: d13c ~ 1 + (1 | family) + block ## mod4: d13c ~ 1 + (1 | population) + (1 | family) + block ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## mod.noPop 7 2097.2 2129.8 -1041.6 2083.2 ## mod4 8 2066.4 2103.7 -1025.2 2050.4 32.819 1 1.012e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod4, mod.noFam, refit=FALSE) ## Data: phen ## Models: ## mod.noFam: d13c ~ 1 + (1 | population) + block ## mod4: d13c ~ 1 + (1 | population) + (1 | family) + block ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## mod.noFam 7 2077.4 2110.0 -1031.7 2063.4 ## mod4 8 2066.4 2103.7 -1025.2 2050.4 12.996 1 0.0003122 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can do a similar test for the fixed effecs. Here we have only one effect. mod.noBlock &lt;- lmer(d13c ~ 1 + (1 | population) + (1 | family), data=phen, REML=TRUE) mod.noFam &lt;- lmer(d13c ~ 1 + (1 | population) + block, data=phen, REML=TRUE) anova(mod4, mod.noBlock, refit=TRUE, REML=FALSE) ## refitting model(s) with ML (instead of REML) ## Data: phen ## Models: ## mod.noBlock: d13c ~ 1 + (1 | population) + (1 | family) ## mod4: d13c ~ 1 + (1 | population) + (1 | family) + block ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## mod.noBlock 4 2067.7 2086.3 -1029.8 2059.7 ## mod4 8 2051.2 2088.5 -1017.6 2035.2 24.454 4 6.475e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If we have several fixed effects, it may be more convenient to use function Anova from the car package to perform a Wald chi-square test. However, the model must be fitted with ML. Choose between type II and type II sums of squares (see video). mod4.ML &lt;- lmer(d13c ~ 1 + (1 | population) + (1 | family) + block, data=phen, REML=FALSE) car::Anova(mod4.ML, type=&quot;II&quot;, test.statistic=&quot;Chisq&quot;) ## Registered S3 methods overwritten by &#39;car&#39;: ## method from ## influence.merMod lme4 ## cooks.distance.influence.merMod lme4 ## dfbeta.influence.merMod lme4 ## dfbetas.influence.merMod lme4 ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: d13c ## Chisq Df Pr(&gt;Chisq) ## block 24.905 4 5.257e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3. Estimate trait heritability Motivation: Now that we have learned how to estimate genetic values for \\(^{13}C\\), lets learn how to estimate what fraction of the total variation in trait values is due to genetic effects. More precisely, we shall estimate the heritable proportion of the trait variation. Not all genetic effects are transmitted from one generation to the next, but the so-called additive genetic effects (see Week 6 lecture). Here we have data on half-siblings because seeds come from the same mother tree, but their father is most likely different because our species is wind pollinated. In such family structure, the estimation of the additive genetic variance is straightforward because these analyses provide key information about whether or not local adaptation should even be considered. Remember that local adaptation is about genetically determined phenotypes that vary across environments in responses to differing selective pressures. This step allows us to assess how genetic variation for a phenotypic trait is distributed across the landscape. Goals &amp; Background: The goal for this part of the laboratory is to estimate heritability, trait differentiation, and correlation with environment for trait values determined in Part 1. To do this, we will be using the output from the previous part of the laboratory and the environmental data contained in the file named WWP_environmental_data.txt. As with the phenotype file this is a tab-delimited text file. a. Estimate heritability Lets start with estimating the heritability of \\(^{13}C\\). If you remember from your undergraduate evolution course, heritability refers generally to the proportion of phenotypic variance due to genetic variance. It comes in at least two different versions. The version we are interested in is narrow-sense heritability (\\(h^2\\)), which is defined as the ratio of additive genetic variance to total phenotypic variance: \\[h^{2} = \\frac{\\sigma^{2}_{additive}}{\\sigma^{2}_{total}}\\] We need to extract the variance components from mod3 for all model terms. We do this visually by printing mod3 to screen or using a set of functions applied to mod3. Here, we will do both. mod4 ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: d13c ~ 1 + (1 | population) + (1 | family) + block ## Data: phen ## REML criterion at convergence: 2050.381 ## Random effects: ## Groups Name Std.Dev. ## family (Intercept) 0.2857 ## population (Intercept) 0.3295 ## Residual 0.8452 ## Number of obs: 779, groups: family, 157; population, 10 ## Fixed Effects: ## (Intercept) block2 block3 block4 block5 ## -30.6263 -0.1383 -0.3507 -0.1006 -0.3944 Using the results from above, lets calculate \\(h^2\\). If we assume that the seedlings from each maternal tree are half-siblings (i.e. same mom, but each with a different father) then \\(^2_A = 4 ^2_{family}\\) (so variance due to family). If the seedlings were all full-siblings, then the 4 would be replaced with 2. Lets assume half-siblings. We can then do the following: Copying the values visually from the output above (note that we need to square the standard deviations to get the variances): add_var &lt;- 4*(0.2857^2) total_wp_var &lt;- (0.2857^2) + (0.3295^2) + (0.8452^2) h2 &lt;- add_var/total_wp_var h2 ## [1] 0.3609476 And the same using the variance components from above: one.over.relatedness &lt;- 1/0.25 h2 &lt;- (one.over.relatedness*Components$fam.var) / sum(Components) h2 ## [1] 0.3610402 Question: Inspect your value of \\(h^2\\). What does it mean? We have generated a point estimate for \\(h^2\\). It represents the average \\(h^2\\) across populations after removing the genetic effects due to population differences. To take this further and also get a SE (so that you can construct a confidence interval), you could check out this R package: https://bioconductor.org/packages/release/bioc/html/GeneticsPed.html b. Estimate trait differentiation Great, we have shown that within population genetic variation is statistically greater than zero. What about among population genetic variation? Lets get to that right now. To measure among population genetic variation we will use a statistic known as \\(Q_{ST}\\). It is similar in concept to \\(F_{ST}\\) from population genetics. To estimate \\(Q_{ST}\\), we will use our LMM output again. If we assume that all seedlings are again half-siblings, then: \\[Q_{ST} = \\frac{\\sigma^{2}_{population}} {\\sigma^{2}_{population}+8\\sigma^{2}_{family}}\\] Again, we can do this by copying the values visually from the output: num_qst &lt;- 0.3295^2 dem_qst &lt;- 0.3295^2 + (8*(0.2857^2)) qst &lt;- num_qst/dem_qst qst ## [1] 0.1425618 Or using the variance components from above: num_qst &lt;- Components$prov.var dem_qst &lt;- Components$prov.var + (8*Components$fam.var) qst &lt;- num_qst/dem_qst qst ## [1] 0.1425583 Question: Inspect your qst value: What does it mean? Look at the quantities in the equation above, what is the denominator equal to? Is it the total phenotypic variance or the total genetic variance? 4. Compare \\(Q_{st}\\) to \\(F_{st}\\) measured from SNP data If you want to learn more about analyzing SNP data in R, here are some resources: adegenet SNP tutorial: http://adegenet.r-forge.r-project.org/files/tutorial-genomics.pdf popgen.nescent.org (click on Users tab): http://popgen.nescent.org/ a. Import ecogen object Load the data set WWP.ecogen. data(WWP.ecogen, package=&quot;LandGenCourse&quot;) This ecogen object has information in the following slots: XY: data frame with spatial coordinates (here: longitude, latitude) P: data frame with phenotypic traits (here: intercept, family effects and population effects from mod4) G: data frame with genotypic data (here: 164 SNPs) A: generated automatically from G: matrix of allele counts (codominant markers only) E: data frame with environmental data (here: 7 bioclimatic site variables) S: data frame with structure (hierarchical sampling levels) b. Compare \\(Q_{ST}\\) to \\(F_{ST}\\) See Week 6 Bonus Material for calculating Fst from the SNP data. Here, we will use the QstFstComp library to formally test whether or not \\(Q_{ST} &gt; F_{ST}\\) for \\(^{13}C\\). The function QstFstComp from the package with the same name calculates both \\(Q_{ST}\\) and \\(F_{ST}\\) itself, then tests whethere they are different from each other. Note: as this is a permutation test, if you run it several times, the results may change slightly from run to run. First, however, we need to re-format the data to the format that QstFstComp expects. This is easy with the function ecogen2hierfstat. WWP.hierfstat &lt;- EcoGenetics::ecogen2hierfstat(WWP.ecogen, pop=&#39;population&#39;, to_numeric=TRUE, nout=1) phen_mod &lt;- phen[,-c(2,4)] QstFst_out &lt;- QstFstComp::QstFstComp(fst.dat = WWP.hierfstat, qst.dat = phen_mod, numpops = nlevels(WWP@S$population), nsim = 10000, breeding.design = &quot;half.sib.dam&quot;, dam.offspring.relatedness = 0.25, output = &quot;concise_nowrite&quot;) ## [1] &quot;No output file of Q minus F values written.&quot; QstFst_out ## [[1]] ## Calculated Qst-Fst Lower Bound crit. value.2.5% ## 0.11917632 -0.02711158 ## Upper bound crit. value.97.5% ## 0.06541427 ## ## [[2]] ## [1] &quot;Qst-Fst values output to file: C:/Users/wagnerh1/Desktop/LandGenCourse_book/vignettes/QminusFvalues_2021-05-04_16-27-41.txt&quot; ## ## [[3]] ## Lower one-tailed p value Upper one-tailed p value Two-tailed p value ## 0.9937 0.0063 0.0126 ## ## [[4]] ## Estimated Fst Lower Bound CI.2.5% Upper bound CI.97.5% ## 0.01419871 0.01116264 0.01741068 ## ## [[5]] ## Estimated Qst Lower Bound CI.2.5% Upper bound CI.97.5% ## 0.13337503 0.02584884 0.36287226 ## ## [[6]] ## Va Lower bound CI.2.5% Upper bound CI.97.5% ## 0.3180848 0.1108536 0.5565961 The output contains the following elements: [[1]]: The calculated difference between Qst and Fst, with 95% critical values of the distribution of the distribution under the null hypothesis of no difference. [[2]]: an output file name (though we suppressed the writing of this file with the option output=consise_nowrite) [[3]]: p-values for a hypothesis test with H0: \\(Q_{ST} = F_{ST}\\), with three different alternatives (less, greater, two.sided) [[4]]: the Fst value estimated from the data with 95% confidence intervals. [[5]]: the Qst value estimated from the data with 95% confidence intervals. [[6]]: the additive genetic variance for the trait with 95% confidence intervals. Note: the values are somewhat different from what we calculated, most likely because the function QstFstComp did not account for the block effect. Questions: Inspect the first and third elements of the list QstFst_out: For the observed values, is \\(Q_{ST} &gt; F_{ST}\\)? Which alternative is most appropriate here? Can the null hypothesis be rejected with alpha = 0.05? What does this mean biologically? 5. Assess correlation between trait and environment Motivation: Similar to the fixed effects, estimates of the random effects can be also extracted from a mixed effects model and used in further analysis. Such estimates are called Best Linear Unbiased Predictions or BLUPs. They are technically predictions because when you fit a mixed effects model, initially only the mean and variance of the random effects are estimated. Then, the random effects or BLUPs for a given experimental unit (e.g., population or family) can be predicted from the estimated mean and variance (and the data). Goals and background: Here we will extract block, population and family effects from the model. Then we will correlate the plot-level bioclimatic data with the population effects to see if population differences in seedling d13C trait can be explained by the environment at the site where the mother tree stands. The underlying idea is to look if selection has made the population different. a. Describe and extract effects Which blocks, populations or trees had higher mean d13c values than others? To answer this question, we extract and plot the fixed and random effects. Well start with the fixed effect for block. The effects are not simply the mean of all seedlings from the same block, but the marginal effect of block, i.e., estimates of the mean per block after accounting for all other effects (i.e., keeping population and family constant). There are two ways to extract the block effects. The first is with function fixef. This treats the first level of block as a reference level and uses it to estimate the global intercept, then expresses the effect of the other blocks as deviations from the first block. lme4::fixef(mod4) ## (Intercept) block2 block3 block4 block5 ## -30.6263478 -0.1383311 -0.3507053 -0.1006031 -0.3944314 A more convenient way to extract the effects is with function ggeffect of the package ggeffects. Note: The package ggeffects is not automatically imported with LandGenCourse to avoid incompatibility issues. You can install by runing install.packages(ggeffects) below without #. ## i.stall.packages(&quot;ggeffects&quot;) ggeffects::ggeffect(mod4, terms=c(&quot;block&quot;)) ## Package `effects` is not available, but needed for `ggeffect()`. Either install package `effects`, or use `ggpredict()`. Calling `ggpredict()` now.FALSE ## # Predicted values of d13c ## ## block | Predicted | 95% CI ## ------------------------------------ ## 1 | -30.63 | [-30.87, -30.38] ## 2 | -30.76 | [-31.01, -30.51] ## 3 | -30.98 | [-31.22, -30.73] ## 4 | -30.73 | [-30.97, -30.48] ## 5 | -31.02 | [-31.27, -30.77] ## ## Adjusted for: ## * population = 0 (population-level) ## * family = 0 (population-level) The table lists the predicted d13c value for each block (accounting for population and family) and a 95% confidence interval for each predicted value. Now lets plot the effects (mean with 95% confidence interval) for each random factor, accounting for all other factors. Again, these are not simply means of observed traits values but marginal effect, accounting for all other factors in the model. Note: If the plots appear in the Plot tab in RStudio, use the arrows to scroll between plots. lattice::dotplot(ranef(mod4,condVar=TRUE)) ## $family ## ## $population We can see that there are considerable differences between populations, where blk cyn has the highest, and mt watson the lowest mean. We can extract the values with ranef. Again, they are deviations from the global intercept (block 1). We are now ready to correlate the population effect to the environmental data. prov.eff &lt;- lme4::ranef(mod4)$population fam.eff &lt;- lme4::ranef(mod4)$family prov.eff ## (Intercept) ## armstrong -0.16372457 ## blk cyn 0.56424812 ## echo lk -0.10480714 ## flume 0.37270696 ## hvn -0.25055204 ## incline lk 0.21873458 ## jakes 0.03776884 ## meiss mdw -0.18557714 ## montreal cyn -0.02673127 ## mt watson -0.46206634 head(fam.eff) ## (Intercept) ## 59 0.269660618 ## 60 -0.036688490 ## 61 -0.007035082 ## 63 -0.259031610 ## 64 0.087029419 ## 65 -0.168188771 b. Correlation of population effect with environment We have population effects estimated for 10 populations, and bioclimatic data for 30 plots (3 plots x 10 populations). If we used all 157 rows of the site data set, this would inflate our sample size. To avoid this, we first extract 30 unique rows (one per plot) from the site data WWP.ecogen@E. Then we use match to find for each plot the corresponding population. This returns the row number (index) and we use it to extract the corresponding population effect. Plot.data &lt;- dplyr::distinct(data.frame(WWP.ecogen@E, population=WWP.ecogen@S$population)) index &lt;- match(Plot.data$population, row.names(prov.eff)) dat2 &lt;- data.frame(prov.eff=prov.eff[index,1], Plot.data[,-8]) Now we estimate the correlation between the population effect and each bioclimatic variable. We summarize the results in a correlation matrix plot. pairs(dat2, lower.panel=panel.smooth, upper.panel=panel.cor, diag.panel=panel.hist) How to read this plot: Each diagonal cell shows the name and histogram of one variable. Each cell in the upper triangle shows the correlation coefficient r, and the p-value for the correlation, between the two variables that define the corresponding row and column. (A larger font indicates a stronger correlation, though this seems to be inconsistent). Each cell in the lower triangle shows a scatterplot of the two variables that define the corresponding row and column. A smooth regression line is shown in red. Questions: Which site variables show the strongest correlation with the population effect? Which site variables are significantly correlated with the population effect? OVerall, does the population effect appear to be related to the environment? "],["r-exercise-week-6-1.html", "R Exercise Week 6", " R Exercise Week 6 Task: Test whether observed heterozygosity of Pulsatilla vulgaris adults depends on census population size. Fit a model at the individual level where you include a random effect for population. Hints: Load packages: Please install the package inbreedR (to calculate individual measures of heterozygosity) from CRAN, if it is not yet installed. You may want to load the packages dplyr and ggplot2. Alternatively, you can use :: to call functions from packages. Import data and extract adults. Use the code below to import the data. Use dplyr::filter to extract adults with OffID == 0. Pulsatilla &lt;- read.csv(system.file(\"extdata\",\"pulsatilla_genotypes.csv\", package = \"LandGenCourse\")) Calculate multilocus heterozygosity: Use package inbreedR to calculate multilocus heterozygosity for each adult. Use the function inbreedR::convert_raw(x), where x is the matrix of genotypes only (no ID or other non-genetic data), with two columns per locus. Check the help file of the function convert_raw. Use the function inbreedR::MLH to calculate observed heterozygosity for each individual. Add the result as a variable het to the adults dataset. Example code from inbreedR::MLH help file: data(mouse_msats) genotypes &lt;- convert_raw(mouse_msats) het &lt;- MLH(genotypes) Add population-level data: Import the file pulsatilla_population.csv with the code below. Do a left-join to add the data to your adults dataset. Check the dataset. Pop.data &lt;- read.csv(system.file(\"extdata\", \"pulsatilla_population.csv\", package = \"LandGenCourse\")) Scatterplot with regression line: Use ggplot2 to create a scatterplot of adult heterozygosity against census population size (population.size), with a regression line. Fit linear mixed model: Adapt code from section 3.c to perform a regression of individual-level observed heterozygosity (response variable) on census population size (predictor), including population as a random effect. Fit the model with REML and print a summary. Test fixed effect: Adapt code from section 2.f to test the fixed effect with function car::Anova. Check residual plots: Adapt code from section 2.d to create residual plots. Questions: There is one influential point in the regression analysis: What was the direction of the relationship, did heterozygosity increase or decrease with census population size? Was the fixed effect statistically significant? Was the model valid, or was there a problem with the residual plots? What would be the main issue, and what remedy could you suggest? "],["Week8.html", "Week 8: Simulation Experiments ", " Week 8: Simulation Experiments "],["video-8.html", "View Course Video", " View Course Video Video, Part 1 Video, Part 2 Preview Slides Download "],["tutorial-8.html", "Interactive Tutorial 8", " Interactive Tutorial 8 List of R commands used Function Package expand.grid base seq base plot(asp=1) graphics coord_fixed ggplot2 rnorm stats runif stats LETTERS base paste base paste0 base sprintf base grep base substr base strsplit base gsub base Instructions a) How to access tutorials: Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (Landscape_Genetics_R_Course) and tutorial (Weeks 1 - 8) b) How to complete tutorial: Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: MyName2) c) How to submit answers (participating institutions only): The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose yes, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use skip). If you used skip because you could not answer a question, please contact your instructor for advice. "],["WE-8.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals This worked example shows: Simulate a metapopulation on a resistance landscape Evaluate the power of a partial Mantel test Compare partial Mantel test to Sunder Run many simulations and synthesize results b. Data set We will simulate data using the landgenreport function of the package PopGenReport. See: www.popgenreport.org c. Required R libraries library(LandGenCourse) library(PopGenReport ) #load the package library(secr) #to create a random habitat #library(gdistance) #library(mmod) library(raster) #library(tibble) #library(here) #library(ggplot2) #library(Sunder) # requires mnormt Package secr not automatically installed with LandGenCourse: if(!require(secr)) install.packages(&quot;secr&quot;, repos=&#39;http://cran.us.r-project.org&#39;) #library(secr) The following setup chunk is used to set the root address of file paths to the root of the project folder. knitr::opts_knit$set(root.dir = normalizePath(&quot;..&quot;)) 2. Initialize a landscape a. Create a random landscape We will use the randomHabitat function from the secr package to create a random habitat map and assign resistance values to habitat and non-habitat. There are many alternative ways to define your map, e.g. simply load a png file or any other file format using the raster function from package raster (?raster::raster, see the examples in there). If your map is categorical, assign resistance values to the different values in the raster as shown below for the missing values. If your map is already a resistance surface, skip this step. Here we use the function set.seed at the beginning of the simulation to make sure we get the same sequence of random numbers everytime we run this code. This makes the exact results reproducible even though we use a random process. The function make.grid here creates a landscape of nx=50 times xy=50 gridpoints spaced 1 unit (meter) apart. This is returned as a data frame tempgrid with two columns that represent x and y grid coordinates. nx=50 ny=50 set.seed(555) #(to make sure we have the same example running) #tempmask&lt;-secr::make.mask(nx=nx,ny=ny,spacing=1) tempgrid&lt;-secr::make.grid(nx=nx,ny=ny,spacing=1) In the function randomHabitat, the argument A specifies the expected proportion of habitat, and p controls the level of fragmentation, i.e., degree of spatial aggregation (sorry this is naming may be a bit confusing, but thats what it says in the help file: ?randomHabitat). The function simulates a map with these parameters and returns a data frame with only those points from tempgrid that are habitat. It expects an input object of class mask (an object type specific to the secr package), hence we pass as.mask(tempgrid). The function raster of the secr package (NOT from the raster package) converts the data frame into a raster object. This object has one numeric variable that is 1 for habitat cells and missing for all other cells, we verify this with table. Note: there may be a warning about non-missing arguments, you can ignore it. #r &lt;- raster(secr::randomHabitat(secr::as.mask(tempgrid), p = 0.5, A = 0.5)) #table(values(r), exclude=&quot;&quot;) NOTE: April 2021: This does not seem to work anymore. Workaround: tmp &lt;- secr::randomHabitat(secr::as.mask(tempgrid), p = 0.5, A = 0.5) r &lt;- as.data.frame(tempgrid) r$resistance &lt;- 10 r$resistance[as.numeric(row.names(tmp))] &lt;- 1 r &lt;- rasterFromXYZ(r) Now we set all missing values (i.e., non-habitat) to 10 and verify this again with table. #values(r)[is.na(values(r))==T]&lt;-10 table(values(r), exclude=&quot;&quot;) ## ## 1 10 ## 1308 1192 par(mar=c(1,1,1,1)) plot(r) We have thus created a numeric raster with a resistance surface where habitat cells (grey) have a resistance value of 1 and non-habitat cells (green) have a resistance value of 10. b. Add populations to the landscape (using minimal distance) We create a function that allows us to set up n subpopulations in the habitat only (grid cells with value = 1). The subpopulations should be at least minDist units apart, given any resistance surface landscape. We also include an option to plot a raster map with the sampled locations of the populations. We define a few variables within the function that help keep track. Note that we keep track of the cells by their raster cell number (which goes from 1:ncells). Heres what the code does: Extract all cells that are habitat and store cell number in HabitatCells. Randomly sample one habitat cell and store its cell number in Selected. Store cell numbers of all remaining habitat cells in Remaining. Create a while loop that continues until one of two things happens: Sample size n is reached. There are no cells left in Remaining. Inside the loop: Randomly sample one habitat cell and store its number in Candidate. Remove the Candidate from Remaining (we dont want to consider it twice). Calculate the Distance between Candidate and all populations in Selected. The function xyFromCell gets the cell coordinates for each cell number, and the function pointDistance calculates the distance between two sets of coordinates, here the coordinates for Candidate and for all cells in Selected. The argument lonlat=FALSE tells pointDistance that the coordinates are Euclidean. If the minimum of Distance is larger than minDist, add a population. This is done by appending the value in Candidate to the vector Selected. Repeat. If requested, the raster map is plotted, cell coordinates for all populations (Selected) are extracted and added to the map as points with point symbol pch=16 (filled circle). createpops &lt;- function(n=10, minDist=5, landscape=r, habitat=1, plot=TRUE) { HabitatCells &lt;- c(1:length(landscape))[values(landscape)==habitat] Selected &lt;- sample(HabitatCells, 1) Remaining &lt;- HabitatCells[!is.element(HabitatCells, Selected)] while (length(Selected) &lt; n &amp; length(Remaining) &gt; 0) { Candidate &lt;- sample(Remaining, 1) Remaining &lt;- Remaining[!is.element(Remaining, Candidate)] Distances &lt;- raster::pointDistance(raster::xyFromCell(landscape, Candidate), raster::xyFromCell(landscape, Selected), lonlat=FALSE) if(min(Distances) &gt; minDist) { Selected &lt;- append(Selected, Candidate) } } if(plot==TRUE) { plot(landscape) points(xyFromCell(landscape, Selected), pch=16) } return(Selected) } Test the function above: par(mar=c(1,1,1,1)) createpops(n=8, minDist = 3, landscape = r, plot = TRUE) ## [1] 1329 477 396 2498 441 692 602 1190 c. Initialise a metapopulation We use the function init.popgensim from package PopGenReport to initialise a metapopulation based on the grid cells that we just selected. To do this we need to initialise a number of parameters (the locations of the subpopulations, the number of individuals per subpopulation, the number of loci and alleles per loci. For a full list check ?init.popgensim). To store all the parameters we create a list called para where we store all of them 3. Define simulation parameters a. Define your metapopulation Define metapopulation: para&lt;- list() #Define populations (dynamics) para$n.pops=8 para$n.ind=100 para$sex.ratio &lt;- 0.5 #age distribution.... para$n.cov &lt;- 3 #number of covariates (before the loci in the data.frame, do not change this!!) Define population dynamics: #reproduction para$n.offspring = 2 #migration para$mig.rate &lt;- 0.1 #dispersal: exponential dispersal with maximal distance in map units para$disp.max=50 #average dispersal of an individual in meters para$disp.rate = 0.05 #proportion of dispersing individuals #Define genetics para$n.allels &lt;- 10 para$n.loci &lt;- 20 para$mut.rate &lt;- 0.001 Define cost distance method: par(mar=c(1,1,1,1)) para$method &lt;- &quot;leastcost&quot; #rSPDdistance, commute para$NN &lt;- 8 #number of neighbours for the cost distance method # Initialize simulation of populations from scratch landscape&lt;- r #&lt;-raster(system.file(&quot;external/rlogo.grd&quot;, package=&quot;raster&quot;)) # Define x and y locations para$cells &lt;- createpops(n=para$n.pops, minDist = 3, landscape = landscape, plot = FALSE) para$locs &lt;- raster::xyFromCell(landscape, para$cells) #give the population some names rownames(para$locs) &lt;- LETTERS[1:para$n.pops] # Create a matrix of pairwise cost distances... cost.mat &lt;- PopGenReport::costdistances(landscape, para$locs, para$method, para$NN) # ... and a matrix of pairwise Euclidean distances eucl.mat &lt;- as.matrix(dist(para$locs)) #needed for the analysis later # Plot your landscape with the populations.... plot(landscape) points(para$locs[,1], para$locs[,2], pch=16, cex=2, col=&quot;orange&quot;) text(para$locs[,1],para$locs[,2], row.names(para$locs), cex=1.5) # Check the parameter list para ## $n.pops ## [1] 8 ## ## $n.ind ## [1] 100 ## ## $sex.ratio ## [1] 0.5 ## ## $n.cov ## [1] 3 ## ## $n.offspring ## [1] 2 ## ## $mig.rate ## [1] 0.1 ## ## $disp.max ## [1] 50 ## ## $disp.rate ## [1] 0.05 ## ## $n.allels ## [1] 10 ## ## $n.loci ## [1] 20 ## ## $mut.rate ## [1] 0.001 ## ## $method ## [1] &quot;leastcost&quot; ## ## $NN ## [1] 8 ## ## $cells ## [1] 458 894 2359 389 2426 295 1394 680 ## ## $locs ## x y ## A 7 40 ## B 43 32 ## C 8 2 ## D 38 42 ## E 25 1 ## F 44 44 ## G 43 22 ## H 29 36 b. Initialise your population on the landscape Now finally we can initialise our population using the init function. Well call it simpops.0 to indicate that this is the initial generation. simpops.0 &lt;- PopGenReport::init.popgensim(para$n.pops, para$n.ind, para$sex.ratio, para$n.loci, para$n.allels, para$locs, para$n.cov ) You may want to check the simpops object, which is simply a list of our subpopulation and each individual is coded in a single run in one of the subpopulations. names(simpops.0) #the names of the subpopulations ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; head(simpops.0$A[,1:6]) ## a.list of the first 6 individuals and columns of population A ## pop sex age locus1A locus1B locus2A ## 1 1 female NA 6 10 2 ## 2 1 female NA 4 3 6 ## 3 1 female NA 2 10 9 ## 4 1 female NA 7 9 5 ## 5 1 female NA 3 1 9 ## 6 1 female NA 6 7 3 We can also analyse our simpop object. (e.g. calculate the pairwise Fst value between all the populations). To be able to do that we first need to convert it into a genind object (because many functions need this type of object as input). gsp &lt;- PopGenReport::pops2genind(simpops.0, locs =para$locs) gsp #check the genind object ## /// GENIND OBJECT ///////// ## ## // 800 individuals; 20 loci; 200 alleles; size: 738.5 Kb ## ## // Basic content ## @tab: 800 x 200 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 10-10) ## @loc.fac: locus factor for the 200 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: df2genind(X = res, sep = &quot;/&quot;, ind.names = rownames(res), pop = combine$pop) ## ## // Optional content ## @pop: population of each individual (group size range: 100-100) ## @other: a list containing: xy summary(gsp) #some summary statistics ## ## // Number of individuals: 800 ## // Group sizes: 100 100 100 100 100 100 100 100 ## // Number of alleles per locus: 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 ## // Number of alleles per group: 200 200 200 200 200 200 200 200 ## // Percentage of missing data: 0 % ## // Observed heterozygosity: 0.9 0.89 0.89 0.89 0.9 0.89 0.9 0.92 0.9 0.9 0.91 0.9 0.9 0.88 0.9 0.9 0.91 0.9 0.89 0.88 ## // Expected heterozygosity: 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 round(mmod::pairwise_Gst_Nei(gsp),5) ## A B C D E F G ## B -0.00012 ## C 0.00006 0.00001 ## D -0.00019 -0.00060 0.00022 ## E 0.00010 -0.00030 -0.00014 -0.00058 ## F 0.00016 -0.00031 0.00002 -0.00019 -0.00009 ## G -0.00020 -0.00046 0.00022 -0.00023 -0.00027 -0.00013 ## H -0.00045 -0.00013 -0.00009 -0.00008 -0.00007 -0.00033 -0.00020 Is there an effect of the landscape on the population structure (there should not be after initialisation)? The function pairwise.fstb is around 150 times faster than mmod::pairwise_Gst_Nei, but slightly different. gen.mat &lt;- PopGenReport::pairwise.fstb(gsp) round(gen.mat ,5) ## A B C D E F G H ## A 0.00000 0.00239 0.00256 0.00231 0.00260 0.00266 0.00231 0.00205 ## B 0.00239 0.00000 0.00252 0.00191 0.00220 0.00220 0.00205 0.00238 ## C 0.00256 0.00252 0.00000 0.00273 0.00237 0.00253 0.00273 0.00241 ## D 0.00231 0.00191 0.00273 0.00000 0.00193 0.00232 0.00228 0.00243 ## E 0.00260 0.00220 0.00237 0.00193 0.00000 0.00241 0.00224 0.00244 ## F 0.00266 0.00220 0.00253 0.00232 0.00241 0.00000 0.00237 0.00218 ## G 0.00231 0.00205 0.00273 0.00228 0.00224 0.00237 0.00000 0.00230 ## H 0.00205 0.00238 0.00241 0.00243 0.00244 0.00218 0.00230 0.00000 Now we perform a two partial Mantel tests, one for the effect of the cost distance partialling out the effect of Euclidean distance (Gen ~cost | Euclidean), and one the other way round. The method wassermann from the PopGenReport package returns a data frame with two rows (one for each test) and three columns (model, r = Mantel r statistic, p = p-value), following this method: Wassermann, T.N., Cushman, S. A., Schwartz, M. K. and Wallin, D. O. (2010). Spatial scaling and multi-model inference in landscape genetics: Martes americana in northern Idaho. Landscape Ecology, 25(10), 1601-1612. PopGenReport::wassermann(eucl.mat = eucl.mat, cost.mats = list(cost=cost.mat), gen.mat = gen.mat, plot=F)$mantel.tab ## model r p ## 2 Gen ~Euclidean | cost 0.1578 0.248 ## 1 Gen ~cost | Euclidean 0.151 0.268 Check the pairwise Fst values, why are they so low? Hints: How were genotypes assigned to the initial generation How many generations have we simulated thus far? At this point in the simulation, do you expect to see an effet of IBD, IBR, or neither? 4. Run simulations and analyze results a. Run your simulation over multiple time steps (years) Now we can run our simulation by simply passing our object simpops to the function run.popgensim, with some additional parameters that are needed for the simulation. We specify the number of generations the simulation should run with the steps parameter. (Check ?run.popgensim for a description of all parameters). Important to understand is the idea of the cost.mat (which is the cost matrix that is used for the distance between subpopulation). The n.alleles, n.ind cannot be different from the initialisation. simpops &lt;- PopGenReport::run.popgensim(simpops.0, steps=3, cost.mat, n.offspring=para$n.offspring, n.ind=para$n.ind, para$mig.rate, para$disp.max, para$disp.rate, para$n.allels, para$mut.rate, n.cov=para$n.cov, rec=&quot;none&quot;) In essence we were running a metapopulation with 100 individuals per subpopulation on our resistance landscape for 3 generations. The question is now was that enough time to create an effect on population structure? b. Analyse your simulated population with a partial Mantel test Lets check the pairwise Fst values and then do a landscape genetic analysis using partial Mantel tests. Convert to genind to calculate pairwise Fst. gsp &lt;- PopGenReport::pops2genind(simpops, para$locs, para$n.cov) Calculate your genetic distance matrix e.g. fst or D. gen.mat &lt;- PopGenReport::pairwise.fstb(gsp) round(gen.mat ,3) ## A B C D E F G H ## A 0.000 0.008 0.008 0.006 0.008 0.006 0.007 0.008 ## B 0.008 0.000 0.008 0.007 0.009 0.007 0.008 0.008 ## C 0.008 0.008 0.000 0.008 0.009 0.007 0.008 0.007 ## D 0.006 0.007 0.008 0.000 0.008 0.006 0.007 0.007 ## E 0.008 0.009 0.009 0.008 0.000 0.009 0.009 0.010 ## F 0.006 0.007 0.007 0.006 0.009 0.000 0.007 0.006 ## G 0.007 0.008 0.008 0.007 0.009 0.007 0.000 0.007 ## H 0.008 0.008 0.007 0.007 0.010 0.006 0.007 0.000 Partial Mantel test: PopGenReport::wassermann(eucl.mat = eucl.mat, cost.mats = list(cost=cost.mat), gen.mat = gen.mat, plot=F)$mantel.tab ## model r p ## 1 Gen ~cost | Euclidean 0.5525 0.025 ## 2 Gen ~Euclidean | cost -0.3513 0.911 We can extract a specific value from this result, e.g., the p-value of the test Gen ~cost | Euclidean. (Note that every time we call the function wassermann, a permutation test is performed (default: nperm = 999), and the p-value may thus vary somewhat). res &lt;- PopGenReport::wassermann(eucl.mat = eucl.mat, cost.mats = list(cost=cost.mat), gen.mat = gen.mat, plot=F)$mantel.tab res[res$model == &quot;Gen ~cost | Euclidean&quot;, &quot;p&quot;] ## [1] &quot;0.035&quot; c. Optional: Analyze your simulated populations using Sunder See: http://www.nbi.dk/~botta/Sunder.html#overview Sunder is an alternative (faster) implementation of BEDASSLE, which uses molecular covariance analysis as an alternative to partial Mantel test. All we want from this method for now is the best-model (note that there is no alternative none): G: Geographic distance model (IBD) G: Ecological distance model (IBR) G+E: Both To perform the Sunder analysis, we need to reformat the allele frequencies into a three-dimensional array (site x locus x allele). Here we define a function that takes as argument a genind object and reformats it as needed. getArray &lt;- function(gen) { tmp &lt;- Reduce(rbind,lapply(split(data.frame(gen@tab), gen@pop), colSums, na.rm=TRUE)) row.names(tmp) &lt;- levels(gen@pop) tmp &lt;- split(data.frame(t(tmp)), gen@loc.fac) Array &lt;- array(0, dim=c(ncol(tmp[[1]]), length(tmp), nrow(tmp[[1]]))) for(i in 1:length(tmp)) { Array[,i,] &lt;- t(tmp[[i]]) } return(Array) } Apply the function to the genind object gsp with the simulated genotypes: Array &lt;- getArray(gsp) dim(Array) ## [1] 8 20 10 Run the analysis (parameter settings: http://www.nbi.dk/~botta/Sunder.html#overview) D.G &lt;- as.matrix(dist(para$locs)) D.E &lt;- cost.mat nit &lt;- 10^2 ### j.st for the example, should be much larger, e.g. 50000 output &lt;- Sunder::MCMCCV(Array,D.G,D.E, nit=nit,thinning=max(nit/10^3,1), theta.max=c(10,10*max(D.G),10*max(D.E),1,0.9), theta.init=c(1,2,1,1,0.01), run=c(1,1,1), ud=c(0,1,1,0,0), n.validation.set=dim(Array)[1]*dim(Array)[2]/10, print.pct=FALSE) ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; Quick answer: the winner (best supporte model) is  names(which.max(output$mod.lik)) ## [1] &quot;G+E&quot; What did we just do? We extracted the log-likelihood values for the three models G, E and G+E, determined which value was the highest, and extracted the name of the winning model. Lets verify this by printing the log-likelihood values. The best model has the highest (= least negative) log-likelihood. print(output$mod.lik) ## G+E G E ## -8983.219 -8991.129 -9009.385 Lets combine all of this into our own function to convert the genind object, calculate Sunder and extract the name of the best fitting model getSunder &lt;- function(gen=gsp, locs=para$locs, cost=cost.mat, nit=10^2) { Array &lt;- getArray(gen) D.G &lt;- as.matrix(dist(locs)) D.E &lt;- cost output &lt;- Sunder::MCMCCV(Array,D.G,D.E, nit,thinning=max(nit/10^3,1), theta.max=c(10,10*max(D.G),10*max(D.E),1,0.9), theta.init=c(1,2,1,1,0.01), run=c(1,1,1), ud=c(0,1,1,0,0), n.validation.set=dim(Array)[1]*dim(Array)[2]/10, print.pct=FALSE) return(names(which.max(output$mod.lik))) } Lets test it: getSunder() ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## [1] &quot;G+E&quot; 5. Run simulator using a previously defined parameter set Once the simulator works and you are certain that you understand how the simulator needs to be set up for a single run, in almost all studies on simulations you want to be able to re-run the simulator in an automatized way. There are several reasons why you want to do that. You want to perform a sensitivity analysis on a single parameter, which means, try to find how much does the output (e.g. pairwise Fst between subpopulations) change when you vary an input parameter (e.g. number of loci). You want to explore the complete parameter space, which means, instead of changing values of a single input parameter you want to change all parameters (within certain levels) and run their combinations. Another reason is that you want to create a simulated test data set that forms the backbone of your future studies. So we would like to do the following. Specify and record all the parameter combinations that you would like to run. Run the simulator with every combination [Optional] save your complete simulation run (recommended, but sometimes prohibitive due to needed resources) or only a calculated summary. Read in your simulations, analyse them and synthesize your results via additional statistics, tests, plots. Publish an enormously important paper. Admittedly there are several different approaches and as it seems every modeller has a slightly different way to achieve these steps. One approach is to create a parameter file that records all the parameter setting for each run. Another approach is to create so called scripts for every single run. The advantage here is that scripts can be easily distributed across different cores and machines and therefore this approach is easy to parallelise your runs, which sometimes is necessary. Finally the approach I will present here (also because of practical reasons) is to create an R data.frame that stores all the parameter settings and we run all combinations in serial mode instead of in parallel. Okay before we start we need to think about what kind of parameters we want to explore. I would like to do the following runs: Run our simulations as above (same parameter settings) for varying time steps (say between 5 to 45 years in steps of 20). Well keep the number of levels and the maximum number of steps low in this example to limit computation time. Feel free to expand! As output I would still like to record the Fst value, but also the full genetic data set and the parameters used to run the simulation. In addition I want to repeat each run 5 times (most often you would do more repetitions) to check how much general variation there is between runs with exactly the same parameter combination. a. Specify and record the parameter combinations Lets define the varying numbers of time steps we would like to run the simulations. Here we define a sequence from 5 to 45 in steps of 20, which results in a series c(5, 25, 45). We will interpret these values as numeric, therefore we dont convert to factor. timesteps &lt;- seq(from=5 , to=45, by=20) We also specify the number of repeats (replicate simulation runs). We want to do five replicate simulation runs per for each level of time, and we will label replicates from 1 through 5. These are essentially labels and well save them as a factor: repeats &lt;- factor(1:5) Now we would like to have a data frame that stores all possible combinations for those two parameters. As simple way to do that in R, is to use the expand.grid function. para.space &lt;- expand.grid(rep=repeats, time=timesteps) tibble::as_tibble(para.space) ## # A tibble: 15 x 2 ## rep time ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 5 ## 2 2 5 ## 3 3 5 ## 4 4 5 ## 5 5 5 ## 6 1 25 ## 7 2 25 ## 8 3 25 ## 9 4 25 ## 10 5 25 ## 11 1 45 ## 12 2 45 ## 13 3 45 ## 14 4 45 ## 15 5 45 As you can see this results in 15 combinations (3 time steps x 5 repeats). The beauty of this approach is that it is very flexible and adaptable to runs over other parameter combinations, as you can provide more than two parameter variables to expand.grid. b. Run the simulator over every parameter combination. Remember our parameters are all defined in the para object (a list) and we want to keep them constant, except for running the simulation for different number of years. This means that we only need to modify the argument steps. Summarizing the code from above, a single run of our simulator runs via: #initialize simpops.0 &lt;- PopGenReport::init.popgensim(para$n.pops, para$n.ind, para$sex.ratio, para$n.loci, para$n.allels, para$locs, para$n.cov ) #run for 20 generations simpops &lt;- PopGenReport::run.popgensim(simpops.0, steps=20, cost.mat, n.offspring=para$n.offspring, n.ind=para$n.ind, para$mig.rate, para$disp.max, para$disp.rate, para$n.allels, para$mut.rate, n.cov=para$n.cov, rec=&quot;none&quot;) We adapt this code as follows: Create a for loop that cycles through every row i in para.space For each value of i: Initialize population simpops.0. Run the simulation with argument steps = para.space$time[i]. We are not running the code just yet, hence it is commented-out with #. #for (i in 1:nrow(para.space)) #{ # #initialize # simpops.0 &lt;- PopGenReport::init.popgensim(para$n.pops, para$n.ind, # para$sex.ratio, para$n.loci, para$n.allels, # para$locs, para$n.cov ) # # #run for para.space$time[i] generations # simpops &lt;- PopGenReport::run.popgensim(simpops.0, # steps=para.space$time[i], cost.mat, # n.offspring=para$n.offspring, n.ind=para$n.ind, # para$mig.rate, para$disp.max, para$disp.rate, # para$n.allels, para$mut.rate, # n.cov=para$n.cov, rec=&quot;none&quot;) #} Have a close look at the change. Question: what changes between replicate runs, and what not? Consider the following aspects: Landscape Population locations Pairwise distances (cost, Euclidean) Initial populations with initial genotypes Migration and gene flow c. Save your complete simulation run (input and output) Simply running the simulation 15 times (number of rows in para.space) by itself is not useful yet. We need to store the simulation runs somehow, so we can collect them afterwards to calculate summary statistics and analyse the runs. How do we store the repeats seperately in a file? One approach would be to have a different file name for every repeat, but in my view, a cleaner approach is to store all simulation outputs and also store the complete parameter and input information in a file, so everything that is need is in one place. A nice way to do that in R is to create a list object that stores all in a single object, which can be saved (and is automatically packed) and re-loaded as an R object. This is convenient as long as I only want to analyze the results in R, not export to other software. Here we do the following: Create a timer with the function proc.time so that we know roughly how long the computations take. For each line i in para.space: Initialize simpops.0. Run the simulation with steps=para.space$time[i]. Convert the resulting simpop to a genind object gi (smaller to store) Create a list sim of all simulation parameters we want to store. Save the object sim as an RData file with a unique file name in the folder output/simout in the project directory. Print a message after each run to report progress and computation time. Flush the output console to make sure it is current (only relevant for console-based versions of R). The list sim will contain the following elements. Note: the name is repeated (e.g., gi = gi) to create a named list, i.e., to specify the names of the list elements. para.space: the row para.space[i] with settings of the simulation run. para: a copy of the list para that contains the other parameters that are the same for all simulation runs. landscape: the landscape. cost.mat: the matrix of pairwise cost distances gi: the genind object that contains the genotypes at the end of the simulation. First we make sure the folder simout exists within the output folder in the R project: if(!dir.exists(paste0(here::here(),&quot;/output&quot;))) dir.create(paste0(here::here(),&quot;/output&quot;)) if(!dir.exists(paste0(here::here(),&quot;/output/simout&quot;))) dir.create(paste0(here::here(),&quot;/output/simout&quot;)) ## c.eate a timer (just to know how long it will take roughly) timer0 &lt;- round(proc.time()[3],2) for (i in 1:nrow(para.space)) { ## i.itialize simpops.0 &lt;- PopGenReport::init.popgensim(para$n.pops, para$n.ind, para$sex.ratio, para$n.loci, para$n.allels, para$locs, para$n.cov ) # run for para.space$time[i] generations simpops &lt;- PopGenReport::run.popgensim(simpops.0, steps=para.space$time[i], cost.mat, n.offspring=para$n.offspring, n.ind=para$n.ind, para$mig.rate, para$disp.max, para$disp.rate, para$n.allels, para$mut.rate, n.cov=para$n.cov, rec=&quot;none&quot;) ## c.nvert to genind object (smaller) gi &lt;- PopGenReport::pops2genind(simpops) ## c.eate a list of all I want to collect sim &lt;- list(para.space=para.space[i,], para=para, landscape=landscape, cost.mat=cost.mat, gi=gi) # save everything in an output folder (with a consecutive number, with three leading zeros, so the file sorting is nicer) save(sim, file = paste0(here::here(),&quot;/output/simout/sim_time5-45_rep5_&quot;, sprintf(&quot;%03i&quot;,i) ,&quot;.RData&quot;)) cat(paste0(&quot;Finished run: &quot;, i,&quot; out of &quot;,nrow(para.space), &quot;. So far, it took: &quot;, round(proc.time()[3]-timer0,2),&quot; sec.\\n&quot;)) flush.console() } ## Finished run: 1 out of 15. So far, it took: 0.49 sec. ## Finished run: 2 out of 15. So far, it took: 0.94 sec. ## Finished run: 3 out of 15. So far, it took: 1.46 sec. ## Finished run: 4 out of 15. So far, it took: 1.89 sec. ## Finished run: 5 out of 15. So far, it took: 2.36 sec. ## Finished run: 6 out of 15. So far, it took: 4.42 sec. ## Finished run: 7 out of 15. So far, it took: 6.42 sec. ## Finished run: 8 out of 15. So far, it took: 8.53 sec. ## Finished run: 9 out of 15. So far, it took: 10.5 sec. ## Finished run: 10 out of 15. So far, it took: 12.5 sec. ## Finished run: 11 out of 15. So far, it took: 16.69 sec. ## Finished run: 12 out of 15. So far, it took: 22.13 sec. ## Finished run: 13 out of 15. So far, it took: 29.05 sec. ## Finished run: 14 out of 15. So far, it took: 35.8 sec. ## Finished run: 15 out of 15. So far, it took: 39.02 sec. d. Analyze and synthesize results If you check your output folder (simout) you should see 15 files. Note: File paths can be different when you execute a chunk in an R notebook compared to when you copy-paste the same line into the console! We avoid this problem by using the function here from package here. head(dir(paste0(here::here(), &quot;/output/simout&quot;))) ## [1] &quot;sim_time5-45_rep5_001.RData&quot; &quot;sim_time5-45_rep5_002.RData&quot; ## [3] &quot;sim_time5-45_rep5_003.RData&quot; &quot;sim_time5-45_rep5_004.RData&quot; ## [5] &quot;sim_time5-45_rep5_005.RData&quot; &quot;sim_time5-45_rep5_006.RData&quot; Now we are at step D where we need to read in all our files one by one, calculate some summary statistics and plot our results. Again, this could be easy, but be aware if you have thousands of files it could take quite some time and memory. The most convenient way is to load everyting and store it in a list, so we can access all of our simulations from memory. I will show how to do this in the example below, but be aware in larger simulations (think millions of runs, or large sample sizes) this is not possible and we would load a single simulation, calculate a statistic, store only the result in a table and free the memory for the next simulation run. Lets load our simulation runs. There is one caveat: when we load the object sim from the .RData file, we cant assign it a new object name. I.e., we cant use newName &lt;- load(sim.Rdata). Instead, we can only type load(sim.Rdata) and it will create or overwrite the object sim. Also, R takes the name from the object that was saved, not from the file name. Hence, once we load the object, any existing object of the same name will be overwritten. So if we want to keep, it we need to rename it before using load. Here we do the following: Create an empty table with three columns rep, time, fst and other columns to collect summary results for the 15 simulation runs. Create a vector that contains all filenames. The function list.files does just that. We specify with path=./simout that the files in folder simout should be listed, and with pattern=sim we specify that we want all file names that contain the expression sim (we could also have used time, for example - any component of the file name that is shared by all the files we are interested in but no other files that might be in the same folder). Loop through the files. For each filename i: Load the file, which will create or overwrite the object sim. We need to supply not only the file name but the path, hence paste0(./simout/,filenames[i]) (see Week 8 video for more on file paths). Extract simulation parameters: Copy the ith row from para.space (repeat number, timesteps) into the first two columns of the ith row of res. Extract the genind object gi with the final genotypes. Calculate the mean of pairwise fst values and store in the third column of the ith row of res. Perform partial Mantel tests with function wasserman and store the Mantel r statistics and the p-values in the corresponding columns. Note that this is somewhat tricky because the function wasserman ranks the models and the better fitting model is listed in the first row, so that the order of the models can vary between runs. Therefore we extract the values based on model name. Perform Sunder covariance analysis and extract which model is best supported (G, E or GE). res &lt;- data.frame(rep=NA, time=NA, fst=NA, r.Eucl=NA, p.Eucl=NA, r.cost=NA, p.cost=NA, Sunder=NA) #load all files in the folder filenames &lt;- list.files(path= paste0(here::here(), &quot;/output/simout&quot;), pattern=&quot;sim&quot;) for (i in 1:length(filenames)) { #creates a sim object load(paste0(here::here(), &quot;/output/simout/&quot;,filenames[i])) #now let us take what we need from the simulation res[i,1:2] &lt;- sim$para.space #calculate a summary statistic: mean of pairwise fst values ## h.re we only take the lower triangle of the matrix to avoid the diagonal values, # which are zero by definition (comparing each population to itself) gen.mat &lt;- PopGenReport::pairwise.fstb(sim$gi) res [i,3] &lt;- mean(gen.mat[lower.tri(gen.mat)]) #partial Mantel tests wass &lt;- PopGenReport::wassermann(eucl.mat = dist(sim$para$locs), cost.mats = list(cost=sim$cost.mat), gen.mat = gen.mat, plot=F)$mantel.tab res[i,4:5] &lt;- wass[wass$model == &quot;Gen ~Euclidean | cost&quot;, 2:3] res[i,6:7] &lt;- wass[wass$model == &quot;Gen ~cost | Euclidean&quot;, 2:3] #Sunder res[i,8] &lt;- getSunder(gen=sim$gi, locs=sim$para$locs, cost=sim$cost.mat, nit=10^2) #if you have time, set nit=10^3 } ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; ## ## Computations for model &#39;geog+envi&#39; ## Computations for model &#39;geog&#39; ## Computations for model &#39;envi&#39; Look at the res data frame and check the results. head(res) ## rep time fst r.Eucl p.Eucl r.cost p.cost Sunder ## 1 1 5 0.009340057 0.0455 0.494 0.1492 0.305 G+E ## 2 2 5 0.009210392 -0.1807 0.723 0.4638 0.045 E ## 3 3 5 0.008996691 0.2389 0.198 -0.0054 0.508 G ## 4 4 5 0.010043283 -0.0772 0.624 0.1089 0.356 E ## 5 5 5 0.009755578 -0.1557 0.739 0.21 0.25 G ## 6 1 25 0.022190031 0.0424 0.425 0.5067 0.059 G+E The next step would be to visualise the results (e.g. plot runs over times and color by rep). A quick way to do that is to use the function ggplot from the ggplot2 package. Here we add a jitter to keep points from overlapping too much. ggplot2::ggplot(res, ggplot2::aes(x=time, y=fst)) + ggplot2::geom_point(position = ggplot2::position_jitter(w = 0.5)) Now it is again time for you to experiment. For example, why not set up a simulation that varies the number of loci. Or as you may have seen even after 100 generation there was no sign that the mean pairwise Fst value is levelling off. So how long do you have to run a simulation in terms of time to see this (be aware that simulation runs take longer if you increase the number of timesteps)? Questions: How would you set up a simulation experiment to compare type I error rates between partial Mantel test and Sunder? How about statistical power? Have fun and please give us feed back what you think about this Worked Example. Bernd Gruber, Erin Landguth, Helene Wagner. "],["r-exercise-week-8.html", "R Exercise Week 8", " R Exercise Week 8 Task: Carry out a permutation test for the Mantel rank correlation to test for fine-scale spatial genetic structure in Pulsatilla vulgaris adults, using the pooled data from all seven patches. Use a one-sided alternative greater, as we expect the Mantel rank correlation to be positive. Hints: Exclude all pairs that involve individuals from different patches. Permute individuals only within patches, not between patches. Calculate the Mantel rank correlation for the observed data (M.rho.obs) For each of R = 499 permutations, calculate M.rho.sim Determine the approximate p-value of the one-sided test with alternative greater as the percentage of the 500 values (1 observed, 499 permuted) that are greater or equal to the observed one. Load packages: You may want to load the packages dplyr and gstudio. Alternatively, you can use :: to call functions from packages. Import data, extract adults. Use the code below to import the data into gstudio and extract adults (OffID == 0). library(dplyr) Pulsatilla.gstudio &lt;- gstudio::read_population(path=system.file(&quot;extdata&quot;, &quot;pulsatilla_genotypes.csv&quot;, package = &quot;LandGenCourse&quot;), type=&quot;column&quot;, locus.columns=c(6:19), phased=FALSE, sep=&quot;,&quot;, header=TRUE) Adults.gstudio &lt;- Pulsatilla.gstudio %&gt;% filter(OffID == 0) Sort individuals by patch. Create a new ID variable Names that combines the existing variables Population and ID (starting with population). Then use the function dplyr::arrange to sort Adults.gstudio by Names (check the help file for arrange). This is important here for two reasons: In order to efficiently permute individuals within patches, they need to be sorted by patch. In some cases, the function gstudio::genetic_distance will sort the distance matrix alphabetically. To avoid mismatches between the data and the resulting distance matrix, it is best to sort the data alphabetically already. Calculate Euclidean distances: Use the metric coordinates in variables X and Y to calculate Euclidean distances (see Week 5, section 4). Here it is important to store the distances Dgeo in the full matrix format, with as.matrix. Calculate genetic distances (Dps): Use the following code (as needed (adapt if needed) to calculate individual-level genetic distances (proportion of shared alleles) and store them in the full matrix format. We subtract values from 1 to obtain a distance measure. Note: the calculation in gstudio is based on Bray distance and the resulting values are proportional to those calculated by adegenet::propShared. I.e., the two measures have a correlation of 1 but the actual values differ. Dgen &lt;- 1 - as.matrix(gstudio::genetic_distance(Adults.gstudio, stratum=\"Names\", mode=\"dps\")) Plot distances, calculate M.rho: Create a plot of Dgen vs Dgeo, and calculate the Mantel rank correlation. Note: the function cor with default settings does not allow missing values (NA). This can be changed e.g. with the argument use. Use as.dist to access only the values from the lower triangle of each matrix. The function plot will do. Inspect the plot. Where would you find the pairs of individuals within the same patch? Use the function cor with method = \"spearman\" to calculate the Mantel rank correlation. Allow for missing values with the argument use = \"complete.obs\". Limit to within-patch pairs: Restrict the analysis to pairs within the same patch. For this, we want to set all between-site comparisons in Dgeo to NA. Uncomment the code below to: Create a matrix that is TRUE if the two individuals are in the same patch, and FALSE if not (first line) Change FALSE to NA (second line) Multiply Dgeo by this matrix to set distances between patches to NA (third line). Adapt your code from above to plot the distances Dgen vs. Dgeo.within. Calculate the Mantel rank correlation between Dgen vs. Dgeo.within and store it as Cor.obs. #SamePatch &lt;- outer(Adults.gstudio$Population, Adults.gstudio$Population, FUN = &quot;==&quot;) #SamePatch[SamePatch == &quot;FALSE&quot;] &lt;- NA #Dgeo.within &lt;- SamePatch * Dgeo Note: check the help file or run the following examples to figure out what outer does: outer(c(1:5), c(1:5), FUN = \"*\") outer(c(1:5), c(1:5), FUN = \"-\") outer(LETTERS[1:5], c(1:5), FUN = \"paste0\") Unrestricted permutation test: Create a variable Order with row numbers (from 1 to the number of individuals in Adults.gstudio). Then, uncomment the code below (adapt as needed) to carry out a permutation test by permuting the order of individuals, R = 499 times. Notes to permute a distance matrix, we need to permute the rows and columns of the full distance matrix simultaneously with the same order: Dgen[a,a]. We only need to permute one of the two matrices (Dgen or Dgeo.within), but not both. The approximate p-value is calculated as the proportion of values (R simulated ones and 1 observed Cor.obs) that were as large, or larger, than Cor.obs. #R = 499 #Cor.perm.unrestricted &lt;- rep(NA, R) #for(r in 1:R) #{ # a &lt;- sample(Order) # Cor.perm.unrestricted[r] &lt;- cor(as.dist(Dgen[a,a]),as.dist(Dgeo.within), method=&quot;spearman&quot;, use=&quot;complete.obs&quot;) #} #approx.p.unrestricted &lt;- mean(c(Cor.obs, Cor.perm.unrestricted) &gt;= Cor.obs) #approx.p.unrestricted Restricted permutation test: Adapt the code to permute individuals only within patches. For this, split Order by population, randomize row numbers within groups (list elements = populations) with sapply, and use unlist to convert to a vector again. Make sure to change object names from unrestricted to restricted. Before the for loop, add this code: b &lt;- split(Order, Adults.gstudio$Population) Inside the for loop, replace the calculation of a : a &lt;- unlist(sapply(b, sample)) Compare results: Create side-by-side boxplots of the simulated Mantel rank correlation values for the unrestricted and the restricted permutation tests. Note: if none of the simulated values was larger than Cor.obs, the approx. p-value will be 1/(R+1). This indicates the resolution of the permutation test, i.e., the smallest possible p-value given R. Questions: Did the direction, size or statistical significance of the observed Mantel rank correlation (as a measure of fine-scale spatial genetic structure in P. vulgaris) change between the unrestricted and the restricted permutation test? Why, or why not? How did the distributions of the simulated values of the Mantel rank correlation differ between the unrestricted and the restricted test? Can you think of a reason for this? "],["bonus-8a.html", "Bonus: Efficient R", " Bonus: Efficient R "],["Week9.html", "Week 9: Population Structure ", " Week 9: Population Structure "],["WE-9.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals The goals of this lab are to: Assess how patterns of genetic variation can be used to delimit natural populations. Compare methods that assess population structure. Understand how population structure can be used to interpret biogeographic history. b. Data sets All files are distributed as system files with the LandGenCourse package (folder extdata). Simulated data using the two-island model and admixture model. SNP data from Catchen et al. 2013 Catchen et al. 2013. The population structure and recent colonization history of Oregon threespine stickleback determined using restriction-site associated DNA-sequencing. Molecular Ecology 22:1365-294X. http://dx.doi.org/10.1111/mec.12330 c. Required R libraries Note: the function library will always load the package, even if it is already loaded, whereas require will only load it if it is not yet loaded. Either will work. require(LandGenCourse) #require(LandGenCourseData) #require(fields) #require(RColorBrewer) #require(maps) #require(mapplots) #require(here) The package LEA is available from the Bioconductor repository. Also, package fields was not installed with LandGenCourse automatically due to compatibility issues. if(!requireNamespace(&quot;fields&quot;, quietly = TRUE)) install.packages(&quot;fields&quot;, repos=&#39;http://cran.us.r-project.org&#39;) if(!requireNamespace(&quot;LEA&quot;, quietly = TRUE)) { if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;LEA&quot;) } The data are in a data package: if(!requireNamespace(&quot;LandGenCourseData&quot;, quietly = TRUE)) devtools::install_github(&quot;hhwagner1/LandGenCourseData&quot;) 2. Simulated data: 2-island model We simulated data under a classic two-island model, using the coalescent simulation program ms, developed by Richard Hudson. The program simulates a coalescent tree under various demographic models, and uses those trees to create allelic data. For those interested in using ms, the source code is available here: http://home.uchicago.edu/rhudson1/source/mksamples.html We simulated 200 haploid individuals genotyped at 100 loci. The effective mutation rate was  = 0.5. We sampled 2 islands with 100 individuals in each. The effective migration rate was Nm 2 (N is the effective size in each of the two island, m is the bidirectional rate of gene flow). Our ms command was as follows: ms 200 100 -t .5 -I 2 100 100 -ma x 2 2 x &gt; dataNm2.txt These raw data need to be converted in a format amenable to statistical analyses in R. a. Import data file &lt;- scan(file = system.file(&quot;extdata&quot;, &quot;dataNm2.txt&quot;, package = &quot;LandGenCourse&quot;), what =&quot;character&quot;, sep=&quot;\\n&quot;, skip = 2) genotype &lt;- NULL for(locus in 1:100){ res.locus &lt;- file[4:203] file &lt;- file[-(1:203)] genotype &lt;- cbind(genotype, as.numeric(as.factor(res.locus)))} dim(genotype) ## [1] 200 100 Now we have a new data file, genotype, loaded in R. This file contains 200 rows and 100 columns. Each row corresponds to a simulated individual. The columns code for their multi-locus genotypes. b. Perform Principal Components Analysis (PCA) Our first objective is to use ordination (Principal components analysis, or PCA) to examine population structure for the Nm = 2 data set. The R command for PCA is fairly simple and fast: pc = prcomp(genotype, scale =T) In order to visualize how the first two eigenvectors capture genotype variation, we will color each population. par(mar=c(4,4,0.5,0.5)) plot(pc$x, pch = 19, cex = 2, col = rep(c(&quot;blue2&quot;, &quot;orange&quot;), each = 100)) Question 1: Is population structure (genetic differentiation) evident? How much of the genetic variance can be explained by our first two components? To answer the second part of this question, use: summary(pc) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.77699 1.65283 1.5587 1.53452 1.48944 1.48496 1.46718 ## Proportion of Variance 0.07712 0.02732 0.0243 0.02355 0.02218 0.02205 0.02153 ## Cumulative Proportion 0.07712 0.10443 0.1287 0.15228 0.17446 0.19651 0.21804 ## PC8 PC9 PC10 PC11 PC12 PC13 PC14 ## Standard deviation 1.43774 1.41957 1.40980 1.40041 1.38541 1.37335 1.35879 ## Proportion of Variance 0.02067 0.02015 0.01988 0.01961 0.01919 0.01886 0.01846 ## Cumulative Proportion 0.23871 0.25886 0.27874 0.29835 0.31754 0.33640 0.35487 ## PC15 PC16 PC17 PC18 PC19 PC20 PC21 ## Standard deviation 1.35395 1.31441 1.3078 1.30115 1.29493 1.26846 1.26525 ## Proportion of Variance 0.01833 0.01728 0.0171 0.01693 0.01677 0.01609 0.01601 ## Cumulative Proportion 0.37320 0.39048 0.4076 0.42451 0.44128 0.45737 0.47338 ## PC22 PC23 PC24 PC25 PC26 PC27 PC28 ## Standard deviation 1.23786 1.23212 1.22814 1.20358 1.18924 1.17923 1.17139 ## Proportion of Variance 0.01532 0.01518 0.01508 0.01449 0.01414 0.01391 0.01372 ## Cumulative Proportion 0.48870 0.50388 0.51896 0.53345 0.54759 0.56150 0.57522 ## PC29 PC30 PC31 PC32 PC33 PC34 PC35 ## Standard deviation 1.16216 1.12304 1.10610 1.10157 1.09177 1.07816 1.05735 ## Proportion of Variance 0.01351 0.01261 0.01223 0.01213 0.01192 0.01162 0.01118 ## Cumulative Proportion 0.58873 0.60134 0.61357 0.62571 0.63763 0.64925 0.66043 ## PC36 PC37 PC38 PC39 PC40 PC41 PC42 ## Standard deviation 1.04793 1.02672 1.01936 1.01364 1.00225 0.99349 0.98594 ## Proportion of Variance 0.01098 0.01054 0.01039 0.01027 0.01005 0.00987 0.00972 ## Cumulative Proportion 0.67141 0.68195 0.69235 0.70262 0.71267 0.72254 0.73226 ## PC43 PC44 PC45 PC46 PC47 PC48 PC49 ## Standard deviation 0.96673 0.96357 0.9485 0.94380 0.92308 0.91570 0.90220 ## Proportion of Variance 0.00935 0.00928 0.0090 0.00891 0.00852 0.00839 0.00814 ## Cumulative Proportion 0.74160 0.75089 0.7599 0.76879 0.77731 0.78570 0.79384 ## PC50 PC51 PC52 PC53 PC54 PC55 PC56 ## Standard deviation 0.89163 0.88277 0.86488 0.84941 0.83872 0.83328 0.82304 ## Proportion of Variance 0.00795 0.00779 0.00748 0.00721 0.00703 0.00694 0.00677 ## Cumulative Proportion 0.80179 0.80958 0.81706 0.82427 0.83131 0.83825 0.84503 ## PC57 PC58 PC59 PC60 PC61 PC62 PC63 ## Standard deviation 0.81748 0.80746 0.79085 0.77042 0.76650 0.76104 0.7553 ## Proportion of Variance 0.00668 0.00652 0.00625 0.00594 0.00588 0.00579 0.0057 ## Cumulative Proportion 0.85171 0.85823 0.86448 0.87042 0.87629 0.88209 0.8878 ## PC64 PC65 PC66 PC67 PC68 PC69 PC70 ## Standard deviation 0.72978 0.7279 0.72353 0.70575 0.69818 0.68517 0.68335 ## Proportion of Variance 0.00533 0.0053 0.00523 0.00498 0.00487 0.00469 0.00467 ## Cumulative Proportion 0.89312 0.8984 0.90365 0.90863 0.91351 0.91820 0.92287 ## PC71 PC72 PC73 PC74 PC75 PC76 PC77 ## Standard deviation 0.67501 0.65756 0.63896 0.62114 0.61388 0.60925 0.60217 ## Proportion of Variance 0.00456 0.00432 0.00408 0.00386 0.00377 0.00371 0.00363 ## Cumulative Proportion 0.92743 0.93175 0.93583 0.93969 0.94346 0.94717 0.95080 ## PC78 PC79 PC80 PC81 PC82 PC83 PC84 ## Standard deviation 0.58408 0.56742 0.55817 0.54962 0.53610 0.52625 0.52442 ## Proportion of Variance 0.00341 0.00322 0.00312 0.00302 0.00287 0.00277 0.00275 ## Cumulative Proportion 0.95421 0.95743 0.96054 0.96356 0.96644 0.96921 0.97196 ## PC85 PC86 PC87 PC88 PC89 PC90 PC91 ## Standard deviation 0.50528 0.49755 0.4798 0.46734 0.45256 0.44882 0.44121 ## Proportion of Variance 0.00255 0.00248 0.0023 0.00218 0.00205 0.00201 0.00195 ## Cumulative Proportion 0.97451 0.97699 0.9793 0.98147 0.98352 0.98554 0.98748 ## PC92 PC93 PC94 PC95 PC96 PC97 PC98 ## Standard deviation 0.41786 0.41163 0.39781 0.37758 0.36923 0.36602 0.34913 ## Proportion of Variance 0.00175 0.00169 0.00158 0.00143 0.00136 0.00134 0.00122 ## Cumulative Proportion 0.98923 0.99092 0.99251 0.99393 0.99529 0.99663 0.99785 ## PC99 PC100 ## Standard deviation 0.34128 0.31336 ## Proportion of Variance 0.00116 0.00098 ## Cumulative Proportion 0.99902 1.00000 Next we would like to see how population genetic structure relates to geographic space. To this aim, we could display PC maps. A PC map is a spatial interpolation of a particular component. Lets map PC 1. c. Create synthetic spatial coordinates (X,Y) and map them par(mar=c(4,4,0.5,0.5)) coord &lt;- cbind(sort(c(rnorm(100, -2, 1), rnorm(100, 2, 1))), runif(200)) fit &lt;- fields::Krig(coord, pc$x[,1], m=1) fields::surface(fit) points(coord, pch=19) This map predicts the value of the first principal component at each location in our study area. We observe that the study area is partitioned into two zones that correspond to the 2 clusters visible from PC 1. We have superimposed individual sample sites to see our species distribution. To check that the PC map is consistent with having 2 islands, we can examine the PC assignment vs the sampling location. Remember that, in the data sets, the 100 first individuals were sampled from island 1 and the last 100 were sampled from island 2. We compare these assignments to the PCA classification as follows. table((pc$x[,1] &gt; 0) == (1:200 &lt;= 100)) ## ## FALSE TRUE ## 1 199 In this example, we found that only one individual was not assigned to its island of origin. Well, this individual might be a migrant from the last generation. These results indicated that a very simple method based on principal component analysis can correctly describe spatial population genetic structure. Question 2: Does PCA provide an accurate description of population genetic structure when the genetic differentiation between the 2 islands is less pronounced? To answer this question, re-run the first analytical steps up to PCA for data simulated with Nm = 10 (a higher value of gene flow), which is stored in datafile dataNm10.txt. This was generated with the following ms command: ms 200 100 -t .5 -I 2 100 100 -ma x 10 10 x &gt; dataNm10.txt You can import the file as: file &lt;- scan(file = system.file(&quot;extdata&quot;, &quot;dataNm10.txt&quot;, package = &quot;LandGenCourse&quot;), what =&quot;character&quot;, sep=&quot;\\n&quot;, skip = 2) 3. Simulated data: 2-island model with admixture 2-island model A 2-island model is a relatively simple scenario and unlikely to capture the variation we will see in empirical studies. Will our very basic assignment method based on PCA hold up to more complex scenarios? Lets consider a scenario where the 2 populations had been evolving for a long time under an equilibrium island model, and then their environment suddenly changed. Our 2 populations had to move to track their shifting habitat, and after these movements they come into contact in an intermediate region. This contact event resulted in an admixed population with the density of mixed individuals greater in the center of the contact zone than at the ancestral origins at the edges of the landscape. Using R and our previous simulation, a multi-locus cline that resumes this scenario can be simulated has follows. The source population data are in the file dataNm1.str. First we define a function for the shape of a cline: # A function for the shape of a cline sigmoid &lt;- function(x){ 1/(1 + exp(-x))} p1 &lt;- sigmoid( 0.5 * coord[,1]) Our admixed genotypes are built from a 2 island model with low gene flow (Nm=1) genotype = read.table(file = system.file(&quot;extdata&quot;, &quot;dataNm1.str&quot;, package = &quot;LandGenCourse&quot;))[,-(1:2)] admixed.genotype &lt;- matrix(NA, ncol = 100, nrow = 200) for (i in 1:100){ for (j in 1:100) admixed.genotype[i,j] = sample( c(genotype[i, j],genotype[i+100, j]), 1, prob = c(p1[i], 1 - p1[i]) )} for (i in 101:200){ for (j in 1:100) admixed.genotype[i,j] = sample( c(genotype[i - 100, j],genotype[i, j]), 1, prob = c(p1[i], 1 - p1[i]) )} res &lt;- data.frame(coord, admixed.genotype) Now our data set is the R object res. The next exercise is to apply PCA to these data to evaluate how geographical genetic variation can be captured by this approach. In comparison with the previous example where we had two geographically discrete populations, we now have a continuous population in a landscape. Geographical genetic variation is thus expected to be more gradual than in the previous example. Generate and examine the PC 1 map. par(mar=c(4,4,0.5,0.5)) pcA = prcomp(admixed.genotype, scale =T) plot(pcA$x, pch = 19, cex = 2, col = rep(c(&quot;blue2&quot;,&quot;orange&quot;), each = 100)) Look at the PC Map par(mar=c(4,4,0.5,0.5)) fit &lt;- fields::Krig(res, pcA$x[,1], m=1) ## Warning: ## Grid searches over lambda (nugget and sill variances) with minima at the endpoints: ## (REML) Restricted maximum likelihood ## minimum at right endpoint lambda = 0.05291003 (eff. df= 190 ) fields::surface(fit) points(res) Question 3: How does admixture change our prediction of population structure (PCA plot)? Is genomic ancestry correlated with geographical location? To answer this latter part, check the R2 and significance of statistical association between PC1 component scores and geographical position (p1): summary(lm(pcA$x[,1]~ p1)) ## ## Call: ## lm(formula = pcA$x[, 1] ~ p1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6420 -0.8105 -0.0512 0.8546 3.8711 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.8508 0.1822 21.14 &lt;2e-16 *** ## p1 -7.8107 0.3356 -23.28 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.08 on 198 degrees of freedom ## Multiple R-squared: 0.7324, Adjusted R-squared: 0.731 ## F-statistic: 541.8 on 1 and 198 DF, p-value: &lt; 2.2e-16 4. Empirical data: Threespine sticklebacks The Threespine stickleback (Gasterosteus aculeatus) is a fish that has emerged as a model of rapid and parallel adaptation. Catchen et al. (2013) were interested in how populations colonize freshwater habitats in the Pacific Northwest, USA. These sticklebacks have diversified into three life history forms, one exclusively dwelling in the ocean, another being adapted to freshwater habitats, and one unusual population that can utilize both habitats. It was unclear if this one particular population (Riverbend), from a stream in central Oregon, was introduced due to unintentional human transport, and if this could be an example of rapid adaptation to freshwater from oceanic populations. Single nucleotide polymorphism data were generated using genotyping-by-sequencing, for 9 populations occupying coastal areas and inland streams. Map In this tutorial, we will analyze the genetic data generated by Catchen et al. (2013) using a few exploratory methods to quantify and visualize genetic differentiation among the stickleback populations sampled. By the end of this tutorial, hopefully you will be able to make a convincing argument for the regional origin of the recently-introduced inland stickleback population. a. Import the data: data &lt;- read.table(system.file(&quot;extdata&quot;, &quot;stickleback_data.txt&quot;, package = &quot;LandGenCourseData&quot;), sep=&quot;\\t&quot;, as.is=T, check.names=F) Create a list of population IDs: pops &lt;- unique(unlist(lapply(rownames(data), function(x){y&lt;-c();y&lt;-c(y,unlist(strsplit(x,&quot;_&quot;)[[1]][1]))}))) To understand the experimental design a bit better, lets look at the sample size at each site. sample_sites &lt;- rep(NA,nrow(data)) for (i in 1:nrow(data)){ sample_sites[i] &lt;- strsplit(rownames(data),&quot;_&quot;)[[i]][1]} N &lt;- unlist(lapply(pops,function(x){length(which(sample_sites==x))})) names(N) &lt;- pops N ## cr cs pcr pl sj stl wc rb ms ## 23 97 67 20 86 50 22 138 68 b. Examine population structure with PCA Lets start examining population structure, first using PCA. Well look at the amount of variation explained in the first few components, and then well plot individuals for four components, coloring them by population. par(mar=c(4,4,2,0.5)) pcaS &lt;- prcomp(data,center=T) plot(pcaS$sdev^2 / sum(pcaS$sdev^2), xlab=&quot;PC&quot;, ylab=&quot;Fraction Variation Explained&quot;, main=&quot;Scree plot&quot;) Get % variance explained for first few PCs: perc &lt;- round(100*(pcaS$sdev^2 / sum(pcaS$sdev^2))[1:10],2) names(perc) &lt;- apply(array(seq(1,10,1)), 1, function(x){paste0(&quot;PC&quot;, x)}) perc ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 ## 37.47 3.78 2.55 0.88 0.76 0.44 0.44 0.39 0.37 0.32 Use the RColorBrewer package to select a color palette: colors &lt;- RColorBrewer::brewer.pal(9, &quot;Paired&quot;) Plot first three PCs with colored symbols: par(mfrow=c(2,2), mar=c(4,4,0.5,0.5)) plot(pcaS$x[,1:2], col=colors[factor(sample_sites)], pch=16,cex=1.2) legend(&quot;bottomleft&quot;, legend=levels(factor(sample_sites)), col=colors, pch=16, ncol=3, cex=0.8) plot(pcaS$x[,2:3], col=colors[factor(sample_sites)], pch=16, cex=1.2) plot(pcaS$x[,3:4], col=colors[factor(sample_sites)], pch=16, cex=1.2) Question 4: Do you see evidence of population structure? Is the number of components (here 4) a good representation of the number of populations? c. Clustering with SNMF (similar to STRUCTURE) Now we are going to use a clustering method to examine population structure. There are many approaches, with various assumptions, and it is important to consider the underlying biology of your research organism (and your dataset size) before choosing an appropriate method. Here, we will use sparse negative matrix factorization (SNMF) because it is fast to compute for large datasets and it approximates the results of the well-known STRUCTURE algorithm. Notably, it relaxes population genetic assumptions such as Hardy-Weinberg proportions, so it may not converge on the same results as other programs. We can use SNMF to estimate the number of genetic clusters (K) among our sampled populations. However, this may take a long time. If you want to run the analysis, un-comment the lines by removing the # symbol at the beginning of each line. We use SNMFs cross-entropy criterion to infer the best estimate of K. The lower the cross-entropy, the better our model accounts for population structure. Sometimes cross-entropy continues to decline, so we might choose K where cross entropy first decreases the most. #snmf2 &lt;- LEA::snmf(paste0(here::here(), &quot;/data/stickleback.geno&quot;), # K=1:8, ploidy=2, entropy=T, alpha=100, project=&quot;new&quot;) #snmf2 &lt;- LEA::snmf(&quot;stickleback.geno&quot;, K=1:8, ploidy=2, entropy=T, # alpha=100, project=&quot;new&quot;) #par(mfrow=c(1,1)) #plot(snmf2, col=&quot;blue4&quot;, cex=1.4, pch=19) INSERT FIGURE WITH RESULT? The number of clusters is hard to determine, but four seems to be important and is similar to the results revealed by PCA. I will proceed assuming K=4. We will rerun SNMF using this setting. K=4 snmf = LEA::snmf(system.file(&quot;extdata&quot;, &quot;stickleback.geno&quot;, package = &quot;LandGenCourseData&quot;), K = K, alpha = 100, project = &quot;new&quot;) ## The project is saved into : ## urseData/extdata/stickleback.snmfProject ## ## To load the project, use: ## project = load.snmfProject(&quot;urseData/extdata/stickleback.snmfProject&quot;) ## ## To remove the project, use: ## remove.snmfProject(&quot;urseData/extdata/stickleback.snmfProject&quot;) ## ## [1] &quot;*************************************&quot; ## [1] &quot;* sNMF K = 4 repetition 1 *&quot; ## [1] &quot;*************************************&quot; ## summary of the options: ## ## -n (number of individuals) 571 ## -L (number of loci) 10000 ## -K (number of ancestral pops) 4 ## -x (input file) C:\\Users\\wagnerh1\\Documents\\R\\R-4.0.5\\library\\LandGenCourseData\\extdata\\stickleback.geno ## -q (individual admixture file) C:/Users/wagnerh1/Documents/R/R-4.0.5/library/LandGenCourseData/extdata/stickleback.snmf/K4/run1/stickleback_r1.4.Q ## -g (ancestral frequencies file) C:/Users/wagnerh1/Documents/R/R-4.0.5/library/LandGenCourseData/extdata/stickleback.snmf/K4/run1/stickleback_r1.4.G ## -i (number max of iterations) 200 ## -a (regularization parameter) 100 ## -s (seed random init) 180745462 ## -e (tolerance error) 1E-05 ## -p (number of processes) 1 ## - diploid ## ## Read genotype file C:\\Users\\wagnerh1\\Documents\\R\\R-4.0.5\\library\\LandGenCourseData\\extdata\\stickleback.geno: OK. ## ## ## Main algorithm: ## [ ] ## [=========] ## Number of iterations: 24 ## ## Least-square error: 722698.001314 ## Write individual ancestry coefficient file C:/Users/wagnerh1/Documents/R/R-4.0.5/library/LandGenCourseData/extdata/stickleback.snmf/K4/run1/stickleback_r1.4.Q: OK. ## Write ancestral allele frequency coefficient file C:/Users/wagnerh1/Documents/R/R-4.0.5/library/LandGenCourseData/extdata/stickleback.snmf/K4/run1/stickleback_r1.4.G: OK. ## ## The project is saved into : ## urseData/extdata/stickleback.snmfProject ## ## To load the project, use: ## project = load.snmfProject(&quot;urseData/extdata/stickleback.snmfProject&quot;) ## ## To remove the project, use: ## remove.snmfProject(&quot;urseData/extdata/stickleback.snmfProject&quot;) d. Plot ancestry proportions Create matrix of ancestry proportions: qmatrix = LEA::Q(snmf, K = K) Plot results with a barplot similar to that used to represent STRUCTURE results par(mar=c(4,4,0.5,0.5)) barplot(t(qmatrix), col=RColorBrewer::brewer.pal(9,&quot;Paired&quot;), border=NA, space=0, xlab=&quot;Individuals&quot;, ylab=&quot;Admixture coefficients&quot;) #Add population labels to the axis: for (i in 1:length(pops)){ axis(1, at=median(which(sample_sites==pops[i])), labels=pops[i])} e. Visualize admixture proportions on a map Import geographical coordinates for the populations: sites &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;stickleback_coordinates.csv&quot;, package = &quot;LandGenCourseData&quot;), as.is=T, check.names=F, h=T) Calculate population average ancestry proportions and create an array with population coordinates: #initialize array for ancestry proportions: qpop &lt;- matrix(NA,nrow=length(pops),ncol=K) #intialize array for coordinates: coord.pop &lt;- matrix(NA,nrow=length(pops),ncol=2) index=0 for (i in 1:length(pops)){ if (i==1){ ## i.put pop ancestry proportions for each K cluster: qpop[i,] &lt;- apply(qmatrix[1:N[i],], 2, mean) #input pop coordinates: coord.pop[i,1] &lt;- sites[which(sites[,1]==names(N[i])),6] #input pop coordinates: coord.pop[i,2] &lt;- sites[which(sites[,1]==names(N[i])),5] index = index + N[i] } else { qpop[i,] &lt;- apply(qmatrix[(index+1):(index+N[i]),], 2, mean) coord.pop[i,1] &lt;- sites[which(sites[,1]==names(N[i])),6] coord.pop[i,2] &lt;- sites[which(sites[,1]==names(N[i])),5] index = index + N[i] } } Create map with pie charts depicting ancestry proportions: par(mar=c(4,4,0.5,0.5)) plot(coord.pop, xlab = &quot;Longitude&quot;, ylab = &quot;Latitude&quot;, type = &quot;n&quot;) maps::map(database=&#39;state&#39;,add = T, col = &quot;grey90&quot;, fill = TRUE) for (i in 1:length(pops)){ mapplots::add.pie(z=qpop[i,], x=coord.pop[i,1], y=coord.pop[i,2], labels=&quot;&quot;, col=RColorBrewer::brewer.pal(K,&quot;Paired&quot;),radius=0.1) } Question 5: How does the pattern of clustering vary in space? Is there evidence of population admixture? How do you interpret this pattern biologically? Question 6: Based on the analyses done in this tutorial, where do you hypothesize the Riverbend (rb) population originated from? What evidence supports your rationale? "],["Week10.html", "Week 10: Landscape Resistance ", " Week 10: Landscape Resistance "],["WE-10.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals This worked example shows how to: Convert spatial data into weights (costs) based on different weighting approaches. Calculate landscape resistance using least-cost and commute time (analogous to circuit theory) approaches. b. Data set This code builds on the data from the GeNetIt package by Jeff Evans and Melanie Murphy that weve used already in the Week 2 Worked Example. We will use spatial data from central Idaho for Columbia spotted frogs (Murphy et al. 2010). Raster data: A raster is essentially a georeferenced matrix (i.e., a matrix with geographical co-ordinates). Here well analyze raster data that are available in the GeNetIt packages in the form of a SpatialPixelsDataFrame called rasters (for meta data, type: ?GeNetIt::rasters). These data include classified Landsat land-cover data (NLCD), spline-based climate predictions (Rehfeldt et al. 2006), and topographically derived variables (Moore et al. 1993, Evans 1972). Landscape variables were selected based on knowledge of species ecology and previous research (Murphy et al. 2010, Pilliod et al. 2002, Funk et al. 2005): cti: Compound Topographic Index (wetness) err27: Elevation Relief Ratio ffp: Frost Free Period gsp: Growing Season Precipitation hli: Heat Load Index nlcd: USGS Landcover (categorical map) Sampling locations: We will model landscape resistance between the ponds from which Colombia spotted frogs were sampled. The spatial coordinates and site data are available in the GeNEtIt package in the form of a SpatialPointsDataFrame ralu.site. The slot @coords contains UTM coordinates (zone 11), and the slot @data contains 17 site variables for 31 sites. For meta data type: ?GeNetIt::ralu.site. c. Required R libraries Install some packages needed for this worked example. if(!requireNamespace(&quot;GeNetIt&quot;, quietly = TRUE)) remotes::install_github(&quot;jeffreyevans/GeNetIt&quot;) require(LandGenCourse) #require(GeNetIt) #require(raster) #require(gdistance) d. Import rasters (gsp, err, cti, and ffp). Here we load the raster data as a SpatialPixelsDataFrame and convert them in to a raster stack. data(rasters, package=&quot;GeNetIt&quot;) RasterMaps &lt;- raster::stack(rasters) e. Import site data with sampling locations In addition to spatial data, sample locations are also needed. Sample locations are located in the data folder (RALU_UTM.csv). Read in site locations (wetlands with Columbia spotted frogs). data(ralu.site, package=&quot;GeNetIt&quot;) sites &lt;- ralu.site Question 1: What are UTMs and why might it be important to work in UTMs (as opposed to latitude and longitude)? 2. Explore the data set Explore these rasters by plotting them. As you go through this exercise, use the plot function at each step to make sure your outputs make sense. a. Plot all rasters raster::plot(RasterMaps) b. Plot spatial points over ffp raster These are the locations of the ponds where Columbia spotted frogs were sampled within the study area. par(mar=c(2,2,1,1)) raster::plot(RasterMaps$ffp) points(sites, pch=3) 3. Setting cost values and calculating conductance a. Resistance vs. conductance values The next step involves creating cost categories from the continuous data. For the purposes of this exercise, a single landscape resistance cost surface will be created, based on merging the 4 variables, and employing expert opinion. Rescaling the grids will take some processor time (depending on your computer). First, use a relative ranking of the costs of the landscape variables based on expert opinion. Higher rank indicates more resistance as follows. Landscape resistances: 1) err27, 2) ffp, 3) gsp, 4) cti Important considerations: The transition matrix in package gdistance is based on conductance and not resistance. If we are going to create a single landscape resistance (i.e., add the costs together to create one synthetic landscape variable), costs need to represent relative importance of the variables. Keep in mind there are a variety of approaches for creating landscape resistance values. This exercise implements one simplistic approach. To create relative conductance values, reverse the rank order of the resistance values: 1) cti, 2) gsp, 3) ffp, 4) err27 b. As needed: Get all rasters at the same scale Note: this step is not needed here as all rasters do already have the same resolution. Originally, the topographic variables were calculated off a 10 m DEM. The climate variables are at a 30 m resolution. In order to calculate costs, all of the rasters need to have exactly the same resolution, dimensions, and co-ordinate locations (i.e., the cells need to match up perfectly. The method of resampling (bilinear vs nearest-neighbor) depends on the type of data. Nearest-neighbor is for categorical data whereas bilinear interpolation is for continuous data. Here is some sample code to show how you would resample a raster cti (assumed to have 10 m resolution) to the dimensions of a raster gsp (with 30 m resolution): #cti &lt;- raster::resample(cti, gsp, method= &quot;bilinear&quot;) c. Calculate conductance values Functions (e.g., 1/err, ffp/5) rescale the raw data from each raster to represent costs that correspond to perceived differences among variables arising from expert opinion. A wide variety of functions could be used to achieve a variety of cost structures in order to test alternative hypotheses. The err function is inverted compared to the other variables because the raw err data have low values relative to the other variables (&lt; 1). To use the below tools, conductance values must be calculated. First, look at the range of the variable in a given raster. Then apply a function to get the desired relative conductance values. Make sure to use the plot function and visually inspect your conductance surfaces. RasterMaps$err27 ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : err27 ## values : 0.03906551, 0.7637643 (min, max) Elevation relief ratio (err27) is identifying major topographic features. In this first case, the goal is for err27 to have the lowest conductance values compared to our other landscape variables. Greater err27 means more change in topography in a given area. So, the higher the value, the more resistance to a Columbian spotted frog. Note that the function below gives a linear relationship between the raw value and the cost. Any type of functional relationship could be used however. err.cost &lt;- (1/RasterMaps$err27) err.cost ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : err27 ## values : 1.309305, 25.59803 (min, max) RasterMaps$ffp ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : ffp ## values : 0, 51 (min, max) ffp.cost &lt;- (RasterMaps$ffp/5) ffp.cost ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : ffp ## values : 0, 10.2 (min, max) RasterMaps$gsp ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : gsp ## values : 227, 338.0697 (min, max) gsp.cost &lt;- (RasterMaps$gsp-196)/15 gsp.cost ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : gsp ## values : 2.066667, 9.471311 (min, max) RasterMaps$cti ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : cti ## values : 0.8429851, 23.71476 (min, max) cti.cost &lt;- RasterMaps$cti/5 cti.cost ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : cti ## values : 0.168597, 4.742952 (min, max) d. Create a single landscape conductance raster Testing variables independently may be more appropriate depending on the research question but for the purposes of todays lab, a single conductance raster will be employed. cost1 &lt;- (gsp.cost + cti.cost + err.cost + ffp.cost) cost1 ## class : RasterLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : memory ## names : layer ## values : 9.012874, 35.90095 (min, max) Question 2: Plot your cost surface with your sample locations on top. What does this tell you? 4. Convert conductance into effective distance The higher the conductance, the lower the cost or resistance of a cell, and vice versa. We want to integrate conductance across cells to derive some measure of effective (or ecological) distance. a. Create a transition layer Transition layers are constructed from a raster, and they take the geographic references (projection, resolution, extent) from the original raster object. They also contain a matrix of probability of movement between cells which can be interpreted as conductance. Each cell in the matrix represents a cell in the original raster object. The first step is to construct a transition object based on cost1 (which is a conductance layer as calculated). This step is computationally intensive and make take a few minutes to run. Connections can be set based on 4, 8, or 16 neighbor rules. A value of 8 connects all adjacent cells in 8 directions. tr.cost1 &lt;- gdistance::transition(cost1, transitionFunction=mean, directions=8) tr.cost1 ## class : TransitionLayer ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## values : conductance ## matrix class: dsCMatrix b. Visually inspect the raster par(mar=c(2,2,1,1)) raster::plot(raster::raster(tr.cost1)) c. Correct for geometric distortion Transition values are calculated based on values of adjacent cells in the cost raster. However, we used an 8 neighbor rule and the center of diagonally connected raster cells are farther apart from each other than the orthogonally connected cells. We are using UTM co-ordinates; however, in lat-long projections, cell sizes become smaller as you move poleward. Values of the matrix have to be corrected for the first type of distortion for our analysis (and we would need to correct for the second type of distortion if we were using lat-longs). tr.cost1 &lt;- gdistance::geoCorrection(tr.cost1,type = &quot;c&quot;,multpl=FALSE) d. Plot shortest paths in space Here we do the following: Plot site locations on top of conductance raster tr.cost1 Calculate AtoB as the shortest path (least cost path) between sites 1 and 2 Plot the shortest path onto the map Note: the path is stored as a SpatialLines object (package spdep). A path is a sequence of coordinate pairs, which is stored in the @lines slot. Like a SpatialPointsDataFrame, a SpatialLines object has a proje4string attribute that stores the projection information. This means that we can easily plot it on top of a raster map. par(mar=c(2,2,1,2)) AtoB &lt;- gdistance::shortestPath(tr.cost1, origin=sites[1,], goal=sites[2,], output=&quot;SpatialLines&quot;) raster::plot(raster::raster(tr.cost1), xlab=&quot;x coordinate (m)&quot;, ylab=&quot;y coordinate (m)&quot;,legend.lab=&quot;Conductance&quot;) lines(AtoB, col=&quot;red&quot;, lwd=2) points(sites[1:2,]) Thats nice, but it shows only the shortest path between sites 1 and 2. It would not be very useful to plot all pairwise shortest paths. Here we do the following: Plot site locations on top of conductance raster tr.cost1 Use Delaunay triangulation to define neighbors (we want to retain only those paths that wont cross each other, though they may merge) Plot grey lines between neighbours For each unique pair of neighbours, calculate AtoB as the shortest path Plot the shortest paths onto the map in red Note: this may take a while. If it does not show up nicely in the notebook, copy and paste the code to the console and watch how the plot is evolving in the Plots tab (nothing beats being able to actually watch R do the work for you). par(mar=c(2,2,1,2)) raster::plot(raster::raster(tr.cost1), xlab=&quot;x coordinate (m)&quot;, ylab=&quot;y coordinate (m)&quot;, legend.lab=&quot;Conductance&quot;) points(sites) Neighbours &lt;- spdep::tri2nb(sites@coords, row.names = sites$SiteName) plot(Neighbours, sites@coords, col=&quot;darkgrey&quot;, add=TRUE) for(i in 1:length(Neighbours)) { for(j in Neighbours[[i]][Neighbours[[i]] &gt; i]) { AtoB &lt;- gdistance::shortestPath(tr.cost1, origin=sites[i,], goal=sites[j,], output=&quot;SpatialLines&quot;) lines(AtoB, col=&quot;red&quot;, lwd=1.5) } } Question 3: Do the shortest paths (red) deviate from the Euclidean distances (grey)? Why? Question 4: Are there paths that merge? How would you explain this? 5. Create cost-distance matrices a. Least cost distance The cost-distance matrix is based on the corrected transition layer and the site locations (here we supply a SpatialPointsDataFrame object). The costDistance function requires conductance values, even though cost distance will be 1/conductance (i.e., resistance). The cost distance is a function of the transition (tr.cost1, a transition object) and spatial locations (sites, a spatial object). This is a single least-cost path (accumulated cost units as opposed to length units) between each pair of sites.. cost1.dist &lt;- gdistance::costDistance(tr.cost1,sites) b. Cost-distance matrix based on random paths (similar to Circuitscape) Commute-time distance is an alternative to effective resistance from circuit theory. Commute-time is the expected time it takes for a random walk between nodes, and has been shown to correlate highly with effective resistance (Marrotte and Bowman 2017). comm1.dist &lt;- gdistance::commuteDistance(x = tr.cost1, coords = sites) c. Compare cost distances Create a distance table to compare different cost distance estimates: dist_df &lt;- data.frame(&quot;cost1.dist&quot;=as.numeric(cost1.dist), &quot;comm1.dist&quot;=as.numeric(comm1.dist)) Look at the correlation between the two different cost distances: corr.LCD.comm &lt;- cor(dist_df$cost1.dist, dist_df$comm1.dist, method = &quot;spearman&quot;) corr.LCD.comm ## [1] 0.9519704 plot(cost1.dist~comm1.dist) Question 5: Are the distance measures similar or different? Why? Question 6: What is the major difference? What are the implications of this difference? When might you use one or other of the methods? 6. How does changing resolution affect these metrics? Create a loop that runs through a sequential coarsening of the raster and calculate these metrics and compare them to the finer resolution raster metric using a Spearman correlation. The loop will take a few minutes to run. a. Create loop cor_cost &lt;- c() cor_comm &lt;- c() res_fact &lt;- seq(2,20,2) for(fac in res_fact){ cost1_agg &lt;- raster::aggregate(cost1, fact = fac) tr.cost_agg &lt;- gdistance::transition(cost1_agg, transitionFunction=mean, directions=8) tr.cost_agg &lt;- gdistance::geoCorrection(tr.cost_agg,type = &quot;c&quot;,multpl=FALSE) cost.dist_agg &lt;- gdistance::costDistance(tr.cost_agg,sites) comm.dist_agg &lt;- gdistance::commuteDistance(x = tr.cost_agg, coords = sites) cost.dist_agg &lt;- as.numeric(cost.dist_agg) comm.dist_agg &lt;- as.numeric(comm.dist_agg) cor_cost &lt;- c(cor_cost,cor(dist_df$cost1.dist, cost.dist_agg, method = &quot;spearman&quot;)) cor_comm &lt;- c(cor_comm,cor(dist_df$comm1.dist, comm.dist_agg, method = &quot;spearman&quot;)) } b. Plot the results par(mar=c(4,4,1,1)) plot(y = cor_cost, x = res_fact, col = &quot;red&quot;, pch = 19, ylim = c(0.9,1), xlab = &quot;Aggregation factor&quot;, ylab = &quot;Spearman correlation&quot;) points(y = cor_comm, x = res_fact, col = &quot;blue&quot;, pch = 19) legend(&quot;bottomleft&quot;, legend = c(&quot;Costdist&quot;,&quot;Commdist&quot;), pch = 19, col = c(&quot;red&quot;, &quot;blue&quot;)) Question 7: What effect does changing resolution have on these metrics? 7. References Evans IS (1972) General geomorphometry, derivatives of altitude, and descriptive statistics. In: Chorley RJ (ed) Spatial analysis in geomorphology. Harper &amp; Row, New York. Funk WC, Blouin MS, Corn PS, Maxell BA, Pilliod DS, Amish S, Allendorf FW (2005) Population structure of Columbia spotted frogs (Rana luteiventris) is strongly affected by the landscape. Mol Ecol 14(2): 483-496 Marrotte RR, Bowman J (2017) The relationship between least-cost and resistance distance. PLOS ONE 12(3): e0174212 McRae BH, Dickson BG, Keitt TH, Shah VB (2008) Using circuit theory to model connectivity in ecology, evolution, and conservation. Ecology 89 (10):2712-2724 Moore I, Gessler P, Nielsen G, Petersen G (eds) (1993) Terrain attributes and estimation methods and scale effects. Modeling change in environmental systems. Wiley, London. Murphy MA, Dezanni RJ, Pilliod D, Storfer A (2010) Landscape genetics of high mountain frog populations. Mol Ecol 19:3634-3649. Pilliod DS, Peterson CR, Ritson PI (2002) Seasonal migration of Columbia spotted frogs (Rana luteiventris) among complementary resources in a high mountain basin. Can J Zool 80: 1849-1862 Rehfeldt GE, Crookston NL, Warwell MV, Evans JS (2006) Empirical analysis of plan-climate relationships for western United States. International Journal of Plan Sciences 167: 1123-1150 "],["Week11.html", "Week 11: Detecting Adaptation ", " Week 11: Detecting Adaptation "],["WE-11.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals This worked example will illustrate the use of two types of genotype-environment association analyses, one univariate and one multivariate. Specifically, you will learn: Strategies for screening and preparing predictor variables for different GEA analyses; How to run and interpret a Latent Factor Mixed Model (LFMM); One option for post-processing LFMM results using the genomic inflation factor (GIF) and false discovery rate (FDR) to correct for multiple tests; How to run and interpret Redundancy Analysis for GEA. b. Data set We will reanalyze genomic data from 94 North American gray wolves (Canis lupus) sampled across Canada and Alaska (Schweizer et al., 2016). We are interested in understanding how wolves may be locally adapted to environmental conditions across their North American range. The genetic data are individual-based, and are input as allele counts (i.e. 0/1/2) for each locus. In the interest of computational efficiency, we will use a randomly sampled subset of 10,000 single nucleotide polymorphism (SNP) markers from the full data set (which contains 42,587 SNPs). In addition, we have eight environmental predictors that are ecologically relevant and are not highly correlated (|r| &lt; 0.7). This is a reduced set of predictors from the 12 originally included by Schweizer et al. (2016). c. Required R libraries You may need to install packages qvalue from BioClim and lfmm from GitHub: if(!requireNamespace(&quot;qvalue&quot;, quietly = TRUE)) { if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(version = &quot;3.12&quot;) BiocManager::install(&quot;qvalue&quot;) } if(!requireNamespace(&quot;lfmm&quot;, quietly = TRUE)) { remotes::install_github(&quot;bcm-uga/lfmm&quot;) } library(LandGenCourse) library(vegan) # Used to run PCA &amp; RDA library(lfmm) # Used to run LFMM library(qvalue) # Used to post-process LFMM output 2. Import and prepare the data a. Import the genetic data I downloaded these data from the Schweizer et al. (2016) Dryad repository and converted them from .tped to .raw format using plink (Purcell et al., 2007). Then, using the R package adegenet (Jombart 2008), I read in the .raw data and extracted the matrix of 94 individuals x 42,587 SNPs. Finally, I randomly sampled 10,000 columns (SNPs) from the full data set, which is what we will analyze in this worked example. The full data in .raw format are available in the Supplemental Information for Forester et al. (2018). If you want to analyze the full data set, use the read.PLINK call from adegenet to read the data into R. gen &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;wolf_geno_samp_10000.csv&quot;, package = &quot;LandGenCourse&quot;), row.names=1) dim(gen) ## [1] 94 10000 We have 94 individuals (rows) genotyped at 10,000 SNPs (columns). Both LFMM and RDA require complete data frames (i.e., no missing genetic data). For this example, well use a simple approach to imputing missing genotype values: we will impute using the most common genotype at each SNP across all individuals. sum(is.na(gen)) ## 2.,987 NAs in the matrix (~3% missing data) ## [1] 27987 gen.imp &lt;- apply(gen, 2, function(x) replace(x, is.na(x), as.numeric(names(which.max(table(x)))))) sum(is.na(gen.imp)) # No NAs ## [1] 0 We could also use this imputation approach within ecotypes (rather than across all individuals). Other promising imputation methods for species lacking a reference genome include: using ancestry values from snmf in the LEA package (Frichot &amp; Francois 2015), and the program LinkImpute (Money et al., 2015). b. Import the environmental data The original data set comes with 12 predictors, but many of them are highly correlated, which can cause problems for regression-based methods like LFMM and RDA. I conducted variable reduction using the |0.7| rule of thumb (Dormann et al., 2013) and an ecological interpretation of the relevance of possible predictors. Can you double check the correlations among the variables? For more information on the rationale for variable reduction, see the full RDA vignette. env &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;EnvironmentalData_8pred.csv&quot;, package = &quot;LandGenCourse&quot;)) str(env) # Look at the structure of the data frame ## &#39;data.frame&#39;: 94 obs. of 12 variables: ## $ individual : chr &quot;11226.CEL&quot; &quot;11228.CEL&quot; &quot;11232_CLU_NQUE-I&quot; &quot;11234_CLU_NQUE-I&quot; ... ## $ ecotype : chr &quot;Pop_2_BorealForest&quot; &quot;Pop_6_AtlanticForest&quot; &quot;Pop_6_AtlanticForest&quot; &quot;Pop_6_AtlanticForest&quot; ... ## $ long : num -94.5 -88.1 -72 -72 -114.8 ... ## $ lat : num 49.8 49.1 58.8 58.6 60.6 ... ## $ ann_mean_temp : int 23 15 -69 -66 -34 -37 -35 -47 -46 -20 ... ## $ mean_diurnal_range: int 97 115 75 76 102 114 112 99 109 115 ... ## $ temp_seasonality : int 13047 11408 11831 11867 14259 14806 15176 15844 15312 14099 ... ## $ ann_precip : int 610 784 483 477 335 319 372 339 284 375 ... ## $ precip_seasonality: int 47 29 39 40 39 34 43 48 36 46 ... ## $ ndvi : int 7403 8222 6450 6141 7595 7518 7472 8157 7348 6616 ... ## $ elev : int 349 234 264 200 194 219 280 154 190 222 ... ## $ percent_tree_cover: int 36 51 5 9 26 48 47 61 61 47 ... env$individual &lt;- as.character(env$individual) # Make individual names characters (not factors) # Confirm that genotypes and environmental data are in the same order identical(rownames(gen.imp), env[,1]) ## [1] TRUE Now well subset just the environmental predictors &amp; shorten their names: pred &lt;- env[,5:12] colnames(pred) &lt;- c(&quot;AMT&quot;,&quot;MDR&quot;,&quot;sdT&quot;,&quot;AP&quot;,&quot;cvP&quot;,&quot;NDVI&quot;,&quot;Elev&quot;,&quot;Tree&quot;) For the univariate LFMM test, we could run a separate set of tests for each of these eight predictors; this would be a test for each of the 10,000 SNPs with each predictor = 80,000 tests (!). Instead, for LFMM, well perform a PCA on the environmental predictors and use the first principal component (PC) as a synthetic predictor. This will reduce our ability to interpret the output, since the PC predictor will be a linear combination of the original eight variables, but it will reduce the number of corrections needed for multiple tests. Your decision of how to handle multiple predictors for a univarate GEA test will depend on the study goals and characteristics of the data set. There are many ways to run PCA in R; well use the rda function in vegan (Oksanen et al., 2016). Well center and scale the predictors (scale=T), since theyre in different units. Well then determine the proportion of the environmental variance explained by each PC axis &amp; investigate how the original predictors correlate with the first PC axis. pred.pca &lt;- rda(pred, scale=T) summary(pred.pca)$cont ## $importance ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Eigenvalue 3.224 1.8354 1.1258 0.70269 0.48967 0.34983 0.21138 ## Proportion Explained 0.403 0.2294 0.1407 0.08784 0.06121 0.04373 0.02642 ## Cumulative Proportion 0.403 0.6324 0.7731 0.86096 0.92217 0.96590 0.99232 ## PC8 ## Eigenvalue 0.061457 ## Proportion Explained 0.007682 ## Cumulative Proportion 1.000000 screeplot(pred.pca, main = &quot;Screeplot: Eigenvalues of Wolf Predictor Variables&quot;) ## c.rrelations between the PC axis and predictors: round(scores(pred.pca, choices=1:8, display=&quot;species&quot;, scaling=0), digits=3) ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## AMT -0.519 0.029 -0.040 0.198 -0.091 -0.297 -0.436 -0.634 ## MDR -0.315 -0.531 0.258 -0.051 -0.190 -0.259 -0.332 0.579 ## sdT 0.193 -0.586 -0.225 -0.203 0.340 0.464 -0.372 -0.244 ## AP -0.353 0.468 -0.071 0.212 0.228 0.516 -0.387 0.372 ## cvP 0.319 -0.219 0.070 0.888 -0.199 0.130 -0.033 -0.007 ## NDVI -0.404 -0.253 -0.220 0.281 0.618 -0.161 0.481 0.077 ## Elev -0.150 -0.054 0.876 -0.029 0.174 0.301 0.178 -0.231 ## Tree -0.426 -0.200 -0.233 -0.064 -0.580 0.480 0.385 -0.071 ## attr(,&quot;const&quot;) ## [1] 5.222678 40% of the variance in the predictors is explained by the first PC axis, and 23% by the second axis. We could follow up with an LFMM model using the second axis as a predictor, if we wanted. The strongest correlations with PC1 are annual mean temperature (AMT), tree cover (Tree), NDVI, and annual precipitation (AP). Well store our synthetic PC axis predictor as pred.PC1 for use in LFMM. pred.PC1 &lt;- scores(pred.pca, choices=1, display=&quot;sites&quot;, scaling=0) 3. Latent Factor Mixed Models (LFMM): a univariate GEA LFMM is a univariate test, which means that it builds a model for each SNP and each predictor variable. In this case, we will run 10,000 SNPs x 1 synthetic predictor = 10,000 separate LFMM tests. a. Determine K LFMM requires an estimate of the number of populations in the data (K). To determine the most likely value of K, well use PCA, noting that there are many different approaches for determining K from genetic data (more below). It is also reasonable to run LFMM with different values of K, if there is uncertainty. Well use a broken stick criterion to determine K. The broken stick stopping rule states that principal components should be retained as long as observed eigenvalues are higher than corresponding random broken stick components. For example, if we reran the environmental PCA screeplot from above with a broken stick criterion: screeplot(pred.pca, main = &quot;Screeplot of Wolf Predictor Variables with Broken Stick&quot;, bstick=TRUE, type=&quot;barplot&quot;) You can see that PC1 and PC2 explain more than the random broken stick components, while PC3 + do not. If this were genomic data, and we were determining a value of K using this approach, wed set K = 3. Now lets run a PCA with the genommic data and plot the eigenvalues with the broken stick criterion: gen.pca &lt;- rda(gen.imp, scale=T) screeplot(gen.pca, main = &quot;Screeplot of Genetic Data with Broken Stick&quot;, bstick=TRUE, type=&quot;barplot&quot;) For the genomic data, we can see that none of the PCs have eigenvalues greater than random (greater than the broken stick values in red). This effectively means that K=1 for the wolf data set, based on a PCA assessment. [Note that we see a similar result using the full genomic data set of 42K SNPs]. This does not mean there isnt genetic structure in the data; it just means the structure isnt particularly strong and/or easily partitioned into discrete groupings. In the original paper, the authors determined K using Structure. They found support for multiple K values between 3 and 7. The wolf data show an isolation by distance signature, which can confound methods used to identify K and contribute to uncertainty. How do you suspect that differences in K (say K=1 vs. K=3) for LFMM might influence the GEA results? For now, well use K=3 following the original manuscript. It would be interesting to compare K=1 to these results, if you have time. Note that with K=1, LFMM is essentially running a simple linear regression (i.e., no latent factors). K &lt;- 3 b. Run LFMM LFMM is a regression model that includes unobserved variables (latent factors, set with K) that correct the model for confounding effects, such as population structure. The latent factors are estimated simultaneously with the environmental and response variables, which can help improve power when environment and demography are correlated. The previous version of LFMM (v1.5, implemented in the LEA package) uses an MCMC (Markov chain Monte Carlo) algorithm to identify GEAs while correcting for confounding. MCMC made it (very!) time-intensive for large data sets. LFMM v.2 computes LFMMs for GEA using a least-squares estimation method that is substantially faster than v1.5 (Caye et al., 2019). There are two penalty approaches: ridge and lasso. Well use ridge today (see ?lfmm_ridge); see Jumentier et al. (2020) for more information on the ridge vs. lasso penalties. wolf.lfmm &lt;- lfmm_ridge(Y=gen.imp, X=pred.PC1, K=K) ## c.ange K as you see fit That was fast! As youll see, post-processing these results is what takes up most of our time and effort c. Identify LFMM candidates using False Discovery Rate Next, we post-process the model output. We will move fast here; I strongly recommend reading Controlling false discoveries in genome scans for selection (Francois et al., 2016) if you will be running these post-processing steps on your own data! Decisions made here can dramatically impact the candidate markers you identify. The steps for post-processing are: Look at the genomic inflation factor (GIF), which gives us a sense for how well the model has accounted for confounding factors in the data. Plot the p-values to see how application of the GIF influences the p-value distribution. Modify the GIF (if needed) and re-plot the p-values to identify the best possible fit to the ideal p-value distribution. Apply a False Discovery Rate control method to the p-values by converting to q-values. Identify candidates as those below a given FDR threshold. The False Discovery Rate is the expected proportion of false positives among the list of positive tests (see Storey and Tibshirani, 2003). An essential point to understand here is that the FDR is predicated on the ideal p-value distribution (flat with a peak at 0, see Francois et al. 2016). For example, an FDR threshold of 0.10 applied to a dataset with an ideal p-value distribution would produce 10% false positives (aka false discoveries) among the set of positive tests. However, if the p-value distribution deviates from the ideal, this same FDR threshold of 0.10 would produce more or fewer false discoveries among the set of postive tests, depending on the skew in the p-value distribution. So remember: having an actual FDR that is in accordance with the nominal threshold you set is completely dependent on the p-values used. The lfmm package has a nice built-in function to calculate test statistics for the predictor(s), see ?lfmm_test: wolf.pv &lt;- lfmm_test(Y=gen.imp, X=pred.PC1, lfmm=wolf.lfmm, calibrate=&quot;gif&quot;) names(wolf.pv) # this object includes raw z-scores and p-values, as well as GIF-calibrated scores and p-values ## [1] &quot;B&quot; &quot;epsilon.sigma2&quot; &quot;B.sigma2&quot; ## [4] &quot;score&quot; &quot;pvalue&quot; &quot;gif&quot; ## [7] &quot;calibrated.score2&quot; &quot;calibrated.pvalue&quot; Lets look at the genomic inflation factor (GIF): wolf.pv$gif ## PC1 ## 2.772992 An appropriately calibrated set of tests will have a GIF of around 1. The elevated GIF for our tests indicates that the results may be overly liberal in identifying candidate SNPs. If the GIF is less than one, the test may be too conservative. NOTE: Changing the value of K influences the GIF, so additional tests using the best value of K +/- 1 may be needed in some cases. See Francois et al. (2016) for more details. Lets look at how application of the GIF to the p-values impacts the p-value distribution: hist(wolf.pv$pvalue[,1], main=&quot;Unadjusted p-values&quot;) hist(wolf.pv$calibrated.pvalue[,1], main=&quot;GIF-adjusted p-values&quot;) We want to see a relatively flat histogram (most loci not under selection) with a peak near zero, indicating candidate adaptive markers. We see a very large peak with the unadjusted p-values, and a much smaller peak with the GIF-adjusted p-values (note differences in the scale of the y-axis). Note that you can choose a different GIF and readjust the p-values to get a better histogram distribution (that is, a distribution that conforms best with what is expected under a well-calibrated set of models, see Francois et al, 2016). This process is subjective and can be difficult with empirical data sets, especially those with an IBD signature, such as these wolf data. Remember, you can also change the value of K in your lfmm models and see how this impacts the GIF. Below Ill show you how to manually adjust the GIF correction factor: # Let&#39;s change the GIF and readjust the p-values: zscore &lt;- wolf.pv$score[,1] # zscores for first predictor, we only have one in our case... (gif &lt;- wolf.pv$gif[1]) ## d.fault GIF for this predictor ## PC1 ## 2.772992 new.gif1 &lt;- 2.0 ## c.oose your new GIF # Manual adjustment of the p-values: adj.pv1 &lt;- pchisq(zscore^2/new.gif1, df=1, lower = FALSE) Plot the p-value histograms: hist(wolf.pv$pvalue[,1], main=&quot;Unadjusted p-values&quot;) hist(wolf.pv$calibrated.pvalue[,1], main=&quot;GIF-adjusted p-values (GIF=2.8)&quot;) hist(adj.pv1, main=&quot;REadjusted p-values (GIF=2.0)&quot;) For now, well stick with the default GIF calculated by the lfmm package, though it looks like the application of the GIF may be a bit conservative (e.g. it is compressing the peak at 0). Finally, we convert the adjusted p-values to q-values. q-values provide a measure of each SNPs significance, automatically taking into account the fact that thousands are simultaneously being tested. We can then use an FDR threshold to control the number of false positive detections (given that our p-value distribution is well-behaved). wolf.qv &lt;- qvalue(wolf.pv$calibrated.pvalue)$qvalues length(which(wolf.qv &lt; 0.1)) ## h.w many SNPs have an FDR &lt; 10%? ## [1] 12 (wolf.FDR.1 &lt;- colnames(gen.imp)[which(wolf.qv &lt; 0.1)]) ## i.entify which SNPs these are ## [1] &quot;chr6.36999927&quot; &quot;chr20.36354466&quot; &quot;chr17.54224832&quot; &quot;chr12.58928631&quot; ## [5] &quot;chr12.55629962&quot; &quot;chr8.41109194&quot; &quot;chr19.17640345&quot; &quot;chr8.63918041&quot; ## [9] &quot;chr20.10469494&quot; &quot;chr22.64010061&quot; &quot;chr5.25188969&quot; &quot;chr9.36496633&quot; Using K=3, the default GIF correction, and an FDR threshold of 0.10, we only detect 12 candidate SNPs under selection in response to our PC1 environmental predictor. What changes could we make to have a less conservative test? 4. Redundancy Analysis (RDA): a multivariate GEA a. Run RDA RDA is a multivariate ordination technique that can analyze many loci and environmental predictors simultaneously. For this reason, we can input all of the SNPs and environmental predictors at once, with no need to correct for multiple tests. RDA determines how groups of loci covary in response to the multivariate environment, and can better detect processes that result in weak, multilocus molecular signatures relative to univariate tests (Rellstab et al., 2015; Forester et al., 2018). RDA can be used on both individual and population-based sampling designs. The distinction between the two may not be straightforward in all cases. A simple guideline would be to use an individual-based framework when you have individual coordinates for most of your samples, and the resolution of your environmental data would allow for a sampling of environmental conditions across the site/study area. For population-level data, you would input the genetic data as allele frequencies within demes. The code to run the RDA is simple. However, I highly recommend reading Borcard et al. (2011) for details on the implementation and interpretation of RDA models and the objects created by vegan. RDA runs relatively quickly on most data sets, though you may need a high memory node on a cluster for very large data sets (i.e., millions of SNPs). wolf.rda &lt;- rda(gen.imp ~ ., data=pred, scale=T) wolf.rda ## Call: rda(formula = gen.imp ~ AMT + MDR + sdT + AP + cvP + NDVI + Elev ## + Tree, data = pred, scale = T) ## ## Inertia Proportion Rank ## Total 8307.000 1.000 ## Constrained 1113.488 0.134 8 ## Unconstrained 7193.512 0.866 85 ## Inertia is correlations ## ## Eigenvalues for constrained axes: ## RDA1 RDA2 RDA3 RDA4 RDA5 RDA6 RDA7 RDA8 ## 281.19 216.87 179.53 110.84 89.78 87.15 75.93 72.20 ## ## Eigenvalues for unconstrained axes: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## 261.51 211.68 197.60 171.80 127.67 121.61 115.54 110.04 ## (Showing 8 of 85 unconstrained eigenvalues) First, note that we will have as many constrained (RDA) axes as we have predictors in the model. All residual variance is then modeled by PCA (the unconstrained PC axes). The proportion of the variance explained by the environmental predictors is given under the Proportion column for Constrained; this is equivalent to the R2 of a multiple regression. Just like in multiple regression, this R2 will be biased and should be adjusted based on the number of predictors. We can calculate the adjusted R2 using: RsquareAdj(wolf.rda) ## $r.squared ## [1] 0.1340421 ## ## $adj.r.squared ## [1] 0.05254019 Our constrained ordination explains about 5% of the variation; this low explanatory power is not surprising given that we expect that most of the SNPs in our dataset will not show a relationship with the environmental predictors (e.g., most SNPs will be neutral). The eigenvalues for the constrained axes reflect the variance explained by each canonical axis: summary(wolf.rda)$concont ## $importance ## Importance of components: ## RDA1 RDA2 RDA3 RDA4 RDA5 RDA6 ## Eigenvalue 281.1906 216.8651 179.5282 110.84337 89.78143 87.14904 ## Proportion Explained 0.2525 0.1948 0.1612 0.09955 0.08063 0.07827 ## Cumulative Proportion 0.2525 0.4473 0.6085 0.70807 0.78870 0.86697 ## RDA7 RDA8 ## Eigenvalue 75.92523 72.20485 ## Proportion Explained 0.06819 0.06485 ## Cumulative Proportion 0.93515 1.00000 We can visualize this information using a screeplot of the canonical eigenvalues by calling screeplot: screeplot(wolf.rda) Here, we can see that the first three constrained axes explain most of the variance. The screeplot provides an informal (and quick) way to determine how many constrained axes to include when we search for candidate SNPs (below). We could start by investigating RDA axes that explain the most variance (excluding those after the drop off point in the screeplot.) You can run a formal test of statistical significance of each constrained axis using: anova.cca(wolf.rda, by=\"axis\"). We can assess both the full model and each constrained axis using F-statistics (Legendre et al, 2010). The null hypothesis is that no linear relationship exists between the SNP data and the environmental predictors. See ?anova.cca for more details and options. The screeplot and a formal test (by axis) are both reasonable approaches for determining which RDA axes to assess for candidate SNPs. The permutation process to test the signficiance of each axis takes a while (up to a few hours on large data sets), so well just use the screeplot for a first assessment. If we did run the formal test, we would find that the first three constrained axes are significant (p = 0.001); constrained axis 4 has a p-value of 0.080, while axes 5-8 have p-values &gt; 0.850. This corresponds with our evaluation of the screeplot, above. Finally, vegan has a simple function for checking Variance Inflation Factors for the predictor variables used in the model. This helps identify redundant predictors: vif.cca(wolf.rda) ## AMT MDR sdT AP cvP NDVI Elev Tree ## 7.854243 6.495892 2.775059 4.051610 1.318631 2.285632 2.028377 2.260139 All values are below 10, and most are below 5, which indicates that multicollinearity among these predictors shouldnt be a problem (Zuur et al., 2010). Lets make a quick plot of the RDA output using the default plotting in vegan: plot(wolf.rda, scaling=3) ## d.fault is axes 1 and 2 Here, the SNPs are in red (in the center of each plot), and the individuals are the black circles. The blue vectors are the environmental predictors. The relative arrangement of these items in the ordination space reflects their relationship with the ordination axes, which are linear combinations of the predictor variables. See the full RDA vignette for details on how to make more informative (and prettier!) RDA plots for this data set. For example, we could more clearly visualize the identified candidate loci in the ordination space and see how they are linked to the environmental predictors. We could also use RDA to investigate how wolf ecotypes (based on individual genotypes) are distributed in relation to the environmental predictors (Forester et al., 2018, Figures 9 &amp; 10). b. Identify RDA candidates Well use the loadings of the SNPs (their location) in the ordination space to determine which SNPs are candidates for local adaptation. The SNP loadings are stored as species in the RDA object. Well extract the SNP loadings from the first three constrained axes, based on our assessment of the screeplot above. load.rda &lt;- summary(wolf.rda)$species[,1:3] If we look at histograms of the loadings on each RDA axis, we can see their (relatively normal) distribution. SNPs loading at the center of the distribution are not showing a relationship with the environmental predictors; those loading in the tails are, and are more likely to be under selection as a function of those predictors (or some other predictor correlated with them). hist(load.rda[,1], main=&quot;Loadings on RDA1&quot;) hist(load.rda[,2], main=&quot;Loadings on RDA2&quot;) hist(load.rda[,3], main=&quot;Loadings on RDA3&quot;) Ive written a simple function to identify SNPs that load in the tails of these distributions. Well start with a 3 standard deviation cutoff (two-tailed p-value = 0.0027). As with all cutoffs, this can be modified to reflect the goals of the analysis and our tolerance for true positives vs. false positives. For example, if you needed to be very conservative and only identify those loci under very strong selection (i.e., minimize false positive rates), you could increase the number of standard deviations to 3.5 (two-tailed p-value = 0.0005). This would also increase the false negative rate. If you were less concerned with false positives, and more concerned with identifying as many potential candidate loci as possible (including those that may be under weaker selection), you might choose a 2.5 standard deviation cutoff (two-tailed p-value = 0.012). I define the function here as outliers, where x is the vector of loadings and z is the number of standard deviations to use: outliers &lt;- function(x,z){ lims &lt;- mean(x) + c(-1, 1) * z * sd(x) ## f.nd loadings +/- z SD from mean loading x[x &lt; lims[1] | x &gt; lims[2]] # locus names in these tails } Now lets apply it to the first three constrained axes: cand1 &lt;- outliers(load.rda[,1], 3) ## 3. cand2 &lt;- outliers(load.rda[,2], 3) ## 6. cand3 &lt;- outliers(load.rda[,3], 3) ## 3. wolf.rda.cand &lt;- c(names(cand1), names(cand2), names(cand3)) ## j.st the names of the candidates length(wolf.rda.cand[duplicated(wolf.rda.cand)]) ## 7.duplicate detections (detected on multiple RDA axes) ## [1] 7 wolf.rda.cand &lt;- wolf.rda.cand[!duplicated(wolf.rda.cand)] ## 1.4 unique candidates Lets see where these candidate SNPs are in the ordination space. Well zoom the plot in to just the SNPs, and color code candidate SNPs in red: # Set up the color scheme for plotting: bgcol &lt;- ifelse(colnames(gen.imp) %in% wolf.rda.cand, &#39;gray32&#39;, &#39;#00000000&#39;) snpcol &lt;- ifelse(colnames(gen.imp) %in% wolf.rda.cand, &#39;red&#39;, &#39;#00000000&#39;) ## a.es 1 &amp; 2 - zooming in to just the SNPs here... plot(wolf.rda, type=&quot;n&quot;, scaling=3, xlim=c(-1,1), ylim=c(-1,1), main=&quot;Wolf RDA, axes 1 and 2&quot;) points(wolf.rda, display=&quot;species&quot;, pch=21, cex=1, col=&quot;gray32&quot;, bg=&#39;#f1eef6&#39;, scaling=3) points(wolf.rda, display=&quot;species&quot;, pch=21, cex=1, col=bgcol, bg=snpcol, scaling=3) text(wolf.rda, scaling=3, display=&quot;bp&quot;, col=&quot;#0868ac&quot;, cex=1) ## a.es 2 &amp; 3 plot(wolf.rda, type=&quot;n&quot;, scaling=3, xlim=c(-1,1), ylim=c(-1,1), choices=c(2,3), main=&quot;Wolf RDA, axes 2 and 3&quot;) points(wolf.rda, display=&quot;species&quot;, pch=21, cex=1, col=&quot;gray32&quot;, bg=&#39;#f1eef6&#39;, scaling=3, choices=c(2,3)) points(wolf.rda, display=&quot;species&quot;, pch=21, cex=1, col=bgcol, bg=snpcol, scaling=3, choices=c(2,3)) text(wolf.rda, scaling=3, display=&quot;bp&quot;, col=&quot;#0868ac&quot;, cex=1, choices=c(2,3)) Which environmental predictor(s) seem to be most strongly related to candidate SNPs in the RDA1/RDA2 plot? How about the RDA2/RDA3 plot? Why do we see candidate SNPs loading in the center of the ordination space in both plots? Lets see which environmental predictors are most strongly correlated with the first three RDA axes: intersetcor(wolf.rda)[,1:3] ## RDA1 RDA2 RDA3 ## AMT -0.8692103 0.10797423 0.19767975 ## MDR -0.4791404 0.77837759 -0.09163633 ## sdT 0.2971194 0.47503944 -0.03234453 ## AP -0.6902664 -0.62757476 0.09806042 ## cvP 0.3155829 0.02749876 -0.64084267 ## NDVI -0.6531881 0.24697034 0.07226876 ## Elev -0.3473015 0.24167932 -0.33073363 ## Tree -0.5699345 0.21080592 0.03478928 Generally, candidate SNPs on axis 1 represent multilocus sets of SNP genotypes associated most strongly with annual mean temperature and annual precipitation; SNPs on axis 2 represent genotypes associated with mean diurnal range; and SNPs on axis 3 represent genotypes associated with precipitation seasonality. See the full RDA vignette for additional investigation of candidate SNPs. 5. Compare LFMM and RDA candidates Lets see what kind of overlap we have in our candidates from the two methods. Remember that we had only 12 candidates for LFMM and 134 candidates for RDA. intersect(wolf.FDR.1, wolf.rda.cand) ## f.und by both LFMM and RDA ## [1] &quot;chr6.36999927&quot; &quot;chr20.36354466&quot; &quot;chr17.54224832&quot; &quot;chr12.58928631&quot; ## [5] &quot;chr12.55629962&quot; &quot;chr8.41109194&quot; &quot;chr19.17640345&quot; &quot;chr8.63918041&quot; ## [9] &quot;chr20.10469494&quot; &quot;chr22.64010061&quot; &quot;chr9.36496633&quot; setdiff(wolf.FDR.1, wolf.rda.cand) # unique to LFMM ## [1] &quot;chr5.25188969&quot; We see a lot of overlap, even though the tests differ (e.g., correction for population structure, univariate vs. multivariate tests, differences in predictor variables, differences in post-processing). An important note: A common approach in studies that use GEAs and populations differentiation methods is to use many tests and then look for the overlap across the detections. This can be helpful for some questions (e.g. where minimizing false positive detections is of greatest importance), but can be overly conservative in many cases and bias detections against weaker signatures of selection. See our paper for an investigation of different approaches for combining detections &amp; how it impacts true and false positive rates: Forester et al. (2018); and see the (excellent!) FDR paper for more options based on combining z-scores across different tests: Francois et al. (2016). 6. A quick note on controlling for population structure in RDA We are still testing approaches to controlling for population structure in RDA. Weve generally found that using ancestry values (e.g. from snmf or Admixture) or MEMs (Morans Eigenvector Maps, aka spatial eigenvectors) is overly conservative. So far, the best approach (e.g. balancing true positives and false negatives) is to compute a PCA of the genetic data and select relevant PCs (e.g. using the broken stick criteron as we did above), then use retained PCs in a partial RDA: ?rda ## h.lp for running RDA # pseudo code for simple RDA: foo &lt;- rda(genomic.data ~ predictor1 + predictor2, data=dat # pseudo code for partial RDA (correcting for population structure with PCs): bar &lt;- rda(genomic.data ~ predictor1 + predictor2 + Condition(PC1), data=dat, scale=T) You will want to check for correlations between retained PCs and your environmental predictors. If environment and population structure are not correlated, you can retain all predictors and PCs and proceed with your partial RDA. If environment and population structure are highly correlated, youll have to decide whether to prune correlated environmental variables or PCs. Pruning environmental variables will likely reduce false positives but will also increase false negatives. Pruning PCs (instead of environmental predictors) will likely increase false positives, but will also reduce false negatives. These trade-offs exist with all approaches for controlling population structure and other confounding factors when environment is correlated with population structure. 7. References Borcard D, Gillet F, Legendre P (2011) Numerical Ecology with R. Springer, New York. Caye K, Jumentier B, Lepeule J, Francois O (2019) LFMM 2: Fast and Accurate Inference of Gene-Environment Associations in Genome-Wide Studies. Molecular Biology and Evolution 36: 852-860. Dormann CF, Elith J, Bacher S, et al. (2013) Collinearity: a review of methods to deal with it and a simulation study evaluating their performance. Ecography, 36: 27-46. Forester BR, Lasky JR, Wagner HH, Urban DL (2018) Comparing methods for detecting multilocus adaptation with multivariate genotype-environment associations. Molecular Ecology. Forester BR (2018) Vignette: Detectingn multilocus adaptation using Redundancy Analysis (RDA). Population Genetics in R: popgen.nescent.org. Francois O, Martins H, Caye, K, Schoville S (2016) Controlling false discoveries in genome scans for selection. Molecular Ecology, 25: 454-469. Frichot E, Francois O (2015) LEA: An R package for landscape and ecological association studies. Methods in Ecology and Evolution, 6: 925-929. Jombart, T (2008) adegenet: a R package for the multivariate analysis of genetic markers. Bioinformatics, 24: 1403-1405. Jumentier B, Caye K, Heude B, Lepeule J, Francois O (2020) Sparse latent factor regression models for genome-wide and epigenome-wide association studies. bioRxiv, 2020.02.07.938381. Legendre P, Oksanen J, ter Braak CJ (2010) Testing the significance of canonical axes in redundancy analysis. Methods in Ecology and Evolution, 2: 269-277. Money D, Migicovsky Z, Gardner K, Myles S (2017) LinkImputeR: user-guided genotype calling and imputation for non-model organisms. BMC Genomics, 18: 1-12. Oksanen J, Blanchet FG, Kindt R, et al. (2016) vegan: Community Ecology Package. R package version 2.3-5. Rellstab C, Gugerli F, Eckert AJ, Hancock AM, Holderegger R (2015) A practical guide to environmental association analysis in landscape genomics. Molecular Ecology, 24: 4348-4370. Schweizer RM, vonHoldt BM, Harrigan R, et al. (2016) Genetic subdivision and candidate genes under selection in North American grey wolves. Molecular Ecology, 25: 380-402. Storey JD, Tibshirani R (2003) Statistical significance for genome-wide experiments. Proceedings of the National Academy of Sciences, 100: 9440-9445. Zuur AF, Ieno EN, Elphick CS (2010) A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution, 1: 3-14. "],["Week12.html", "Week 12: Model Selection ", " Week 12: Model Selection "],["WE-12.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Goals and Background Goals: The goal of this worked example is for you to become familiar with creating and analyzing landscape genetic hypotheses using maximum likelihood population effect models (MLPE; Van Strien et al. 2012) and information theory. With this approach, we focus on evaluating evidence for the influence of the (intervening) landscape on link-based metrics (i.e. genetic distance). Incorporating node-based effects (i.e. pond size, etc.) will be covered in the gravity modeling lab. We will re-analyze a dataset from Goldberg and Waits (2010) using a maximum likelihood population effect model (MLPE). Heres a map of the study area. The lines represent those pair-wise links between sampling locations that we will be considering here (graph model: Delaunay triangulation). Map of study area b. Data set In previous labs, you have created genetic distance matrices, so here we are going to start with those data already complete, as are the landscape data. For MLPE, two vectors representing the nodes at the ends of each link must be created. These are already included in the data set (pop1 and pop2). Each row in the file CSF_network.csv represents a link between two pairs of sampling locations. The columns contain the following variables: Id: Link ID. Name: Link name (combines the two site names). logDc.km: Genetic distance per geographic distance, log transformed (response variable). LENGTH: Euclidean distance. slope: Average slope along the link. solarinso: Average solar insolation along the link. soils: Dominant soil type along the link (categorical). ag: Proportion of agriculture along the link. grass: Proportion of grassland along the link. shrub: Proportion of shrubland along the link. hi: Proportion of high density forest along the link. lo: Proportion of low density forest along the link. dev: Proportion of development (buildings) along the link. forest: Proportion of forest (the sum of hi and lo) along the link. pop1: From population. pop2: To population. c. Required packages library(LandGenCourse) library(ggplot2) #require(lme4) #require(usdm) d. Import data First, read in the dataset. It is named CSF (Columbia spotted frog) to differentiate it from the RALU dataset you have used in previous labs (same species, very different landscapes). CSFdata &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;CSF_network.csv&quot;, package = &quot;LandGenCourse&quot;)) head(CSFdata) ## Id Name logDc.km LENGTH slope solarinso soils ag ## 1 1 CeGr to FeCr -5.664895 5449.643 11.328851 16332.75 thirteen 0.00000000 ## 2 2 CeGr to FlCr -5.786712 8352.643 12.822890 16142.01 one 0.01351351 ## 3 43 CeGr to Krum -5.466544 7232.601 15.852762 16208.67 one 0.00000000 ## 4 3 FlCr to Krum -5.473660 6196.521 14.156965 15871.14 twelve 0.00000000 ## 5 44 Krum to P37 -5.088242 4470.826 15.381000 15883.31 thirteen 0.00000000 ## 6 46 CeGr to LaTr -5.997768 8766.482 9.709769 16298.75 one5 0.00000000 ## grass shrub hi lo dev forest pop1 pop2 ## 1 0.00000000 0.55319149 0.2553191 0.1914894 0.00000000 0.4468085 1 2 ## 2 0.01351351 0.10810811 0.6216216 0.2432432 0.00000000 0.8648649 1 3 ## 3 0.00000000 0.19696970 0.6212121 0.1818182 0.00000000 0.8030303 4 1 ## 4 0.00000000 0.05084746 0.5932203 0.3559322 0.00000000 0.9491525 3 4 ## 5 0.05000000 0.00000000 0.3250000 0.6250000 0.00000000 0.9500000 12 4 ## 6 0.20000000 0.21333333 0.2666667 0.3066667 0.01333333 0.5733333 5 1 Take a look at the column names and make sure you and R agree on what everything is called, and on the data types (e.g., factor vs. character). str(CSFdata) ## &#39;data.frame&#39;: 48 obs. of 16 variables: ## $ Id : int 1 2 43 3 44 46 4 6 7 31 ... ## $ Name : chr &quot;CeGr to FeCr&quot; &quot;CeGr to FlCr&quot; &quot;CeGr to Krum&quot; &quot;FlCr to Krum&quot; ... ## $ logDc.km : num -5.66 -5.79 -5.47 -5.47 -5.09 ... ## $ LENGTH : num 5450 8353 7233 6197 4471 ... ## $ slope : num 11.3 12.8 15.9 14.2 15.4 ... ## $ solarinso: num 16333 16142 16209 15871 15883 ... ## $ soils : chr &quot;thirteen&quot; &quot;one&quot; &quot;one&quot; &quot;twelve&quot; ... ## $ ag : num 0 0.0135 0 0 0 ... ## $ grass : num 0 0.0135 0 0 0.05 ... ## $ shrub : num 0.5532 0.1081 0.197 0.0508 0 ... ## $ hi : num 0.255 0.622 0.621 0.593 0.325 ... ## $ lo : num 0.191 0.243 0.182 0.356 0.625 ... ## $ dev : num 0 0 0 0 0 ... ## $ forest : num 0.447 0.865 0.803 0.949 0.95 ... ## $ pop1 : int 1 1 4 3 12 5 2 1 2 4 ... ## $ pop2 : int 2 3 1 4 4 1 5 6 6 7 ... Name and soils are interpreted as factors, Id, pop1 and pop2 are vectors of integers, and everything else is a numeric vector, i.e., continuous variable. The response Y in our MLPE models will be logDc.km, the genetic distance per geographic distance, log transformed to meet normality assumptions. This was used in the paper as a way to include IBD as the base assumption without increasing k. The landscape variables were then also estimated so that they would not necessarily increase with increasing distance between sites (e.g., proportion of forest between sites). This is a pruned network, where the links are only those neighbors in a Delauney trangulation. &lt;Capture.PNG&gt; For this exercise, you will test 5 alternative hypotheses. At the end, you will be invited to create your own hypotheses from these variables and test support for them. 2. Fitting candidate models a. Alternative hypotheses In information theory, we evaluate evidence for a candidate set of hypotheses that we develop from the literature and theory. Although it is mathematically possible (and often practiced) to fit all possible combinations of input variables, that approach is not consistent with the methodology (see Burnham and Anderson 2002 Chapter 8, the reading for this unit, for more detail). Here, your a priori\" set of hypotheses (as in the Week 12 Conceptual Exercise) is as follows: Full model: solarinso, forest, ag, shrub, dev Landcover model: ag, shrub, forest, and dev Human footprint model: ag, dev Energy conservation model: slope, shrub, dev Historical model: soils, slope, solarinso b. Prepare for model fitting Because link data are not independent (i.e. the genetic diversity at node 1 influences both the node1/node2 genetic distance and that from node1/node3, etc.), we need to develop a set of random effects to account for this dependency, so that we seperate the pattern due to this covariance from the signal of landscape influence on genetic distance. We start by creating matrices from the pop1 and pop2 variables: Create the Zl and ZZ matrices: Zl &lt;- lapply(c(&quot;pop1&quot;,&quot;pop2&quot;), function(nm) Matrix:::fac2sparse(CSFdata[[nm]], &quot;d&quot;, drop=FALSE)) ZZ &lt;- Reduce(&quot;+&quot;, Zl[-1], Zl[[1]]) Note: The list Zl contains two sparse matrices, one for the from node in pop1 and one for the to node in pop2. Each sparse matrix has 20 rows (for the 20 populations) and 48 columns (for the 48 links). Each column has a single value of 1 that indicates the from or to population of the corresponding link. The matrix ZZ is again a sparse matrix that is the sum of the two sparse matrices in Zl. Each column thus has two values of 1, one for the from and one for the to node. Now we can fit an lmer model (linear mixed model; see Week 6 videos) to the data. mod1 &lt;- lme4::lFormula(logDc.km ~ solarinso + forest + ag + shrub + dev + (1|pop1), data = CSFdata, REML = TRUE) ## Warning: Some predictor variables are on very different scales: consider ## rescaling At this point an error message appears: Warning message: Some predictor variables are on very different scales: consider rescaling. Which variable is causing this problem? To check, get R to show you the first few rows of the dataset: head(CSFdata) ## Id Name logDc.km LENGTH slope solarinso soils ag ## 1 1 CeGr to FeCr -5.664895 5449.643 11.328851 16332.75 thirteen 0.00000000 ## 2 2 CeGr to FlCr -5.786712 8352.643 12.822890 16142.01 one 0.01351351 ## 3 43 CeGr to Krum -5.466544 7232.601 15.852762 16208.67 one 0.00000000 ## 4 3 FlCr to Krum -5.473660 6196.521 14.156965 15871.14 twelve 0.00000000 ## 5 44 Krum to P37 -5.088242 4470.826 15.381000 15883.31 thirteen 0.00000000 ## 6 46 CeGr to LaTr -5.997768 8766.482 9.709769 16298.75 one5 0.00000000 ## grass shrub hi lo dev forest pop1 pop2 ## 1 0.00000000 0.55319149 0.2553191 0.1914894 0.00000000 0.4468085 1 2 ## 2 0.01351351 0.10810811 0.6216216 0.2432432 0.00000000 0.8648649 1 3 ## 3 0.00000000 0.19696970 0.6212121 0.1818182 0.00000000 0.8030303 4 1 ## 4 0.00000000 0.05084746 0.5932203 0.3559322 0.00000000 0.9491525 3 4 ## 5 0.05000000 0.00000000 0.3250000 0.6250000 0.00000000 0.9500000 12 4 ## 6 0.20000000 0.21333333 0.2666667 0.3066667 0.01333333 0.5733333 5 1 One of these variables is a lot larger than the others, but has a small range. This often happens with variables such as easting and can cause issues for model fit. The common solution is to z-transform (standardize) the data, which is conveniently done in R using the scale function. Note: we could use the scale function without specifying the arguments center or scale, as both are TRUE by default. The argument center=TRUE centers the variable by subtracting the mean, the scale=TRUE argument rescales the variables to unit variance by dividing each value by the standard deviation. The resulting variable has a mean of zero and a standard deviation and variance of one. solarinsoz &lt;- scale(CSFdata$solarinso, center=TRUE, scale=TRUE) We then can attach these data back to our dataframe and check to make sure this worked: CSFdata &lt;- cbind(CSFdata, solarinsoz) names(CSFdata) ## [1] &quot;Id&quot; &quot;Name&quot; &quot;logDc.km&quot; &quot;LENGTH&quot; &quot;slope&quot; ## [6] &quot;solarinso&quot; &quot;soils&quot; &quot;ag&quot; &quot;grass&quot; &quot;shrub&quot; ## [11] &quot;hi&quot; &quot;lo&quot; &quot;dev&quot; &quot;forest&quot; &quot;pop1&quot; ## [16] &quot;pop2&quot; &quot;solarinsoz&quot; If solarinsoz is there, you are good. While we are at it, we will make sure that these variables do not have issues with multicollinearity. Make a dataframe of just the variables to test (note, you cannot use factors here): CSF.df &lt;- with(CSFdata, data.frame(solarinsoz, forest, dev, shrub, ag)) usdm::vif(CSF.df) ## Variables VIF ## 1 solarinsoz 1.479273 ## 2 forest 3.127104 ## 3 dev 1.800218 ## 4 shrub 1.523341 ## 5 ag 2.532198 If you get an error that there is no package called usdm, use install.packages(usdm) Numbers less than 10, or 3, or 4, depending on who you ask, are considered to not be collinear enough to affect model outcomes. So we are good to go here. What would happen if we added grass to this? usdm::vif(cbind(CSF.df, grass=CSFdata$grass)) ## Variables VIF ## 1 solarinsoz 1.541954 ## 2 forest 97.167730 ## 3 dev 55.768172 ## 4 shrub 11.657666 ## 5 ag 68.852254 ## 6 grass 45.970721 Question 1: Why would adding in the last land cover cause a high amount of collinearity? Consider how these data were calculated. The variables forest, dev, shrub, ag and grass together cover almost all land cover types in the study area. Each is measured as percent area, and together, they make up 100% or close to 100% of the land cover types along each link. Thus if we know e.g. that all forest, dev, shrub and ag together cover 80% along a link, then it is a good bet that grass will cover the remaining 20%. With such sets of compositional data, we need to omit one variable to avoid multi-collinearity (where a variable is a linear combination of other variables), even though the pairwise correlations may be low. For example, there the variable grass showed relatively low correlations with the other variables: cor(CSF.df, CSFdata$grass) ## [,1] ## solarinsoz 0.10394523 ## forest -0.41341968 ## dev -0.23617929 ## shrub -0.29807027 ## ag 0.01060842 c. Fit all five models Now we have to remake the modeling objects with our new and improved data frame, but referring to our z-transformed data (thanks to Martin Van Strien and Helene Wagner for the base model code): Zl &lt;- lapply(c(&quot;pop1&quot;,&quot;pop2&quot;), function(nm) Matrix:::fac2sparse(CSFdata[[nm]], &quot;d&quot;, drop=FALSE)) ZZ &lt;- Reduce(&quot;+&quot;, Zl[-1], Zl[[1]]) Fit an lmer model to the data: mod_1 &lt;- lme4::lFormula(logDc.km ~ solarinsoz + forest + ag + shrub + dev + (1|pop1), data = CSFdata, REML = TRUE) In the fitted model replace Zt slot (matrix for the random effects generated with just pop1) with the ZZ matrix created above: mod_1$reTrms$Zt &lt;- ZZ Refit the model (this involves a few steps of optimization starting from the previously fitted model mod_1): dfun &lt;- do.call(lme4::mkLmerDevfun, mod_1) opt &lt;- lme4::optimizeLmer(dfun) mod1 &lt;- lme4::mkMerMod(environment(dfun), opt, mod_1$reTrms, fr = mod_1$fr) summary(mod1) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## ## REML criterion at convergence: 101.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.86045 -0.67339 -0.06932 0.57516 1.87959 ## ## Random effects: ## Groups Name Variance Std.Dev. ## pop1 (Intercept) 0.02986 0.1728 ## Residual 0.49166 0.7012 ## Number of obs: 48, groups: pop1, 20 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -4.681159 0.383071 -12.220 ## solarinsoz -0.001611 0.135868 -0.012 ## forest -0.163231 0.591137 -0.276 ## ag -1.504031 0.642904 -2.339 ## shrub -1.903160 1.343940 -1.416 ## dev 0.176494 0.652835 0.270 ## ## Correlation of Fixed Effects: ## (Intr) slrnsz forest ag shrub ## solarinsoz -0.088 ## forest -0.802 0.360 ## ag -0.848 -0.020 0.626 ## shrub -0.237 -0.384 -0.121 0.259 ## dev -0.691 -0.020 0.533 0.551 0.195 In this output, we focus on the estimates of fixed effects, which we will use later to examine the effect of each variable. Note that there are no p-values because of the nature of these models and the philosophy of the package writers. There are packages available that calculate these but the results may be misleading. This is the closest to a full model in our dataset (although our models are not completely nested), so we will take a look at the residuals: plot(mod1) Residuals are centered around zero and do not show large groupings or patterns, although there is some increase in variation at larger numbers (so the model is having a more difficult time predicting larger genetic distances per km). We will keep that in mind as we move on. Next we will check to make sure the residuals are normally distributed. hist(residuals(mod1)) This can also be done with the qqmath function in package lattice if you are more used to seeing qq plots. These residuals look okay so we will move on (but if this were a publication we would do a closer examination of the shape of the data at the ends of the distribution). Now we will make a function (called MLPE) to fit the rest of our models, so that we are not typing the same code over and over (which causes errors and long code that is difficult to navigate or update): MLPE &lt;- function(variables, data) { mod2 &lt;- lme4::lFormula(variables, data = data, REML = TRUE) dfun &lt;- do.call(lme4::mkLmerDevfun, mod2) opt &lt;- lme4::optimizeLmer(dfun) mod_2 &lt;- lme4::mkMerMod(environment(dfun), opt, mod2$reTrms,fr = mod2$fr) mod2$reTrms$Zt &lt;- ZZ # Refit the model dfun &lt;- do.call(lme4::mkLmerDevfun, mod2) opt &lt;- lme4::optimizeLmer(dfun) modelout &lt;- lme4::mkMerMod(environment(dfun), opt, mod2$reTrms,fr = mod2$fr) return(modelout) } To use this function, we define variables (the model as we want it to run) and data (the object that contains our data). mod2 &lt;- MLPE(logDc.km ~ forest + ag + shrub + dev + (1|pop1), CSFdata) mod3 &lt;- MLPE(logDc.km ~ ag + dev + (1|pop1), CSFdata) mod4 &lt;- MLPE(logDc.km ~ slope + shrub + dev + (1|pop1), CSFdata) mod5 &lt;- MLPE(logDc.km ~ soils + slope + solarinsoz + (1|pop1), CSFdata) d. Compare evidence for models First we need to refit the models with REML = FALSE to get a useable estimate of likelihood for AIC and BIC. To do this, well create a new function called MLPEnoREML and use it to fit our 5 models. MLPEnoREML &lt;- function(variables, data) { mod2 &lt;- lme4::lFormula(variables, data = data, REML = FALSE) dfun &lt;- do.call(lme4::mkLmerDevfun, mod2) opt &lt;- lme4::optimizeLmer(dfun) mod_2 &lt;- lme4::mkMerMod(environment(dfun), opt, mod2$reTrms,fr = mod2$fr) mod2$reTrms$Zt &lt;- ZZ # Refit the model dfun &lt;- do.call(lme4::mkLmerDevfun, mod2) opt &lt;- lme4::optimizeLmer(dfun) modelout &lt;- lme4::mkMerMod(environment(dfun), opt, mod2$reTrms,fr = mod2$fr) return(modelout) } mod1noREML &lt;- MLPEnoREML (logDc.km ~ solarinsoz + forest + ag + shrub + dev + (1|pop1), CSFdata) mod2noREML &lt;- MLPEnoREML(logDc.km ~ forest + ag + shrub + dev + (1|pop1), CSFdata) mod3noREML &lt;- MLPEnoREML(logDc.km ~ ag + dev + (1|pop1), CSFdata) mod4noREML &lt;- MLPEnoREML(logDc.km ~ slope + shrub + dev + (1|pop1), CSFdata) mod5noREML &lt;- MLPEnoREML(logDc.km ~ soils + slope + solarinsoz + (1|pop1), CSFdata) In information theory, we use information criteria (AIC, BIC) to evaluate the relative distance of our hypothesis from truth. For more information, see Burnham and Anderson (2002). Note: package MuMIn will do this for you, but for this exercise it is useful to see all the pieces of what goes into your evaluation. Models &lt;- list(Full=mod1noREML, Landcover=mod2noREML, HumanFootprint=mod3noREML, EnergyConservation=mod4noREML, Historical=mod5noREML) CSF.IC &lt;- data.frame(AIC = sapply(Models, AIC), BIC = sapply(Models, BIC)) CSF.IC ## AIC BIC ## Full 115.9620 130.9316 ## Landcover 113.9757 127.0742 ## HumanFootprint 113.4053 122.7613 ## EnergyConservation 114.4910 125.7182 ## Historical 122.4089 137.3785 We now have some results, great! Now we will work with these a bit. First, because we do not have an infinite number of samples, we will convert AIC to AICc (which adds a small-sample correction to AIC). First, find the k parameters used in the model and add them to the table. CSF.IC &lt;- data.frame(CSF.IC, k = sapply(Models, function(ls) attr(logLik(ls), &quot;df&quot;))) CSF.IC ## AIC BIC k ## Full 115.9620 130.9316 8 ## Landcover 113.9757 127.0742 7 ## HumanFootprint 113.4053 122.7613 5 ## EnergyConservation 114.4910 125.7182 6 ## Historical 122.4089 137.3785 8 Question 2: How does k relate to the number of parameters in each model? Now, calculate AICc and add it to the dataframe: CSF.IC$AICc &lt;- CSF.IC$AIC + 2*CSF.IC$k*(CSF.IC$k+1)/(48-CSF.IC$k-1) CSF.IC ## AIC BIC k AICc ## Full 115.9620 130.9316 8 119.6543 ## Landcover 113.9757 127.0742 7 116.7757 ## HumanFootprint 113.4053 122.7613 5 114.8339 ## EnergyConservation 114.4910 125.7182 6 116.5398 ## Historical 122.4089 137.3785 8 126.1012 e. Calculate evidence weights Next we calculate evidence weights for each model based on AICc and BIC. These can be interpreted as the probability that the model is the closest to truth in the candidate model set. Calculate model weights for AICc: AICcmin &lt;- min(CSF.IC$AICc) RL &lt;- exp(-0.5*(CSF.IC$AICc - AICcmin)) sumRL &lt;- sum(RL) CSF.IC$AICcmin &lt;- RL/sumRL Calculate model weights for BIC: BICmin &lt;- min(CSF.IC$BIC) RL.B &lt;- exp(-0.5*(CSF.IC$BIC - BICmin)) sumRL.B &lt;- sum(RL.B) CSF.IC$BICew &lt;- RL.B/sumRL.B round(CSF.IC,3) ## AIC BIC k AICc AICcmin BICew ## Full 115.962 130.932 8 119.654 0.047 0.012 ## Landcover 113.976 127.074 7 116.776 0.200 0.085 ## HumanFootprint 113.405 122.761 5 114.834 0.527 0.735 ## EnergyConservation 114.491 125.718 6 116.540 0.224 0.167 ## Historical 122.409 137.378 8 126.101 0.002 0.000 Question 3: What did using AICc (rather than AIC) do to inference from these results? In the multi-model inference approach, we can look across all the models in the dataset and their evidence weights to better understand the system. Here we see that models 1 and 5 have very little evidence of being close to the truth, that models 2 and 4 have more, and that model 3 has the most evidence of being the closest to truth, although how much more evidence there is for model 3 over 2 and 4 depends on which criterion you are looking at. BIC imposes a larger penalty for additional parameters, so it has a larger distinction between these. Lets review what these models are: Full model: solarinso, forest, ag, shrub, dev Landcover model: ag, shrub, forest, and dev Human footprint model: ag, dev Energy conservation model: slope, shrub, dev Historical model: soils, slope, solarinso f. Confidence intervals for predictors According to Row et al. (2017), we can use the confidence interval from the variables in the models to identify those that are contributing to landscape resistance. Looking at models 2 and 3, we have some evidence that shrub and forest cover matter, but its not as strong as the evidence for ag and development. Lets look at the parameter estimates to understand more. Note that for parameter estimation we use the REML estimates. ModelsREML &lt;- list(Full=mod1, Landcover=mod2, HumanFootprint=mod3, EnergyConservation=mod4, Historical=mod5) Now well estimate the 95% confidence interval for these estimates: confint(ModelsREML$Landcover, level = 0.95, method = &quot;Wald&quot;) ## 2.5 % 97.5 % ## .sig01 NA NA ## .sigma NA NA ## (Intercept) -5.412493 -3.9372219 ## forest -1.234535 0.8980823 ## ag -2.767849 -0.2789161 ## shrub -4.335188 0.4391626 ## dev -1.077595 1.4292392 confint(ModelsREML$HumanFootprint, level = 0.95, method = &quot;Wald&quot;) ## 2.5 % 97.5 % ## .sig01 NA NA ## .sigma NA NA ## (Intercept) -5.3193590 -4.5159303 ## ag -2.0215023 -0.1997056 ## dev -0.6843463 1.5082812 confint(ModelsREML$EnergyConservation, level = 0.95, method = &quot;Wald&quot;) ## 2.5 % 97.5 % ## .sig01 NA NA ## .sigma NA NA ## (Intercept) -6.83962996 -5.3057702 ## slope 0.02028322 0.1725869 ## shrub -3.83624990 1.0650554 ## dev 0.22109749 2.6485991 In essence, we use the confidence intervals to assess whether we can differentiate the effect of each variable from zero (i.e. statistical significance). The confidence intervals that overlap 0 do not have as much evidence of being important to gene flow, based on simulations in Row et al. (2017). So in this case, there is strong evidence that agriculture influences the rate of gene flow. Note that because the metric is genetic distance, negative values indicate more gene flow. Question 4: What evidence is there that shrub and forest matter to gene flow from these analyses? What about slope? Would you include them in a conclusion about landscape resistance in this system? Question 5: What evidence is there that development influences gene flow from these analyses? Would you include it in a conclusion about landscape resistance in this system? Question 6: We excluded grass from our models because of multicollinearity. How could we investigate the importance of this variable? 3. Your turn to test additional hypotheses! Create your own (small) set of hypotheses to rank for evidence using the dataset provided. Modify the code above to complete the following steps: Define your hypotheses. Calculate the VIF table for full model. Make a residual plot for full model. Create table of AICc and BIC weights. Create confidence intervals from models with high evidence weights. What can you infer from your analysis about what influences gene flow of this species? 4. References Burnham KP and DR Anderson. 2002. Model Selection and Multimodel Inference: A Practical Information Theoretic Approach. Chapter 8, p. 437-454. Springer-Verlag, New York. Goldberg CS and LP Waits (2010). Comparative landscape genetics of two pond-breeding amphibian species in a highly modified agricultural landscape. Molecular Ecology 19: 3650-3663. Row JR, ST Knick, SJ Oyler-McCance, SC Lougheed and BC Fedy (2017). Developing approaches for linear mixed modeling in landscape genetics through landscape-directed dispersal simulations. Ecology &amp; Evolution 7: 37513761. Van Strien MJ, D Keller and R Holderegger (2012). A new analytical approach to landscape genetic modelling: leastcost transect analysis and linear mixed models. Molecular Ecology 21: 4010-4023. "],["Week13.html", "Week 13: Gravity Models ", " Week 13: Gravity Models "],["WE-13.html", "Worked Example", " Worked Example 1. Overview of Worked Example a. Background There are many ways graphs can be implemented to understand population structure and relate that structure to landscape characteristics (see Dyer and Nason 2004). In this exercise, we will calculate various graph metrics and apply graphs to fit a gravity model. Gravity models are a type of inferential model that exploit graph characteristics. Gravity models include both at-site and between-site landscape data. They are a type of graph consisting of nodes and edges. These nodes and edges of landscape characteristics associated with these graph elements. b. Data set In this exercise, you will use the gravity model framework to build an empirical model of gene flow for the Columbia spotted frog dataset in central Idaho that you have used for several other exercises (Murphy et al. 2010). c. Required R libraries Install some packages needed for this worked example. if(!requireNamespace(&quot;GeNetIt&quot;, quietly = TRUE)) remotes::install_github(&quot;jeffreyevans/GeNetIt&quot;) if(!requireNamespace(&quot;spatialEco&quot;, quietly = TRUE)) remotes::install_github(&quot;jeffreyevans/spatialEco&quot;) library(LandGenCourse) library(sp) #library(landscapemetrics) #library(raster) #library(rgdal) #library(GeNetIt) #library(spatialEco) #library(GeNetIt) #library(igraph) #library(deldir) Set working and data directories - NEEDS TO BE REMOVED #wd = C:/R/spatialR/day3/session6 # set your working directory here #setwd(wd) #ddir = file.path(C:/R/spatialR/day3, data, S6) 2. Calculating graph metrics a. Read in, make a spatial object and plot wetlands (Wetlands.csv). Read in the wetland csv file. wetlands &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;Wetlands.csv&quot;, package = &quot;LandGenCourse&quot;), header = TRUE) str(wetlands) ## &#39;data.frame&#39;: 121 obs. of 5 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ X : int 688835 687460 687507 687637 688850 688500 687944 687872 687150 690888 ... ## $ Y : int 5002939 4994400 4994314 4994117 4997750 4998900 5000006 5000041 4995850 5004126 ... ## $ RALU : chr &quot;y&quot; &quot;n&quot; &quot;n&quot; &quot;n&quot; ... ## $ SiteName: chr &quot;AirplaneLake&quot; &quot;AlpineInletCreek&quot; &quot;AlpineInletMeadow&quot; &quot;AlpineLake&quot; ... Coerce to a SpatialPointsDataFrame and look at the structure of the related dataframe. sp::coordinates(wetlands) &lt;- ~X+Y class(wetlands) ## [1] &quot;SpatialPointsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; str(wetlands) ## Formal class &#39;SpatialPointsDataFrame&#39; [package &quot;sp&quot;] with 5 slots ## ..@ data :&#39;data.frame&#39;: 121 obs. of 3 variables: ## .. ..$ ID : int [1:121] 1 2 3 4 5 6 7 8 9 10 ... ## .. ..$ RALU : chr [1:121] &quot;y&quot; &quot;n&quot; &quot;n&quot; &quot;n&quot; ... ## .. ..$ SiteName: chr [1:121] &quot;AirplaneLake&quot; &quot;AlpineInletCreek&quot; &quot;AlpineInletMeadow&quot; &quot;AlpineLake&quot; ... ## ..@ coords.nrs : int [1:2] 2 3 ## ..@ coords : num [1:121, 1:2] 688835 687460 687507 687637 688850 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:121] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:2] &quot;X&quot; &quot;Y&quot; ## ..@ bbox : num [1:2, 1:2] 686244 4993077 695699 5004317 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2] &quot;X&quot; &quot;Y&quot; ## .. .. ..$ : chr [1:2] &quot;min&quot; &quot;max&quot; ## ..@ proj4string:Formal class &#39;CRS&#39; [package &quot;sp&quot;] with 1 slot ## .. .. ..@ projargs: chr NA Plot the wetlands plot(wetlands, asp=1, bty=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, main = &quot;All Wetlands&quot;) points(wetlands, pch=19, cex=0.75, col=&quot;blue&quot;) b. Create a graph from the wetlands Well create a graph that could represent connectivity using Delaunay triangulation, then plot the result. Delaunay triangulation, create delaunay graph options(warn=-1) wetlandgraph &lt;- deldir::deldir(coordinates(wetlands)[,1], coordinates(wetlands)[,2], z = wetlands$SiteName) options(warn=0) Plot the graph with the wetlands plot(wetlands, asp=1, bty=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, main = &quot;All Wetlands&quot;) points(wetlands, pch=19, cex=0.75, col=&quot;blue&quot;) plot(wetlandgraph, wlines = &quot;triang&quot;, wpoints=&quot;none&quot;, number=FALSE, add=TRUE, lty=1) Questions: What other types of graphs could you build? Can you build a different graph and use this graph in your analysis? c. Create adjacency matrix What wetlands are connected to each other based on the Delaunay triangulation? Create an adjacency matrix from the graph object and make an adjacency matrix. ind &lt;- wetlandgraph$delsgs[,5:6] #pull out individual nodes adj &lt;- matrix(0, length(wetlands$X), length(wetlands$Y)) for (i in 1:nrow(ind)){ adj[ind[i,1], ind[i,2]] &lt;- 1 adj[ind[i,2], ind[i,1]] &lt;- 1 } d. Calculate graph metrics This graph, if it is ecologically meaningful contains information in the graph structure itself. Calculate graph metrics of degree and betweenness. Questions: In what way(s) is the resulting graph potentially ecologically meaningful? How might it not be ecologically or biologically meaningful? Make an igraph network from the matrix we just made wetnet &lt;- igraph::graph_from_adjacency_matrix(adj, weighted = NULL, mode=&quot;undirected&quot;) plot(wetnet) e. Add graph metrics to data.frame Calculate degree- the number of connections a node has wetlands@data$degree &lt;- igraph::degree(wetnet) head(wetlands@data) ## ID RALU SiteName degree ## 1 1 y AirplaneLake 8 ## 2 2 n AlpineInletCreek 6 ## 3 3 n AlpineInletMeadow 4 ## 4 4 n AlpineLake 6 ## 5 5 n AxeHandleMeadow 9 ## 6 6 y BachelorMeadow 7 Calculate betweenness- the number of shortest paths going through a node wetlands@data$betweenness &lt;- igraph::betweenness(wetnet) head(wetlands@data) ## ID RALU SiteName degree betweenness ## 1 1 y AirplaneLake 8 279.58405 ## 2 2 n AlpineInletCreek 6 87.97683 ## 3 3 n AlpineInletMeadow 4 16.71910 ## 4 4 n AlpineLake 6 117.72297 ## 5 5 n AxeHandleMeadow 9 682.84635 ## 6 6 y BachelorMeadow 7 292.32533 Challenge: Can you code your own degree or betweenness centrality function? 3. Combine frog data with graph metrics a. Add graph node data to frog site data Using the RALU_Site data, read in the data, add the node data (betweenness and degree) and create a shape file that includes the node data. This process will mirror part 1. Questions: Look at the RALU_Site file. What are the fields here? What data are included? Import site data sites &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;RALU_Site.csv&quot;, package = &quot;LandGenCourse&quot;), header = TRUE) head(sites) ## SiteID Elev Length Area Perim Depth pH Dforest Drock Dshrub ## 1 1 2473.719 101 6856.1 335.7 5.70 7.2 0.000 0.900 0.100 ## 2 2 2609.068 350 36915.0 916.7 12.70 6.7 0.131 0.036 0.000 ## 3 3 2622.113 0 1000.0 120.0 0.25 6.1 0.200 0.000 0.000 ## 4 4 2474.737 336 38912.8 921.7 16.97 7.0 0.374 0.396 0.043 ## 5 5 2589.805 107 6089.4 300.5 3.89 7.1 0.647 0.176 0.000 ## 6 6 2463.088 68 3444.6 220.0 2.00 7.0 0.359 0.017 0.000 ## SiteName X Y ## 1 Walkabout 688020 4993875 ## 2 ParagonLake 687427 4994716 ## 3 ParagonWetland 687170 4994770 ## 4 RamshornLake 687914 4994972 ## 5 Mt.WilsonLake 687645 4995325 ## 6 MooseLake 688549 4995538 Extract degree and betweenness from graph data nodestats &lt;- as.data.frame(wetlands@data[,3:5]) degree.betweenness &lt;- nodestats[which(nodestats$SiteName %in% sites$SiteName),] head(degree.betweenness) ## SiteName degree betweenness ## 1 AirplaneLake 8 279.58405 ## 6 BachelorMeadow 7 292.32533 ## 7 BarkingFoxLake 9 512.37876 ## 12 BobLake 6 466.35631 ## 16 CacheLake 4 11.57913 ## 25 DoeLake 5 31.48064 Add to site data sites &lt;- merge(degree.betweenness, sites, by= &quot;SiteName&quot; ) head(sites) ## SiteName degree betweenness SiteID Elev Length Area Perim Depth ## 1 AirplaneLake 8 279.58405 27 2564.381 390 62582.2 1142.8 21.64 ## 2 BachelorMeadow 7 292.32533 15 2591.781 0 225.0 60.0 0.40 ## 3 BarkingFoxLake 9 512.37876 19 2545.275 160 12000.0 435.0 5.00 ## 4 BobLake 6 466.35631 16 2649.125 143 4600.0 321.4 2.00 ## 5 CacheLake 4 11.57913 10 2475.829 75 2268.8 192.0 1.86 ## 6 DoeLake 5 31.48064 8 2463.006 170 13034.9 463.2 6.03 ## pH Dforest Drock Dshrub X Y ## 1 6.5 0.398 0.051 0.00 688835 5002939 ## 2 6.1 0.000 0.000 0.20 688500 4998900 ## 3 6.5 0.400 0.250 0.05 687944 5000006 ## 4 7.0 0.550 0.000 0.05 690127 4999150 ## 5 6.5 0.508 0.000 0.00 688777 4997264 ## 6 7.6 0.254 0.000 0.00 688968 4996458 Note: Using names is dangerous as, small changes in names can result in non-matches. In this case, the ID fields are not consistent (data were collected at different times for different purposes originally). However, names are standardized in a drop-down list of a database. So they are a matching field. My preference is do to this type of operation on a numeric field. coordinates(sites) &lt;- ~X+Y str(sites) ## Formal class &#39;SpatialPointsDataFrame&#39; [package &quot;sp&quot;] with 5 slots ## ..@ data :&#39;data.frame&#39;: 29 obs. of 13 variables: ## .. ..$ SiteName : chr [1:29] &quot;AirplaneLake&quot; &quot;BachelorMeadow&quot; &quot;BarkingFoxLake&quot; &quot;BobLake&quot; ... ## .. ..$ degree : num [1:29] 8 7 9 6 4 5 6 6 5 3 ... ## .. ..$ betweenness: num [1:29] 279.6 292.3 512.4 466.4 11.6 ... ## .. ..$ SiteID : int [1:29] 27 15 19 16 10 8 11 31 9 12 ... ## .. ..$ Elev : num [1:29] 2564 2592 2545 2649 2476 ... ## .. ..$ Length : int [1:29] 390 0 160 143 75 170 100 0 91 65 ... ## .. ..$ Area : num [1:29] 62582 225 12000 4600 2269 ... ## .. ..$ Perim : num [1:29] 1143 60 435 321 192 ... ## .. ..$ Depth : num [1:29] 21.64 0.4 5 2 1.86 ... ## .. ..$ pH : num [1:29] 6.5 6.1 6.5 7 6.5 7.6 6.8 0 6.7 6.9 ... ## .. ..$ Dforest : num [1:29] 0.398 0 0.4 0.55 0.508 ... ## .. ..$ Drock : num [1:29] 0.051 0 0.25 0 0 0 0.163 0 0 0 ... ## .. ..$ Dshrub : num [1:29] 0 0.2 0.05 0.05 0 0 0 0 0 0 ... ## ..@ coords.nrs : int [1:2] 14 15 ## ..@ coords : num [1:29, 1:2] 688835 688500 687944 690127 688777 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:29] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:2] &quot;X&quot; &quot;Y&quot; ## ..@ bbox : num [1:2, 1:2] 686850 4993875 690900 5004222 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2] &quot;X&quot; &quot;Y&quot; ## .. .. ..$ : chr [1:2] &quot;min&quot; &quot;max&quot; ## ..@ proj4string:Formal class &#39;CRS&#39; [package &quot;sp&quot;] with 1 slot ## .. .. ..@ projargs: chr NA b. Check data types Get data as proper type. Are all of the data fields reading as the correct type? summary(sites@data) ## SiteName degree betweenness SiteID ## Length:29 Min. :3.000 Min. : 0.00 Min. : 1.00 ## Class :character 1st Qu.:4.000 1st Qu.: 11.58 1st Qu.: 8.00 ## Mode :character Median :5.000 Median : 47.29 Median :15.00 ## Mean :5.448 Mean :149.86 Mean :15.76 ## 3rd Qu.:6.000 3rd Qu.:274.32 3rd Qu.:24.00 ## Max. :9.000 Max. :512.38 Max. :31.00 ## Elev Length Area Perim ## Min. :2390 Min. : 0.0 Min. : 0 Min. : 0.0 ## 1st Qu.:2476 1st Qu.: 91.0 1st Qu.: 3526 1st Qu.: 237.7 ## Median :2572 Median : 130.0 Median : 6556 Median : 335.7 ## Mean :2563 Mean : 210.9 Mean : 24504 Mean : 564.5 ## 3rd Qu.:2633 3rd Qu.: 235.0 3rd Qu.: 16601 3rd Qu.: 520.2 ## Max. :2794 Max. :1660.0 Max. :353898 Max. :4312.9 ## Depth pH Dforest Drock ## Min. : 0.000 Min. :0.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.: 2.000 1st Qu.:6.500 1st Qu.:0.0000 1st Qu.:0.0000 ## Median : 3.570 Median :6.900 Median :0.3590 Median :0.0470 ## Mean : 6.161 Mean :6.372 Mean :0.4785 Mean :0.1718 ## 3rd Qu.: 6.000 3rd Qu.:7.000 3rd Qu.:0.5500 3rd Qu.:0.2130 ## Max. :24.300 Max. :7.600 Max. :4.8450 Max. :0.9530 ## Dshrub ## Min. :0.00000 ## 1st Qu.:0.00000 ## Median :0.00000 ## Mean :0.01859 ## 3rd Qu.:0.00000 ## Max. :0.20000 sites@data$SiteName &lt;- as.character(sites@data$SiteName) class(sites@data$SiteName) ## [1] &quot;character&quot; sites@data$SiteID &lt;- as.factor(sites@data$SiteID) class(sites@data$SiteID) ## [1] &quot;factor&quot; 4. Merge graph with genetic distance data Create graph from site locations and merge with genetic distance data. a. Build graph from occupied sites To assess connectivity using a gravity model, we need to build a graph from the occupied frog sites. This could be any type of graph, but I generally use saturated or pruned by some maximum distance. Note: make sure to use correct field here: dist.graph &lt;- GeNetIt::knn.graph(sites, row.names = sites@data[,&quot;SiteID&quot;]) #dist.graph@proj4string@projargs &lt;- &quot;+proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,-0,-0,-0,0 +units=m +no_defs &quot; #dist.graph &lt;- GeNetIt::knn.graph(sites, row.names = sites@data[,&quot;SiteName&quot;], max.dist=5000) The k greater than one-third of the number of data points is a warning issued by the k nearest neighbor function. In our case, this is not a problem as we want a graph that k is equal to 1-n, that is a saturated graph. b. Merge graph with genetic distance This involves: reading in RALU_Dps genetic distance. Read in genetic distance file (RALU_Dps), convert to flow (1-distance) and unfold into a dataframe. Then merge graph with genetic distances. Read in the genetic distance matrix gdist &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;RALU_Dps.csv&quot;, package = &quot;LandGenCourse&quot;), header=TRUE) rownames(gdist) &lt;- t(names(gdist)) gdist &lt;- as.matrix (gdist) head(gdist) ## X1 X2 X3 X4 X5 X6 X7 X8 X9 ## X1 0.00000 0.13106 0.24000 0.10717 0.11773 0.11482 0.11978 0.22386 0.31478 ## X2 0.13106 0.00000 0.15344 0.10157 0.16524 0.14467 0.13044 0.25668 0.27293 ## X3 0.24000 0.15344 0.00000 0.18750 0.25417 0.27656 0.22935 0.31458 0.35767 ## X4 0.10717 0.10157 0.18750 0.00000 0.18065 0.15483 0.12831 0.21910 0.25948 ## X5 0.11773 0.16524 0.25417 0.18065 0.00000 0.11089 0.11491 0.26057 0.31374 ## X6 0.11482 0.14467 0.27656 0.15483 0.11089 0.00000 0.08381 0.23851 0.27842 ## X10 X11 X12 X13 X14 X15 X16 X18 X19 ## X1 0.30506 0.26169 0.34715 0.27857 0.39477 0.36995 0.36053 0.29083 0.34259 ## X2 0.30683 0.28459 0.35504 0.26397 0.37196 0.36541 0.35676 0.31283 0.31005 ## X3 0.38631 0.34337 0.44137 0.34524 0.40587 0.40521 0.37579 0.42240 0.36712 ## X4 0.26875 0.22184 0.33929 0.23413 0.36887 0.37743 0.32979 0.31647 0.26990 ## X5 0.31012 0.29413 0.33393 0.26518 0.36171 0.36771 0.35540 0.31987 0.32218 ## X6 0.27074 0.28646 0.29556 0.26167 0.36458 0.38211 0.35632 0.28228 0.33013 ## X20 X21 X23 X24 X25 X26 X27 X28 X29 ## X1 0.32208 0.39705 0.41417 0.36951 0.44451 0.47939 0.35175 0.39072 0.51191 ## X2 0.24993 0.45253 0.41518 0.37476 0.45304 0.46454 0.37366 0.37362 0.49434 ## X3 0.25417 0.54167 0.48735 0.48620 0.51667 0.45781 0.43750 0.43818 0.50388 ## X4 0.20347 0.44643 0.43088 0.39755 0.46875 0.45087 0.38889 0.42681 0.47263 ## X5 0.30010 0.45017 0.35466 0.32677 0.39583 0.46230 0.34107 0.41169 0.48235 ## X6 0.30192 0.40980 0.37248 0.32394 0.39645 0.46567 0.33234 0.40289 0.49531 ## X30 X31 ## X1 0.50106 0.43008 ## X2 0.50797 0.39241 ## X3 0.53165 0.37708 ## X4 0.54137 0.42292 ## X5 0.44777 0.34762 ## X6 0.46864 0.37533 Convert genetic distance to flow gdist &lt;- GeNetIt::flow(gdist) head(gdist) ## X1 X2 X3 X4 X5 X6 X7 X8 X9 ## X1 NA 0.86894 0.76000 0.89283 0.88227 0.88518 0.88022 0.77614 0.68522 ## X2 0.86894 NA 0.84656 0.89843 0.83476 0.85533 0.86956 0.74332 0.72707 ## X3 0.76000 0.84656 NA 0.81250 0.74583 0.72344 0.77065 0.68542 0.64233 ## X4 0.89283 0.89843 0.81250 NA 0.81935 0.84517 0.87169 0.78090 0.74052 ## X5 0.88227 0.83476 0.74583 0.81935 NA 0.88911 0.88509 0.73943 0.68626 ## X6 0.88518 0.85533 0.72344 0.84517 0.88911 NA 0.91619 0.76149 0.72158 ## X10 X11 X12 X13 X14 X15 X16 X18 X19 ## X1 0.69494 0.73831 0.65285 0.72143 0.60523 0.63005 0.63947 0.70917 0.65741 ## X2 0.69317 0.71541 0.64496 0.73603 0.62804 0.63459 0.64324 0.68717 0.68995 ## X3 0.61369 0.65663 0.55863 0.65476 0.59413 0.59479 0.62421 0.57760 0.63288 ## X4 0.73125 0.77816 0.66071 0.76587 0.63113 0.62257 0.67021 0.68353 0.73010 ## X5 0.68988 0.70587 0.66607 0.73482 0.63829 0.63229 0.64460 0.68013 0.67782 ## X6 0.72926 0.71354 0.70444 0.73833 0.63542 0.61789 0.64368 0.71772 0.66987 ## X20 X21 X23 X24 X25 X26 X27 X28 X29 ## X1 0.67792 0.60295 0.58583 0.63049 0.55549 0.52061 0.64825 0.60928 0.48809 ## X2 0.75007 0.54747 0.58482 0.62524 0.54696 0.53546 0.62634 0.62638 0.50566 ## X3 0.74583 0.45833 0.51265 0.51380 0.48333 0.54219 0.56250 0.56182 0.49612 ## X4 0.79653 0.55357 0.56912 0.60245 0.53125 0.54913 0.61111 0.57319 0.52737 ## X5 0.69990 0.54983 0.64534 0.67323 0.60417 0.53770 0.65893 0.58831 0.51765 ## X6 0.69808 0.59020 0.62752 0.67606 0.60355 0.53433 0.66766 0.59711 0.50469 ## X30 X31 ## X1 0.49894 0.56992 ## X2 0.49203 0.60759 ## X3 0.46835 0.62292 ## X4 0.45863 0.57708 ## X5 0.55223 0.65238 ## X6 0.53136 0.62467 Convert genetid flow matrix into an unfolded dataframe with site IDs gdist &lt;- GeNetIt::dmatrix.df(gdist) head(gdist) ## X1 X2 distance ## 2 X2 X1 0.86894 ## 3 X3 X1 0.76000 ## 4 X4 X1 0.89283 ## 5 X5 X1 0.88227 ## 6 X6 X1 0.88518 ## 7 X7 X1 0.88022 The default column name is distance as this matrix could represent multiple type of distances. We are renaming here to avoid confusion as to the type of distance here. names(gdist)[3] &lt;- &quot;GDIST&quot; names(gdist) ## [1] &quot;X1&quot; &quot;X2&quot; &quot;GDIST&quot; Some housekeeping here to rename colums to from, to nodes and remove Xs added in conversion from matrix. names(gdist)[1] &lt;- &quot;FROM&quot; names(gdist)[2] &lt;- &quot;TO&quot; gdist[,1] &lt;-sub(&quot;X&quot;, &quot;&quot;, gdist[,1]) gdist[,2] &lt;-sub(&quot;X&quot;, &quot;&quot;, gdist[,2]) names(gdist) ## [1] &quot;FROM&quot; &quot;TO&quot; &quot;GDIST&quot; Create an identifier of to from nodes to connect back to the graph and link gdist data to knn graph. gdist &lt;- cbind(from.to=paste(gdist[,1], gdist[,2], sep=&quot;.&quot;), gdist) dist.graph@data$from.to &lt;- paste(dist.graph$from_ID, dist.graph$to_ID, sep=&quot;.&quot;) dist.graph &lt;- merge(dist.graph, gdist, by = &quot;from.to&quot;) head(dist.graph@data) ## from.to i j from_ID to_ID length FROM TO GDIST ## 483 27.15 1 2 27 15 4052.869 27 15 0.47934 ## 486 27.19 1 3 27 19 3065.350 27 19 0.61603 ## 484 27.16 1 4 27 16 4003.222 27 16 0.63321 ## 478 27.10 1 5 27 10 5675.296 27 10 0.51349 ## 503 27.8 1 6 27 8 6482.365 27 8 0.51932 ## 479 27.11 1 7 27 11 5010.120 27 11 0.50584 c. Write out results (ASCII csv and lines shapefile). Saving the results (as ASCII csv and lines shapefile) is is a really useful operation for your own data, especially if you have a large graph, so you dont have to recreate this object. if(!dir.exists(paste0(here::here(),&quot;/output&quot;))) dir.create(paste0(here::here(),&quot;/output&quot;)) write.csv(gdist, file= paste0(here::here(),&quot;/output/gdist.csv&quot;)) Uncheck the following lines to write out a shapefile. This is commented out as rgdal may not work properly, depending on your computer configuration. #rgdal::writeOGR(dist.graph, paste0(here::here(),&quot;/output&quot;), &quot;DistGraph&quot;, # driver=&quot;ESRI Shapefile&quot;, check_exists=TRUE, overwrite_layer=TRUE) 5. Preparing raster-based covariates a. Import raster data as stack This function will import a raster stack with nine raster maps of the study area. xvars &lt;- rio::import(&quot;https://www.dropbox.com/s/xjl9zpgqplwg1us/ralu.rasters.rds?dl=1&quot;) xvars ## class : RasterStack ## dimensions : 2237, 2347, 5250239, 9 (nrow, ncol, ncell, nlayers) ## resolution : 10, 10 (x, y) ## extent : 677292.5, 700762.5, 4987413, 5009783 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,-0,-0,-0,0 +units=m +no_defs ## names : cti, dd5, ffp, gsp, hli, nlcd, pratio, rough27, srr ## min values : 6.269518e-01, 2.864937e+02, 0.000000e+00, 1.966725e+02, 1.014000e+03, 1.100000e+01, 4.276079e-01, 5.816957e-04, 3.869545e-02 ## max values : 2.605896e+01, 2.143000e+03, 1.170000e+02, 3.380697e+02, 1.000000e+04, 9.500000e+01, 5.329815e-01, 1.859006e+04, 7.725910e-01 names(xvars) ## [1] &quot;cti&quot; &quot;dd5&quot; &quot;ffp&quot; &quot;gsp&quot; &quot;hli&quot; &quot;nlcd&quot; &quot;pratio&quot; ## [8] &quot;rough27&quot; &quot;srr&quot; b. Calculate wetland area within buffer You want to know if areas of dense wetlands produce more frogs. Calculate the proportion of the landscape around each site that is wetland. What buffer distance will you use? We can create a wetland raster from the NLCD data. Wetland classes are 11 (open water), 90 and 95. Create a vector of reclassification where non-wetland is a 0, wetland is a 1. m &lt;- c(0,10.8, 0,10.9,12.1,1,12.9,89.1,0, 89.5,95.1,1) reclass &lt;- matrix(m, ncol=3, byrow=TRUE) Reclassifying nlcd by the reclass matrix wetlnd &lt;- raster::reclassify(xvars$nlcd, reclass) wetlnd@data@names &lt;- &quot;wetlnd&quot; plot(wetlnd) Adding this new parameter to our raster stack xvars &lt;- raster::stack(xvars, wetlnd) names(xvars) ## [1] &quot;cti&quot; &quot;dd5&quot; &quot;ffp&quot; &quot;gsp&quot; &quot;hli&quot; &quot;nlcd&quot; &quot;pratio&quot; ## [8] &quot;rough27&quot; &quot;srr&quot; &quot;wetlnd&quot; c. Create raster of wetland proportion Create a raster of the proportion of landscape (PLAND) that is wetland, using a 300 m radius. Challenge: What happens if you change this radius? What radius do you think makes the most sense ecologically? Here we adapt code from Week 2 Worked Example. nlcd_sampled &lt;- landscapemetrics::sample_lsm(landscape = xvars[[&quot;wetlnd&quot;]], what = &quot;lsm_c_pland&quot;, shape = &quot;circle&quot;, y = sites, size = 300, return_raster = FALSE, plot_id=sites@data$SiteID) pwetland &lt;- dplyr::select(dplyr::filter(nlcd_sampled, class == 1, metric == &quot;pland&quot;), plot_id, value) names(pwetland) &lt;- c(&quot;SiteID&quot;, &quot;pwetland&quot;) pwetland$pwetland &lt;- pwetland$pwetland/100 head(pwetland) ## # A tibble: 6 x 2 ## SiteID pwetland ## &lt;fct&gt; &lt;dbl&gt; ## 1 2 0.0948 ## 2 3 0.0835 ## 3 4 0.102 ## 4 5 0.0609 ## 5 7 0.0516 ## 6 8 0.108 Important: there could be cases with no wetlands (where prop.landscape should be zero), which will result in missing rows in pwetland. Here we use left_join to join rows withi sites by SiteID, then replace missing values by 0. sites@data &lt;- dplyr::left_join(sites@data, pwetland) ## Joining, by = &quot;SiteID&quot; sites@data$pwetland[is.na(sites@data$pwetland)] &lt;- 0 head(sites@data) ## SiteName degree betweenness SiteID Elev Length Area Perim Depth ## 1 AirplaneLake 8 279.58405 27 2564.381 390 62582.2 1142.8 21.64 ## 2 BachelorMeadow 7 292.32533 15 2591.781 0 225.0 60.0 0.40 ## 3 BarkingFoxLake 9 512.37876 19 2545.275 160 12000.0 435.0 5.00 ## 4 BobLake 6 466.35631 16 2649.125 143 4600.0 321.4 2.00 ## 5 CacheLake 4 11.57913 10 2475.829 75 2268.8 192.0 1.86 ## 6 DoeLake 5 31.48064 8 2463.006 170 13034.9 463.2 6.03 ## pH Dforest Drock Dshrub pwetland ## 1 6.5 0.398 0.051 0.00 0.195821530 ## 2 6.1 0.000 0.000 0.20 0.015929204 ## 3 6.5 0.400 0.250 0.05 0.051009564 ## 4 7.0 0.550 0.000 0.05 0.000000000 ## 5 6.5 0.508 0.000 0.00 0.004253811 ## 6 7.6 0.254 0.000 0.00 0.107954545 Challenge: Try creating some additional metrics of your own from these rasters. 6. Extract raster stats for nodes and edges a. Extract at-site variables from rasters Add characteristics of sample sites from your rasters as potential at site variables. Here we are extracting the raster values that intersect our wetlands (point data). What raster is not included in this step? Why? What at-site characteristics may impact the production of potential migrants? sites@data &lt;- data.frame(sites@data, raster::extract(xvars, sites)) names(sites@data) ## [1] &quot;SiteName&quot; &quot;degree&quot; &quot;betweenness&quot; &quot;SiteID&quot; &quot;Elev&quot; ## [6] &quot;Length&quot; &quot;Area&quot; &quot;Perim&quot; &quot;Depth&quot; &quot;pH&quot; ## [11] &quot;Dforest&quot; &quot;Drock&quot; &quot;Dshrub&quot; &quot;pwetland&quot; &quot;cti&quot; ## [16] &quot;dd5&quot; &quot;ffp&quot; &quot;gsp&quot; &quot;hli&quot; &quot;nlcd&quot; ## [21] &quot;pratio&quot; &quot;rough27&quot; &quot;srr&quot; &quot;wetlnd&quot; b. Add covariates to graph edges. Calculating statistical moments (e.g., mean, sdev) for categorical variables makes no sense. We we will remove them here. idx &lt;- which(names(xvars) %in% c(&quot;nlcd&quot;,&quot;wetlnd&quot;)) c. Calculating stats along edges Here we define the projection of the distance graph and then calculate statistics You can calculate any statistical moment you wish from your sample of the landscape between nodes. Make sure that these moments are ecologically/biologically meaningful. dist.graph@proj4string@projargs &lt;- &quot;+proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,-0,-0,-0,0 +units=m +no_defs &quot; stats &lt;- GeNetIt::graph.statistics(dist.graph, r = xvars[[-idx]], buffer= NULL, stats = c(&quot;min&quot;, &quot;mean&quot;, &quot;max&quot;, &quot;var&quot;, &quot;median&quot;)) ## Warning in wkt(obj): CRS object has no comment dist.graph@data &lt;- data.frame(dist.graph@data, stats) names(dist.graph@data) ## [1] &quot;from.to&quot; &quot;i&quot; &quot;j&quot; &quot;from_ID&quot; ## [5] &quot;to_ID&quot; &quot;length&quot; &quot;FROM&quot; &quot;TO&quot; ## [9] &quot;GDIST&quot; &quot;min.cti&quot; &quot;min.dd5&quot; &quot;min.ffp&quot; ## [13] &quot;min.gsp&quot; &quot;min.hli&quot; &quot;min.pratio&quot; &quot;min.rough27&quot; ## [17] &quot;min.srr&quot; &quot;mean.cti&quot; &quot;mean.dd5&quot; &quot;mean.ffp&quot; ## [21] &quot;mean.gsp&quot; &quot;mean.hli&quot; &quot;mean.pratio&quot; &quot;mean.rough27&quot; ## [25] &quot;mean.srr&quot; &quot;max.cti&quot; &quot;max.dd5&quot; &quot;max.ffp&quot; ## [29] &quot;max.gsp&quot; &quot;max.hli&quot; &quot;max.pratio&quot; &quot;max.rough27&quot; ## [33] &quot;max.srr&quot; &quot;var.cti&quot; &quot;var.dd5&quot; &quot;var.ffp&quot; ## [37] &quot;var.gsp&quot; &quot;var.hli&quot; &quot;var.pratio&quot; &quot;var.rough27&quot; ## [41] &quot;var.srr&quot; &quot;median.cti&quot; &quot;median.dd5&quot; &quot;median.ffp&quot; ## [45] &quot;median.gsp&quot; &quot;median.hli&quot; &quot;median.pratio&quot; &quot;median.rough27&quot; ## [49] &quot;median.srr&quot; Bonus challenge: Create a function for returning the 95th percentile. How would you create this function and add it to the stats calculated? What other statistical moments do you want? Can you create a function? d. Calculating stats for categorical variables What about categorical variables? Moments are nonsensical. Create a function for returning the % wetland between sites. IMPORTANT: We do not want these values to be correlated with distance, so dont count number pixels. Then use this function to calculate an additional statistic, and then add it to the graph. Are there other categorical variables that you think may be ecologically important? wet.pct &lt;- function(x) { x &lt;- ifelse( x == 11 | x == 90 | x == 95, 1, 0) prop.table(table(x))[2] } wetstats &lt;- GeNetIt::graph.statistics(dist.graph, r=xvars$nlcd, buffer= NULL, stats = c(&quot;wet.pct&quot;)) ## Warning in wkt(obj): CRS object has no comment wetstats[is.na(wetstats)] &lt;- 0 dist.graph@data &lt;- data.frame(dist.graph@data, wetstats) names(dist.graph@data) ## [1] &quot;from.to&quot; &quot;i&quot; &quot;j&quot; &quot;from_ID&quot; ## [5] &quot;to_ID&quot; &quot;length&quot; &quot;FROM&quot; &quot;TO&quot; ## [9] &quot;GDIST&quot; &quot;min.cti&quot; &quot;min.dd5&quot; &quot;min.ffp&quot; ## [13] &quot;min.gsp&quot; &quot;min.hli&quot; &quot;min.pratio&quot; &quot;min.rough27&quot; ## [17] &quot;min.srr&quot; &quot;mean.cti&quot; &quot;mean.dd5&quot; &quot;mean.ffp&quot; ## [21] &quot;mean.gsp&quot; &quot;mean.hli&quot; &quot;mean.pratio&quot; &quot;mean.rough27&quot; ## [25] &quot;mean.srr&quot; &quot;max.cti&quot; &quot;max.dd5&quot; &quot;max.ffp&quot; ## [29] &quot;max.gsp&quot; &quot;max.hli&quot; &quot;max.pratio&quot; &quot;max.rough27&quot; ## [33] &quot;max.srr&quot; &quot;var.cti&quot; &quot;var.dd5&quot; &quot;var.ffp&quot; ## [37] &quot;var.gsp&quot; &quot;var.hli&quot; &quot;var.pratio&quot; &quot;var.rough27&quot; ## [41] &quot;var.srr&quot; &quot;median.cti&quot; &quot;median.dd5&quot; &quot;median.ffp&quot; ## [45] &quot;median.gsp&quot; &quot;median.hli&quot; &quot;median.pratio&quot; &quot;median.rough27&quot; ## [49] &quot;median.srr&quot; &quot;wet.pct.nlcd&quot; Question: What other categorical variables would you like to include in the analysis? Can you create a function for these variables? e. Add node data to graph First create a list of names for the node data node.var &lt;- c(&quot;degree&quot;, &quot;betweenness&quot;, &quot;Elev&quot;, &quot;Length&quot;, &quot;Area&quot;, &quot;Perim&quot;, &quot;Depth&quot;, &quot;pH&quot;,&quot;Dforest&quot;,&quot;Drock&quot;, &quot;Dshrub&quot;, &quot;pwetland&quot;, &quot;cti&quot;, &quot;dd5&quot;, &quot;ffp&quot;,&quot;gsp&quot;,&quot;pratio&quot;,&quot;hli&quot;,&quot;rough27&quot;,&quot;srr&quot;) These are all at site variables. Remember that we pulled all raster varibles. We want to critically think about hypotheses and not use all of these parameters. degree - graph degree betweenness - graph betweeness Elev - elevation (see comments below) Length - geographic distance Area - wetland area (field) Perim - wetland perimeter (field) Depth - wetland depth (field)- highly correlated with predatory fish presence/abundance pH - wetland pH (field) Dforest - distance to forest (field) Drock - distance to rock (field) Dshrub - distance to shrub (field) pwetland - proportion of wetland in X buffer (calculated above) cti - compound topographic wetness index - steady-state measure of wetness based on topography (raster data) dd5 - degree days &gt;5 C (sum of temp) - (raster data) ffp - frost free period (raster data) gsp - growing season precipitation (raster data) pratio - ratio of growing season precip to annual precip (raster data) - can indicate amount of snow to rain hli - heat load index - topographic measure of exposure, related to productivity (ice-off and primary productivity) in this system (raster data) rough27 - unscale topographic variation at a 27 X 27 (cells) window size (raster data) ssr - measure of topographic variation at a 27X27 (cells) windo size - for this system pulling out ridgelines (raster data) NOTE: we are adding elevation here as a covariate. HOWEVER - elevation does not represent ecological processes in and of itself. I strongly encourage using the components (temp, moisture, rainfall, vegetation, accessibility, etc.) directly and not elevation as a surrogate parameter. Add the node data. Remember that we will calculate the value for both the from node and the to node. in a singly constained model, we will only use one of these (constrain (group) by from (produciton) or to (attraction)) node &lt;- GeNetIt::build.node.data(sites@data, group.ids = &quot;SiteID&quot;, from.parms = node.var) head(node) ## SiteID from.degree from.betweenness from.Elev from.Length from.Area ## 1 27 8 279.58405 2564.381 390 62582.2 ## 2 15 7 292.32533 2591.781 0 225.0 ## 3 19 9 512.37876 2545.275 160 12000.0 ## 4 16 6 466.35631 2649.125 143 4600.0 ## 5 10 4 11.57913 2475.829 75 2268.8 ## 6 8 5 31.48064 2463.006 170 13034.9 ## from.Perim from.Depth from.pH from.Dforest from.Drock from.Dshrub ## 1 1142.8 21.64 6.5 0.398 0.051 0.00 ## 2 60.0 0.40 6.1 0.000 0.000 0.20 ## 3 435.0 5.00 6.5 0.400 0.250 0.05 ## 4 321.4 2.00 7.0 0.550 0.000 0.05 ## 5 192.0 1.86 6.5 0.508 0.000 0.00 ## 6 463.2 6.03 7.6 0.254 0.000 0.00 ## from.pwetland from.cti from.dd5 from.ffp from.gsp from.pratio from.hli ## 1 0.195821530 6.018890 522.0973 8.000000 286.1415 1732 0.4485655 ## 2 0.015929204 5.860837 512.0000 8.000000 286.0000 2090 0.4470909 ## 3 0.051009564 2.951499 502.4061 7.285662 288.5140 3070 0.4461185 ## 4 0.000000000 5.120589 457.2643 5.000000 294.8773 1527 0.4458445 ## 5 0.004253811 4.110081 565.8234 9.610519 276.4601 1977 0.4521334 ## 6 0.107954545 3.712187 552.0031 10.000000 278.0000 1751 0.4514005 ## from.rough27 from.srr to.degree to.betweenness to.Elev to.Length to.Area ## 1 460.0905 0.2309995 8 279.58405 2564.381 390 62582.2 ## 2 401.6590 0.4483430 7 292.32533 2591.781 0 225.0 ## 3 2772.4165 0.3047715 9 512.37876 2545.275 160 12000.0 ## 4 352.5726 0.5804766 6 466.35631 2649.125 143 4600.0 ## 5 119.7234 0.4306586 4 11.57913 2475.829 75 2268.8 ## 6 603.1037 0.3480034 5 31.48064 2463.006 170 13034.9 ## to.Perim to.Depth to.pH to.Dforest to.Drock to.Dshrub to.pwetland to.cti ## 1 1142.8 21.64 6.5 0.398 0.051 0.00 0.195821530 6.018890 ## 2 60.0 0.40 6.1 0.000 0.000 0.20 0.015929204 5.860837 ## 3 435.0 5.00 6.5 0.400 0.250 0.05 0.051009564 2.951499 ## 4 321.4 2.00 7.0 0.550 0.000 0.05 0.000000000 5.120589 ## 5 192.0 1.86 6.5 0.508 0.000 0.00 0.004253811 4.110081 ## 6 463.2 6.03 7.6 0.254 0.000 0.00 0.107954545 3.712187 ## to.dd5 to.ffp to.gsp to.pratio to.hli to.rough27 to.srr ## 1 522.0973 8.000000 286.1415 1732 0.4485655 460.0905 0.2309995 ## 2 512.0000 8.000000 286.0000 2090 0.4470909 401.6590 0.4483430 ## 3 502.4061 7.285662 288.5140 3070 0.4461185 2772.4165 0.3047715 ## 4 457.2643 5.000000 294.8773 1527 0.4458445 352.5726 0.5804766 ## 5 565.8234 9.610519 276.4601 1977 0.4521334 119.7234 0.4306586 ## 6 552.0031 10.000000 278.0000 1751 0.4514005 603.1037 0.3480034 7. Gravity Models Now we get to create gravity models! a. Merge distance and site data Merge edge (distance graph) and edge (site) data. gdata &lt;- merge(dist.graph, node, by.x=&quot;from_ID&quot;, by.y=&quot;SiteID&quot;) gdata &lt;- gdata@data names(gdata) ## [1] &quot;from_ID&quot; &quot;from.to&quot; &quot;i&quot; &quot;j&quot; ## [5] &quot;to_ID&quot; &quot;length&quot; &quot;FROM&quot; &quot;TO&quot; ## [9] &quot;GDIST&quot; &quot;min.cti&quot; &quot;min.dd5&quot; &quot;min.ffp&quot; ## [13] &quot;min.gsp&quot; &quot;min.hli&quot; &quot;min.pratio&quot; &quot;min.rough27&quot; ## [17] &quot;min.srr&quot; &quot;mean.cti&quot; &quot;mean.dd5&quot; &quot;mean.ffp&quot; ## [21] &quot;mean.gsp&quot; &quot;mean.hli&quot; &quot;mean.pratio&quot; &quot;mean.rough27&quot; ## [25] &quot;mean.srr&quot; &quot;max.cti&quot; &quot;max.dd5&quot; &quot;max.ffp&quot; ## [29] &quot;max.gsp&quot; &quot;max.hli&quot; &quot;max.pratio&quot; &quot;max.rough27&quot; ## [33] &quot;max.srr&quot; &quot;var.cti&quot; &quot;var.dd5&quot; &quot;var.ffp&quot; ## [37] &quot;var.gsp&quot; &quot;var.hli&quot; &quot;var.pratio&quot; &quot;var.rough27&quot; ## [41] &quot;var.srr&quot; &quot;median.cti&quot; &quot;median.dd5&quot; &quot;median.ffp&quot; ## [45] &quot;median.gsp&quot; &quot;median.hli&quot; &quot;median.pratio&quot; &quot;median.rough27&quot; ## [49] &quot;median.srr&quot; &quot;wet.pct.nlcd&quot; &quot;from.degree&quot; &quot;from.betweenness&quot; ## [53] &quot;from.Elev&quot; &quot;from.Length&quot; &quot;from.Area&quot; &quot;from.Perim&quot; ## [57] &quot;from.Depth&quot; &quot;from.pH&quot; &quot;from.Dforest&quot; &quot;from.Drock&quot; ## [61] &quot;from.Dshrub&quot; &quot;from.pwetland&quot; &quot;from.cti&quot; &quot;from.dd5&quot; ## [65] &quot;from.ffp&quot; &quot;from.gsp&quot; &quot;from.pratio&quot; &quot;from.hli&quot; ## [69] &quot;from.rough27&quot; &quot;from.srr&quot; &quot;to.degree&quot; &quot;to.betweenness&quot; ## [73] &quot;to.Elev&quot; &quot;to.Length&quot; &quot;to.Area&quot; &quot;to.Perim&quot; ## [77] &quot;to.Depth&quot; &quot;to.pH&quot; &quot;to.Dforest&quot; &quot;to.Drock&quot; ## [81] &quot;to.Dshrub&quot; &quot;to.pwetland&quot; &quot;to.cti&quot; &quot;to.dd5&quot; ## [85] &quot;to.ffp&quot; &quot;to.gsp&quot; &quot;to.pratio&quot; &quot;to.hli&quot; ## [89] &quot;to.rough27&quot; &quot;to.srr&quot; b. Define a set of models Think about hypothesis and create a set of models. What type of constraint? Write out model statements. HOWEVER, need to check for correlations (#3) before settling on a final set of model. At this point, make a list of these models in a text editor, but do not run these models until you check correlations. c. Check for correlations You will need to do nodes and edges separately, remember that data have to be ln transformed. For zero values, a common approach is ln(x - (min(x) - 1)) nodeln &lt;- node[,c(2:21)] for(i in 1:ncol(nodeln)) { nodeln[,i] &lt;- log(nodeln[,i] - (min(nodeln[,i]) - 1)) } nodecor.ln &lt;- cor(nodeln, y = NULL, use = &quot;complete.obs&quot;, method = &quot;pearson&quot;) round(nodecor.ln, 3) ## from.degree from.betweenness from.Elev from.Length from.Area ## from.degree 1.000 0.912 0.062 -0.156 -0.078 ## from.betweenness 0.912 1.000 -0.140 -0.152 -0.090 ## from.Elev 0.062 -0.140 1.000 -0.402 -0.449 ## from.Length -0.156 -0.152 -0.402 1.000 0.871 ## from.Area -0.078 -0.090 -0.449 0.871 1.000 ## from.Perim -0.108 -0.111 -0.450 0.838 0.991 ## from.Depth -0.056 -0.075 -0.381 0.759 0.816 ## from.pH -0.102 -0.164 -0.189 0.292 0.474 ## from.Dforest 0.061 0.141 0.158 -0.183 -0.494 ## from.Drock 0.036 0.087 -0.080 0.269 0.277 ## from.Dshrub 0.225 0.237 0.074 -0.297 -0.165 ## from.pwetland 0.003 0.155 -0.758 0.400 0.511 ## from.cti -0.082 0.070 -0.199 -0.070 -0.131 ## from.dd5 -0.111 -0.140 -0.482 0.499 0.713 ## from.ffp -0.069 -0.032 -0.622 0.368 0.457 ## from.gsp -0.006 -0.021 0.580 -0.225 -0.225 ## from.pratio 0.109 -0.037 0.322 0.167 0.172 ## from.hli -0.214 -0.174 -0.640 0.376 0.338 ## from.rough27 0.198 0.055 0.607 -0.117 -0.145 ## from.srr 0.070 -0.031 0.530 -0.400 -0.481 ## from.Perim from.Depth from.pH from.Dforest from.Drock ## from.degree -0.108 -0.056 -0.102 0.061 0.036 ## from.betweenness -0.111 -0.075 -0.164 0.141 0.087 ## from.Elev -0.450 -0.381 -0.189 0.158 -0.080 ## from.Length 0.838 0.759 0.292 -0.183 0.269 ## from.Area 0.991 0.816 0.474 -0.494 0.277 ## from.Perim 1.000 0.762 0.523 -0.545 0.234 ## from.Depth 0.762 1.000 0.239 -0.257 0.370 ## from.pH 0.523 0.239 1.000 -0.587 -0.067 ## from.Dforest -0.545 -0.257 -0.587 1.000 -0.310 ## from.Drock 0.234 0.370 -0.067 -0.310 1.000 ## from.Dshrub -0.135 -0.158 0.101 -0.282 0.326 ## from.pwetland 0.492 0.540 0.049 -0.037 -0.037 ## from.cti -0.117 -0.059 -0.211 0.239 -0.183 ## from.dd5 0.739 0.422 0.655 -0.698 0.208 ## from.ffp 0.456 0.332 0.451 -0.415 0.245 ## from.gsp -0.218 -0.122 -0.283 0.159 -0.050 ## from.pratio 0.160 0.118 0.132 -0.281 0.194 ## from.hli 0.354 0.144 0.453 -0.229 -0.066 ## from.rough27 -0.158 -0.209 -0.110 0.046 0.154 ## from.srr -0.465 -0.369 -0.019 -0.065 0.152 ## from.Dshrub from.pwetland from.cti from.dd5 from.ffp from.gsp ## from.degree 0.225 0.003 -0.082 -0.111 -0.069 -0.006 ## from.betweenness 0.237 0.155 0.070 -0.140 -0.032 -0.021 ## from.Elev 0.074 -0.758 -0.199 -0.482 -0.622 0.580 ## from.Length -0.297 0.400 -0.070 0.499 0.368 -0.225 ## from.Area -0.165 0.511 -0.131 0.713 0.457 -0.225 ## from.Perim -0.135 0.492 -0.117 0.739 0.456 -0.218 ## from.Depth -0.158 0.540 -0.059 0.422 0.332 -0.122 ## from.pH 0.101 0.049 -0.211 0.655 0.451 -0.283 ## from.Dforest -0.282 -0.037 0.239 -0.698 -0.415 0.159 ## from.Drock 0.326 -0.037 -0.183 0.208 0.245 -0.050 ## from.Dshrub 1.000 -0.209 -0.111 0.089 0.143 0.025 ## from.pwetland -0.209 1.000 0.454 0.246 0.256 -0.188 ## from.cti -0.111 0.454 1.000 -0.169 -0.003 -0.034 ## from.dd5 0.089 0.246 -0.169 1.000 0.850 -0.629 ## from.ffp 0.143 0.256 -0.003 0.850 1.000 -0.798 ## from.gsp 0.025 -0.188 -0.034 -0.629 -0.798 1.000 ## from.pratio 0.219 -0.454 -0.736 0.232 0.081 0.083 ## from.hli -0.074 0.223 0.003 0.716 0.830 -0.856 ## from.rough27 0.110 -0.632 -0.670 -0.156 -0.278 0.340 ## from.srr 0.251 -0.771 -0.366 -0.258 -0.239 0.067 ## from.pratio from.hli from.rough27 from.srr ## from.degree 0.109 -0.214 0.198 0.070 ## from.betweenness -0.037 -0.174 0.055 -0.031 ## from.Elev 0.322 -0.640 0.607 0.530 ## from.Length 0.167 0.376 -0.117 -0.400 ## from.Area 0.172 0.338 -0.145 -0.481 ## from.Perim 0.160 0.354 -0.158 -0.465 ## from.Depth 0.118 0.144 -0.209 -0.369 ## from.pH 0.132 0.453 -0.110 -0.019 ## from.Dforest -0.281 -0.229 0.046 -0.065 ## from.Drock 0.194 -0.066 0.154 0.152 ## from.Dshrub 0.219 -0.074 0.110 0.251 ## from.pwetland -0.454 0.223 -0.632 -0.771 ## from.cti -0.736 0.003 -0.670 -0.366 ## from.dd5 0.232 0.716 -0.156 -0.258 ## from.ffp 0.081 0.830 -0.278 -0.239 ## from.gsp 0.083 -0.856 0.340 0.067 ## from.pratio 1.000 -0.015 0.693 0.330 ## from.hli -0.015 1.000 -0.302 -0.227 ## from.rough27 0.693 -0.302 1.000 0.273 ## from.srr 0.330 -0.227 0.273 1.000 #pairs(nodecor.ln, pch=19, cex=0.50) edge.ln &lt;- dist.graph@data[,10:length(dist.graph@data)] for(i in 1:ncol(edge.ln)) { edge.ln[,i] &lt;- log(edge.ln[,i] - (min(edge.ln[,i]) - 1)) } edgecor.ln &lt;- cor(edge.ln, y = NULL, use = &quot;complete.obs&quot;, method = &quot;pearson&quot;) round(edgecor.ln, 3) ## min.cti min.dd5 min.ffp min.gsp min.hli min.pratio min.rough27 ## min.cti 1.000 0.523 0.606 0.114 0.233 0.594 0.102 ## min.dd5 0.523 1.000 0.915 -0.301 0.502 0.868 0.177 ## min.ffp 0.606 0.915 1.000 -0.288 0.498 0.912 0.180 ## min.gsp 0.114 -0.301 -0.288 1.000 -0.001 -0.279 0.265 ## min.hli 0.233 0.502 0.498 -0.001 1.000 0.520 0.347 ## min.pratio 0.594 0.868 0.912 -0.279 0.520 1.000 0.191 ## min.rough27 0.102 0.177 0.180 0.265 0.347 0.191 1.000 ## min.srr 0.254 0.348 0.379 0.125 0.383 0.360 0.750 ## mean.cti 0.264 0.106 0.127 -0.007 -0.055 0.117 -0.544 ## mean.dd5 0.167 0.663 0.670 -0.671 0.362 0.637 -0.135 ## mean.ffp 0.207 0.695 0.720 -0.706 0.380 0.684 -0.130 ## mean.gsp -0.356 -0.705 -0.776 0.625 -0.426 -0.801 -0.021 ## mean.hli -0.423 -0.171 -0.192 -0.045 0.150 -0.253 0.363 ## mean.pratio 0.259 0.702 0.709 -0.655 0.449 0.806 -0.024 ## mean.rough27 -0.570 -0.536 -0.589 0.128 -0.303 -0.686 0.141 ## mean.srr 0.081 0.129 0.152 -0.159 0.023 0.099 0.343 ## max.cti -0.255 -0.275 -0.306 -0.148 -0.263 -0.314 -0.428 ## max.dd5 -0.196 0.183 0.163 -0.790 -0.087 0.128 -0.384 ## max.ffp -0.215 0.188 0.162 -0.801 -0.083 0.121 -0.407 ## max.gsp -0.577 -0.760 -0.850 0.248 -0.456 -0.899 -0.150 ## max.hli -0.608 -0.391 -0.453 -0.163 -0.209 -0.507 -0.069 ## max.pratio -0.147 0.296 0.264 -0.887 0.005 0.322 -0.306 ## max.rough27 -0.672 -0.540 -0.616 -0.017 -0.306 -0.687 -0.034 ## max.srr -0.268 -0.247 -0.263 -0.172 -0.208 -0.303 -0.030 ## var.cti -0.015 0.034 0.035 -0.091 -0.025 -0.027 -0.506 ## var.dd5 -0.565 -0.527 -0.605 -0.314 -0.486 -0.636 -0.374 ## var.ffp -0.593 -0.565 -0.660 -0.327 -0.514 -0.666 -0.417 ## var.gsp -0.620 -0.672 -0.755 -0.174 -0.539 -0.775 -0.330 ## var.hli -0.499 -0.312 -0.346 -0.129 -0.176 -0.409 0.013 ## var.pratio -0.488 -0.572 -0.616 -0.308 -0.552 -0.610 -0.413 ## var.rough27 -0.636 -0.500 -0.569 -0.040 -0.305 -0.641 -0.081 ## var.srr -0.031 -0.225 -0.240 0.028 -0.236 -0.199 -0.559 ## median.cti 0.257 0.015 0.019 0.025 -0.157 0.053 -0.494 ## median.dd5 0.136 0.590 0.609 -0.622 0.373 0.574 -0.114 ## median.ffp 0.205 0.641 0.669 -0.659 0.388 0.641 -0.096 ## median.gsp -0.295 -0.658 -0.714 0.667 -0.405 -0.731 -0.011 ## median.hli -0.297 -0.020 -0.027 -0.085 0.285 -0.111 0.363 ## median.pratio 0.236 0.650 0.655 -0.639 0.446 0.754 -0.019 ## median.rough27 -0.474 -0.488 -0.525 0.193 -0.280 -0.600 0.266 ## median.srr 0.099 0.138 0.160 -0.183 -0.006 0.105 0.233 ## wet.pct.nlcd 0.174 0.159 0.202 0.021 0.011 0.145 -0.395 ## min.srr mean.cti mean.dd5 mean.ffp mean.gsp mean.hli mean.pratio ## min.cti 0.254 0.264 0.167 0.207 -0.356 -0.423 0.259 ## min.dd5 0.348 0.106 0.663 0.695 -0.705 -0.171 0.702 ## min.ffp 0.379 0.127 0.670 0.720 -0.776 -0.192 0.709 ## min.gsp 0.125 -0.007 -0.671 -0.706 0.625 -0.045 -0.655 ## min.hli 0.383 -0.055 0.362 0.380 -0.426 0.150 0.449 ## min.pratio 0.360 0.117 0.637 0.684 -0.801 -0.253 0.806 ## min.rough27 0.750 -0.544 -0.135 -0.130 -0.021 0.363 -0.024 ## min.srr 1.000 -0.433 0.088 0.102 -0.200 0.242 0.129 ## mean.cti -0.433 1.000 0.141 0.162 -0.105 -0.485 0.133 ## mean.dd5 0.088 0.141 1.000 0.972 -0.791 -0.078 0.848 ## mean.ffp 0.102 0.162 0.972 1.000 -0.860 -0.089 0.889 ## mean.gsp -0.200 -0.105 -0.791 -0.860 1.000 0.091 -0.878 ## mean.hli 0.242 -0.485 -0.078 -0.089 0.091 1.000 -0.140 ## mean.pratio 0.129 0.133 0.848 0.889 -0.878 -0.140 1.000 ## mean.rough27 -0.060 -0.417 -0.430 -0.469 0.576 0.562 -0.552 ## mean.srr 0.560 -0.541 0.073 0.082 -0.104 0.160 0.041 ## max.cti -0.455 0.462 -0.046 -0.051 0.091 0.017 -0.090 ## max.dd5 -0.282 0.163 0.659 0.672 -0.480 0.079 0.513 ## max.ffp -0.271 0.140 0.682 0.687 -0.476 0.074 0.513 ## max.gsp -0.320 -0.146 -0.577 -0.638 0.851 0.208 -0.680 ## max.hli -0.161 -0.134 -0.111 -0.120 0.219 0.538 -0.219 ## max.pratio -0.208 0.094 0.683 0.711 -0.579 -0.029 0.744 ## max.rough27 -0.191 -0.270 -0.290 -0.326 0.491 0.453 -0.405 ## max.srr 0.014 -0.312 -0.044 -0.062 0.153 0.151 -0.120 ## var.cti -0.432 0.757 0.151 0.162 -0.102 -0.155 0.057 ## var.dd5 -0.469 0.037 -0.127 -0.144 0.339 0.154 -0.210 ## var.ffp -0.473 -0.005 -0.124 -0.156 0.403 0.082 -0.222 ## var.gsp -0.452 -0.040 -0.296 -0.327 0.517 0.170 -0.377 ## var.hli -0.058 -0.155 -0.111 -0.109 0.166 0.614 -0.208 ## var.pratio -0.450 0.011 -0.159 -0.184 0.354 0.044 -0.232 ## var.rough27 -0.200 -0.149 -0.249 -0.278 0.424 0.454 -0.366 ## var.srr -0.718 0.353 -0.115 -0.126 0.165 -0.394 -0.097 ## median.cti -0.413 0.868 0.031 0.046 0.012 -0.549 0.082 ## median.dd5 0.087 0.134 0.964 0.935 -0.748 -0.052 0.812 ## median.ffp 0.104 0.155 0.931 0.951 -0.835 -0.091 0.868 ## median.gsp -0.192 -0.083 -0.805 -0.870 0.960 0.076 -0.886 ## median.hli 0.260 -0.497 0.066 0.064 -0.022 0.741 -0.005 ## median.pratio 0.121 0.126 0.836 0.872 -0.852 -0.139 0.980 ## median.rough27 0.044 -0.489 -0.474 -0.515 0.549 0.619 -0.573 ## median.srr 0.413 -0.455 0.079 0.100 -0.130 0.067 0.054 ## wet.pct.nlcd -0.321 0.648 0.127 0.141 -0.156 -0.346 0.060 ## mean.rough27 mean.srr max.cti max.dd5 max.ffp max.gsp max.hli ## min.cti -0.570 0.081 -0.255 -0.196 -0.215 -0.577 -0.608 ## min.dd5 -0.536 0.129 -0.275 0.183 0.188 -0.760 -0.391 ## min.ffp -0.589 0.152 -0.306 0.163 0.162 -0.850 -0.453 ## min.gsp 0.128 -0.159 -0.148 -0.790 -0.801 0.248 -0.163 ## min.hli -0.303 0.023 -0.263 -0.087 -0.083 -0.456 -0.209 ## min.pratio -0.686 0.099 -0.314 0.128 0.121 -0.899 -0.507 ## min.rough27 0.141 0.343 -0.428 -0.384 -0.407 -0.150 -0.069 ## min.srr -0.060 0.560 -0.455 -0.282 -0.271 -0.320 -0.161 ## mean.cti -0.417 -0.541 0.462 0.163 0.140 -0.146 -0.134 ## mean.dd5 -0.430 0.073 -0.046 0.659 0.682 -0.577 -0.111 ## mean.ffp -0.469 0.082 -0.051 0.672 0.687 -0.638 -0.120 ## mean.gsp 0.576 -0.104 0.091 -0.480 -0.476 0.851 0.219 ## mean.hli 0.562 0.160 0.017 0.079 0.074 0.208 0.538 ## mean.pratio -0.552 0.041 -0.090 0.513 0.513 -0.680 -0.219 ## mean.rough27 1.000 0.120 0.139 -0.078 -0.061 0.714 0.543 ## mean.srr 0.120 1.000 -0.317 -0.046 -0.003 -0.056 -0.040 ## max.cti 0.139 -0.317 1.000 0.373 0.319 0.256 0.426 ## max.dd5 -0.078 -0.046 0.373 1.000 0.968 -0.156 0.301 ## max.ffp -0.061 -0.003 0.319 0.968 1.000 -0.147 0.312 ## max.gsp 0.714 -0.056 0.256 -0.156 -0.147 1.000 0.441 ## max.hli 0.543 -0.040 0.426 0.301 0.312 0.441 1.000 ## max.pratio -0.186 0.043 0.176 0.803 0.811 -0.222 0.124 ## max.rough27 0.905 0.031 0.278 0.087 0.100 0.727 0.630 ## max.srr 0.330 0.555 0.136 0.120 0.139 0.341 0.243 ## var.cti -0.109 -0.458 0.685 0.244 0.231 -0.040 0.141 ## var.dd5 0.504 -0.103 0.575 0.504 0.481 0.669 0.552 ## var.ffp 0.485 -0.055 0.409 0.443 0.459 0.717 0.493 ## var.gsp 0.587 -0.073 0.482 0.324 0.319 0.804 0.552 ## var.hli 0.565 -0.003 0.406 0.258 0.264 0.348 0.930 ## var.pratio 0.379 -0.037 0.314 0.368 0.377 0.592 0.365 ## var.rough27 0.802 -0.064 0.407 0.167 0.175 0.644 0.790 ## var.srr -0.023 -0.281 0.301 0.078 0.087 0.198 -0.059 ## median.cti -0.401 -0.474 0.297 0.104 0.077 -0.042 -0.186 ## median.dd5 -0.395 0.062 -0.040 0.611 0.634 -0.520 -0.090 ## median.ffp -0.451 0.069 -0.041 0.606 0.618 -0.599 -0.112 ## median.gsp 0.507 -0.147 0.069 -0.508 -0.508 0.727 0.176 ## median.hli 0.367 0.197 -0.096 0.116 0.119 0.072 0.289 ## median.pratio -0.526 0.047 -0.076 0.502 0.500 -0.629 -0.197 ## median.rough27 0.858 0.191 0.048 -0.160 -0.150 0.591 0.430 ## median.srr 0.078 0.946 -0.304 -0.016 0.027 -0.084 -0.075 ## wet.pct.nlcd -0.311 -0.431 0.309 0.029 0.011 -0.223 -0.155 ## max.pratio max.rough27 max.srr var.cti var.dd5 var.ffp var.gsp ## min.cti -0.147 -0.672 -0.268 -0.015 -0.565 -0.593 -0.620 ## min.dd5 0.296 -0.540 -0.247 0.034 -0.527 -0.565 -0.672 ## min.ffp 0.264 -0.616 -0.263 0.035 -0.605 -0.660 -0.755 ## min.gsp -0.887 -0.017 -0.172 -0.091 -0.314 -0.327 -0.174 ## min.hli 0.005 -0.306 -0.208 -0.025 -0.486 -0.514 -0.539 ## min.pratio 0.322 -0.687 -0.303 -0.027 -0.636 -0.666 -0.775 ## min.rough27 -0.306 -0.034 -0.030 -0.506 -0.374 -0.417 -0.330 ## min.srr -0.208 -0.191 0.014 -0.432 -0.469 -0.473 -0.452 ## mean.cti 0.094 -0.270 -0.312 0.757 0.037 -0.005 -0.040 ## mean.dd5 0.683 -0.290 -0.044 0.151 -0.127 -0.124 -0.296 ## mean.ffp 0.711 -0.326 -0.062 0.162 -0.144 -0.156 -0.327 ## mean.gsp -0.579 0.491 0.153 -0.102 0.339 0.403 0.517 ## mean.hli -0.029 0.453 0.151 -0.155 0.154 0.082 0.170 ## mean.pratio 0.744 -0.405 -0.120 0.057 -0.210 -0.222 -0.377 ## mean.rough27 -0.186 0.905 0.330 -0.109 0.504 0.485 0.587 ## mean.srr 0.043 0.031 0.555 -0.458 -0.103 -0.055 -0.073 ## max.cti 0.176 0.278 0.136 0.685 0.575 0.409 0.482 ## max.dd5 0.803 0.087 0.120 0.244 0.504 0.443 0.324 ## max.ffp 0.811 0.100 0.139 0.231 0.481 0.459 0.319 ## max.gsp -0.222 0.727 0.341 -0.040 0.669 0.717 0.804 ## max.hli 0.124 0.630 0.243 0.141 0.552 0.493 0.552 ## max.pratio 1.000 0.006 0.108 0.111 0.365 0.391 0.223 ## max.rough27 0.006 1.000 0.388 0.009 0.625 0.613 0.683 ## max.srr 0.108 0.388 1.000 -0.139 0.346 0.338 0.368 ## var.cti 0.111 0.009 -0.139 1.000 0.170 0.077 0.077 ## var.dd5 0.365 0.625 0.346 0.170 1.000 0.928 0.964 ## var.ffp 0.391 0.613 0.338 0.077 0.928 1.000 0.955 ## var.gsp 0.223 0.683 0.368 0.077 0.964 0.955 1.000 ## var.hli 0.065 0.604 0.219 0.160 0.455 0.364 0.440 ## var.pratio 0.385 0.472 0.234 0.056 0.777 0.888 0.828 ## var.rough27 0.028 0.898 0.304 0.124 0.639 0.594 0.669 ## var.srr 0.058 0.084 0.232 0.315 0.285 0.282 0.278 ## median.cti 0.087 -0.259 -0.275 0.404 0.083 0.085 0.044 ## median.dd5 0.626 -0.252 -0.033 0.141 -0.110 -0.112 -0.263 ## median.ffp 0.656 -0.306 -0.056 0.153 -0.136 -0.156 -0.302 ## median.gsp -0.624 0.404 0.091 -0.089 0.234 0.288 0.404 ## median.hli 0.015 0.259 0.127 -0.219 0.057 0.013 0.061 ## median.pratio 0.716 -0.371 -0.098 0.050 -0.178 -0.191 -0.335 ## median.rough27 -0.260 0.644 0.263 -0.189 0.357 0.334 0.454 ## median.srr 0.074 0.000 0.494 -0.388 -0.099 -0.036 -0.072 ## wet.pct.nlcd -0.024 -0.237 -0.282 0.704 -0.164 -0.194 -0.226 ## var.hli var.pratio var.rough27 var.srr median.cti median.dd5 ## min.cti -0.499 -0.488 -0.636 -0.031 0.257 0.136 ## min.dd5 -0.312 -0.572 -0.500 -0.225 0.015 0.590 ## min.ffp -0.346 -0.616 -0.569 -0.240 0.019 0.609 ## min.gsp -0.129 -0.308 -0.040 0.028 0.025 -0.622 ## min.hli -0.176 -0.552 -0.305 -0.236 -0.157 0.373 ## min.pratio -0.409 -0.610 -0.641 -0.199 0.053 0.574 ## min.rough27 0.013 -0.413 -0.081 -0.559 -0.494 -0.114 ## min.srr -0.058 -0.450 -0.200 -0.718 -0.413 0.087 ## mean.cti -0.155 0.011 -0.149 0.353 0.868 0.134 ## mean.dd5 -0.111 -0.159 -0.249 -0.115 0.031 0.964 ## mean.ffp -0.109 -0.184 -0.278 -0.126 0.046 0.935 ## mean.gsp 0.166 0.354 0.424 0.165 0.012 -0.748 ## mean.hli 0.614 0.044 0.454 -0.394 -0.549 -0.052 ## mean.pratio -0.208 -0.232 -0.366 -0.097 0.082 0.812 ## mean.rough27 0.565 0.379 0.802 -0.023 -0.401 -0.395 ## mean.srr -0.003 -0.037 -0.064 -0.281 -0.474 0.062 ## max.cti 0.406 0.314 0.407 0.301 0.297 -0.040 ## max.dd5 0.258 0.368 0.167 0.078 0.104 0.611 ## max.ffp 0.264 0.377 0.175 0.087 0.077 0.634 ## max.gsp 0.348 0.592 0.644 0.198 -0.042 -0.520 ## max.hli 0.930 0.365 0.790 -0.059 -0.186 -0.090 ## max.pratio 0.065 0.385 0.028 0.058 0.087 0.626 ## max.rough27 0.604 0.472 0.898 0.084 -0.259 -0.252 ## max.srr 0.219 0.234 0.304 0.232 -0.275 -0.033 ## var.cti 0.160 0.056 0.124 0.315 0.404 0.141 ## var.dd5 0.455 0.777 0.639 0.285 0.083 -0.110 ## var.ffp 0.364 0.888 0.594 0.282 0.085 -0.112 ## var.gsp 0.440 0.828 0.669 0.278 0.044 -0.263 ## var.hli 1.000 0.268 0.786 -0.110 -0.250 -0.103 ## var.pratio 0.268 1.000 0.455 0.269 0.093 -0.156 ## var.rough27 0.786 0.455 1.000 0.074 -0.173 -0.217 ## var.srr -0.110 0.269 0.074 1.000 0.342 -0.087 ## median.cti -0.250 0.093 -0.173 0.342 1.000 0.036 ## median.dd5 -0.103 -0.156 -0.217 -0.087 0.036 1.000 ## median.ffp -0.115 -0.191 -0.261 -0.098 0.049 0.955 ## median.gsp 0.136 0.271 0.349 0.158 0.019 -0.797 ## median.hli 0.306 -0.021 0.240 -0.310 -0.536 0.137 ## median.pratio -0.196 -0.215 -0.336 -0.077 0.084 0.839 ## median.rough27 0.468 0.299 0.578 -0.115 -0.473 -0.444 ## median.srr -0.051 0.003 -0.099 -0.142 -0.395 0.066 ## wet.pct.nlcd -0.124 -0.128 -0.130 0.257 0.391 0.111 ## median.ffp median.gsp median.hli median.pratio median.rough27 ## min.cti 0.205 -0.295 -0.297 0.236 -0.474 ## min.dd5 0.641 -0.658 -0.020 0.650 -0.488 ## min.ffp 0.669 -0.714 -0.027 0.655 -0.525 ## min.gsp -0.659 0.667 -0.085 -0.639 0.193 ## min.hli 0.388 -0.405 0.285 0.446 -0.280 ## min.pratio 0.641 -0.731 -0.111 0.754 -0.600 ## min.rough27 -0.096 -0.011 0.363 -0.019 0.266 ## min.srr 0.104 -0.192 0.260 0.121 0.044 ## mean.cti 0.155 -0.083 -0.497 0.126 -0.489 ## mean.dd5 0.931 -0.805 0.066 0.836 -0.474 ## mean.ffp 0.951 -0.870 0.064 0.872 -0.515 ## mean.gsp -0.835 0.960 -0.022 -0.852 0.549 ## mean.hli -0.091 0.076 0.741 -0.139 0.619 ## mean.pratio 0.868 -0.886 -0.005 0.980 -0.573 ## mean.rough27 -0.451 0.507 0.367 -0.526 0.858 ## mean.srr 0.069 -0.147 0.197 0.047 0.191 ## max.cti -0.041 0.069 -0.096 -0.076 0.048 ## max.dd5 0.606 -0.508 0.116 0.502 -0.160 ## max.ffp 0.618 -0.508 0.119 0.500 -0.150 ## max.gsp -0.599 0.727 0.072 -0.629 0.591 ## max.hli -0.112 0.176 0.289 -0.197 0.430 ## max.pratio 0.656 -0.624 0.015 0.716 -0.260 ## max.rough27 -0.306 0.404 0.259 -0.371 0.644 ## max.srr -0.056 0.091 0.127 -0.098 0.263 ## var.cti 0.153 -0.089 -0.219 0.050 -0.189 ## var.dd5 -0.136 0.234 0.057 -0.178 0.357 ## var.ffp -0.156 0.288 0.013 -0.191 0.334 ## var.gsp -0.302 0.404 0.061 -0.335 0.454 ## var.hli -0.115 0.136 0.306 -0.196 0.468 ## var.pratio -0.191 0.271 -0.021 -0.215 0.299 ## var.rough27 -0.261 0.349 0.240 -0.336 0.578 ## var.srr -0.098 0.158 -0.310 -0.077 -0.115 ## median.cti 0.049 0.019 -0.536 0.084 -0.473 ## median.dd5 0.955 -0.797 0.137 0.839 -0.444 ## median.ffp 1.000 -0.884 0.084 0.895 -0.503 ## median.gsp -0.884 1.000 -0.047 -0.897 0.501 ## median.hli 0.084 -0.047 1.000 0.016 0.436 ## median.pratio 0.895 -0.897 0.016 1.000 -0.556 ## median.rough27 -0.503 0.501 0.436 -0.556 1.000 ## median.srr 0.083 -0.164 0.126 0.059 0.143 ## wet.pct.nlcd 0.125 -0.116 -0.316 0.043 -0.334 ## median.srr wet.pct.nlcd ## min.cti 0.099 0.174 ## min.dd5 0.138 0.159 ## min.ffp 0.160 0.202 ## min.gsp -0.183 0.021 ## min.hli -0.006 0.011 ## min.pratio 0.105 0.145 ## min.rough27 0.233 -0.395 ## min.srr 0.413 -0.321 ## mean.cti -0.455 0.648 ## mean.dd5 0.079 0.127 ## mean.ffp 0.100 0.141 ## mean.gsp -0.130 -0.156 ## mean.hli 0.067 -0.346 ## mean.pratio 0.054 0.060 ## mean.rough27 0.078 -0.311 ## mean.srr 0.946 -0.431 ## max.cti -0.304 0.309 ## max.dd5 -0.016 0.029 ## max.ffp 0.027 0.011 ## max.gsp -0.084 -0.223 ## max.hli -0.075 -0.155 ## max.pratio 0.074 -0.024 ## max.rough27 0.000 -0.237 ## max.srr 0.494 -0.282 ## var.cti -0.388 0.704 ## var.dd5 -0.099 -0.164 ## var.ffp -0.036 -0.194 ## var.gsp -0.072 -0.226 ## var.hli -0.051 -0.124 ## var.pratio 0.003 -0.128 ## var.rough27 -0.099 -0.130 ## var.srr -0.142 0.257 ## median.cti -0.395 0.391 ## median.dd5 0.066 0.111 ## median.ffp 0.083 0.125 ## median.gsp -0.164 -0.116 ## median.hli 0.126 -0.316 ## median.pratio 0.059 0.043 ## median.rough27 0.143 -0.334 ## median.srr 1.000 -0.363 ## wet.pct.nlcd -0.363 1.000 c. Write out resultss Figure as pdf file: pdf(file=paste0(here::here(),&quot;/output/node.cor.pdf&quot;), width=20, height=20) pairs(nodecor.ln, pch=19, cex=0.50) dev.off() ## png ## 2 Correlation data: write.csv(round(edgecor.ln, 4), file = paste0(here::here(),&quot;/output/EdgeCorrelationsLn.csv&quot;)) write.csv(round(nodecor.ln, 4), file = paste0(here::here(),&quot;/output/NodeCorrelationsLn.csv&quot;)) e. Compare models Run and compare models representing your hypotheses. Remember - we compete models using ML but use REML for final fit. Also remember that the null model contains distance (and all models must contain distance). Null model: ( null &lt;- GeNetIt::gravity(y = &quot;GDIST&quot;, x = c(&quot;length&quot;), d = &quot;length&quot;, group = &quot;from_ID&quot;, data = gdata, method = &quot;ML&quot;) ) ## [1] &quot;Running singly-constrained gravity model&quot; ## Warning: invalid formula &quot;random = ~1 | from_ID&quot;: assignment is deprecated ## Gravity model ## ## Linear mixed-effects model fit by maximum likelihood ## Data: gdata ## AIC BIC logLik ## -913.6676 -894.8696 460.8338 ## ## Random effects: ## Formula: ~1 | from_ID ## (Intercept) Residual ## StdDev: 0.07766627 0.1314787 ## ## Fixed effects: list(fmla) ## Value Std.Error DF t-value p-value ## (Intercept) 0.7820529 0.05054865 782 15.47129 0 ## length -0.1524624 0.00594561 782 -25.64286 0 ## Correlation: ## (Intr) ## length -0.954 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -7.19945126 -0.52191436 0.09864931 0.60400073 3.04975798 ## ## Number of Observations: 812 ## Number of Groups: 29 Global model (this was based on my hypotheses): ( global &lt;- GeNetIt::gravity(y = &quot;GDIST&quot;, x = c(&quot;length&quot;, &quot;wet.pct.nlcd&quot;, &quot;median.gsp&quot;, &quot;from.Depth&quot;, &quot;from.ffp&quot;, &quot;from.hli&quot;, &quot;from.pratio&quot;, &quot;from.degree&quot;, &quot;from.betweenness&quot;, &quot;from.pwetland&quot;, &quot;median.srr&quot;, &quot;median.rough27&quot;), d = &quot;length&quot;, group = &quot;from_ID&quot;, data = gdata, method = &quot;ML&quot;) ) ## [1] &quot;Running singly-constrained gravity model&quot; ## Warning: invalid formula &quot;random = ~1 | from_ID&quot;: assignment is deprecated ## Gravity model ## ## Linear mixed-effects model fit by maximum likelihood ## Data: gdata ## AIC BIC logLik ## -1007.525 -937.0325 518.7625 ## ## Random effects: ## Formula: ~1 | from_ID ## (Intercept) Residual ## StdDev: 0.06165765 0.1230698 ## ## Fixed effects: list(fmla) ## Value Std.Error DF t-value p-value ## (Intercept) 10.531419 3.128409 778 3.366382 0.0008 ## length -0.131565 0.006892 778 -19.090228 0.0000 ## wet.pct.nlcd 0.002516 0.002910 778 0.864492 0.3876 ## median.gsp -1.873423 0.248176 778 -7.548755 0.0000 ## from.Depth 0.002233 0.012909 21 0.172981 0.8643 ## from.ffp 0.067345 0.083928 21 0.802407 0.4313 ## from.hli -1.480776 3.332751 21 -0.444310 0.6614 ## from.pratio -0.060628 0.040288 21 -1.504873 0.1472 ## from.degree 0.049009 0.097357 21 0.503400 0.6199 ## from.betweenness -0.008488 0.015550 21 -0.545856 0.5909 ## from.pwetland -0.004191 0.007459 21 -0.561924 0.5801 ## median.srr 0.124025 0.045645 778 2.717134 0.0067 ## median.rough27 -0.018661 0.010294 778 -1.812790 0.0702 ## Correlation: ## (Intr) length wt.pc. mdn.gs frm.Dp frm.ff frm.hl frm.pr frm.dg ## length -0.040 ## wet.pct.nlcd -0.040 0.467 ## median.gsp -0.439 0.003 0.027 ## from.Depth 0.211 -0.004 -0.006 -0.055 ## from.ffp -0.818 0.019 0.021 0.076 -0.284 ## from.hli 0.889 -0.030 -0.026 0.008 0.192 -0.865 ## from.pratio -0.068 0.006 -0.026 0.055 -0.149 -0.074 0.048 ## from.degree -0.059 0.046 0.046 0.054 -0.020 0.070 -0.011 -0.116 ## from.betweenness 0.202 -0.048 -0.037 -0.066 0.119 -0.198 0.165 0.019 -0.875 ## from.pwetland -0.041 -0.004 0.037 0.038 -0.200 -0.004 -0.014 0.170 0.106 ## median.srr -0.025 0.052 -0.062 0.131 0.032 -0.026 0.022 0.003 0.022 ## median.rough27 0.265 -0.319 -0.074 -0.542 0.012 -0.045 0.035 -0.062 -0.062 ## frm.bt frm.pw mdn.sr ## length ## wet.pct.nlcd ## median.gsp ## from.Depth ## from.ffp ## from.hli ## from.pratio ## from.degree ## from.betweenness ## from.pwetland -0.155 ## median.srr -0.032 -0.045 ## median.rough27 0.058 -0.019 -0.185 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -7.5986170 -0.5245075 0.0551797 0.5560370 3.9428671 ## ## Number of Observations: 812 ## Number of Groups: 29 Published model: ( published &lt;- GeNetIt::gravity(y = &quot;GDIST&quot;, x = c(&quot;length&quot;, &quot;median.gsp&quot;, &quot;from.Depth&quot;, &quot;from.hli&quot;, &quot;median.cti&quot;, &quot;median.srr&quot;), d = &quot;length&quot;, group = &quot;from_ID&quot;, data = gdata, method = &quot;ML&quot;)) ## [1] &quot;Running singly-constrained gravity model&quot; ## Warning: invalid formula &quot;random = ~1 | from_ID&quot;: assignment is deprecated ## Gravity model ## ## Linear mixed-effects model fit by maximum likelihood ## Data: gdata ## AIC BIC logLik ## -1012.517 -970.2212 515.2583 ## ## Random effects: ## Formula: ~1 | from_ID ## (Intercept) Residual ## StdDev: 0.06480527 0.1234197 ## ## Fixed effects: list(fmla) ## Value Std.Error DF t-value p-value ## (Intercept) 13.349985 1.6618555 779 8.033180 0.0000 ## length -0.137964 0.0057632 779 -23.938869 0.0000 ## median.gsp -2.112507 0.2093307 779 -10.091719 0.0000 ## from.Depth 0.001318 0.0124098 26 0.106217 0.9162 ## from.hli 0.790610 1.7086186 26 0.462719 0.6474 ## median.cti 0.001195 0.0629880 779 0.018973 0.9849 ## median.srr 0.109397 0.0488379 779 2.240004 0.0254 ## Correlation: ## (Intr) length mdn.gs frm.Dp frm.hl mdn.ct ## length 0.112 ## median.gsp -0.575 -0.227 ## from.Depth -0.104 -0.002 -0.032 ## from.hli 0.709 -0.025 0.166 -0.143 ## median.cti -0.142 0.037 0.115 -0.017 -0.010 ## median.srr -0.064 0.042 0.081 0.011 -0.006 0.408 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -7.5866483 -0.5091002 0.0666032 0.5547811 3.9918695 ## ## Number of Observations: 812 ## Number of Groups: 29 Habitat hypothesis ( habitat &lt;- GeNetIt::gravity(y = &quot;GDIST&quot;, x = c(&quot;length&quot;, &quot;wet.pct.nlcd&quot;, &quot;median.gsp&quot;), d = &quot;length&quot;, group = &quot;from_ID&quot;, data = gdata, method = &quot;ML&quot;) ) ## [1] &quot;Running singly-constrained gravity model&quot; ## Warning: invalid formula &quot;random = ~1 | from_ID&quot;: assignment is deprecated ## Gravity model ## ## Linear mixed-effects model fit by maximum likelihood ## Data: gdata ## AIC BIC logLik ## -1013.066 -984.8689 512.533 ## ## Random effects: ## Formula: ~1 | from_ID ## (Intercept) Residual ## StdDev: 0.06630675 0.1237554 ## ## Fixed effects: list(fmla) ## Value Std.Error DF t-value p-value ## (Intercept) 12.820217 1.1520073 780 11.128590 0.0000 ## length -0.135641 0.0065255 780 -20.786371 0.0000 ## wet.pct.nlcd 0.002538 0.0028896 780 0.878297 0.3801 ## median.gsp -2.149016 0.2052203 780 -10.471750 0.0000 ## Correlation: ## (Intr) length wt.pc. ## length 0.171 ## wet.pct.nlcd -0.005 0.470 ## median.gsp -0.999 -0.212 -0.011 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -7.46729705 -0.49570850 0.07437762 0.55168658 3.94762511 ## ## Number of Observations: 812 ## Number of Groups: 29 Compare models: these are the names of the hypotheses (models) I tested. #compare.models(null, depth, product, climate, wetlands, topo, habitat, global) #compare.models(depth, product, climate, wetlands, topo, habitat, published, global, null) GeNetIt::compare.models(null, habitat, global, published) #NOTE - global will need to be edited to match your paramters ## model AIC BIC log.likelihood RMSE nparms deltaAIC ## 1 null -913.6676 -894.8696 460.8338 0.1415 0 99.3983240 ## 2 habitat -1013.0659 -984.8689 512.5330 0.1314 3 0.0000000 ## 3 global -1007.5251 -937.0325 518.7625 0.1297 12 5.5408783 ## 4 published -1012.5167 -970.2212 515.2583 0.1307 6 0.5492757 ## deltaBIC ## 1 89.99932 ## 2 0.00000 ## 3 47.83638 ## 4 14.64778 f. Diagnostic plots par(mfrow=c(2,3)) for (i in 1:6) { plot(global, type=i) } ## Warning in bxp(list(stats = structure(c(-3.01594610828498, -1.23135461837275, : ## some notches went outside hinges (&#39;box&#39;): maybe set notch=FALSE g. Fit final model(s) Habitat: habitat_fit &lt;- GeNetIt::gravity(y = &quot;GDIST&quot;, x = c(&quot;length&quot;, &quot;wet.pct.nlcd&quot;, &quot;median.gsp&quot;), d = &quot;length&quot;, group = &quot;from_ID&quot;, data = gdata, method = &quot;REML&quot;) ## [1] &quot;Running singly-constrained gravity model&quot; ## Warning: invalid formula &quot;random = ~1 | from_ID&quot;: assignment is deprecated Global: global_fit &lt;- GeNetIt::gravity(y = &quot;GDIST&quot;, x = c(&quot;length&quot;, &quot;wet.pct.nlcd&quot;, &quot;median.gsp&quot;, &quot;from.Depth&quot;, &quot;from.ffp&quot;, &quot;from.hli&quot;, &quot;from.pratio&quot;, &quot;from.degree&quot;, &quot;from.betweenness&quot;, &quot;from.pwetland&quot;, &quot;median.srr&quot;, &quot;median.rough27&quot;), d = &quot;length&quot;, group = &quot;from_ID&quot;, data = gdata, method = &quot;REML&quot;) ## [1] &quot;Running singly-constrained gravity model&quot; ## Warning: invalid formula &quot;random = ~1 | from_ID&quot;: assignment is deprecated Published: published_fit &lt;- GeNetIt::gravity(y = &quot;GDIST&quot;, x = c(&quot;length&quot;, &quot;median.gsp&quot;, &quot;from.Depth&quot;, &quot;from.hli&quot;, &quot;median.cti&quot;, &quot;median.srr&quot;), d = &quot;length&quot;, group = &quot;from_ID&quot;, data = gdata, method = &quot;REML&quot;) ## [1] &quot;Running singly-constrained gravity model&quot; ## Warning: invalid formula &quot;random = ~1 | from_ID&quot;: assignment is deprecated Compare models GeNetIt::compare.models(global_fit, habitat_fit, published_fit) ## Warning in GeNetIt::compare.models(global_fit, habitat_fit, published_fit): AIC/ ## BIC not valid under REML and will not be reported ## model log.likelihood RMSE nparms ## 1 global_fit 484.6774 0.1296 12 ## 2 habitat_fit 499.2604 0.1314 3 ## 3 published_fit 500.8846 0.1307 6 h. Effect size This effect size is NOT backwards transformed. We are working on effect sizes for backwards transformed data. GeNetIt::gravity.es(habitat_fit) ## t.value df cohen.d p.value low.ci up.ci ## length -20.7995890 780 -1.4894895 0.000000 -1.60183164 -1.377147 ## wet.pct.nlcd 0.8787948 780 0.0629318 0.379783 -0.03648035 0.162344 ## median.gsp -10.4619997 780 -0.7491994 0.000000 -0.85201875 -0.646380 GeNetIt::gravity.es(global_fit) ## t.value df cohen.d p.value low.ci up.ci ## length -19.1893863 778 -1.37594566 0.000000 -1.48662548 -1.26526585 ## wet.pct.nlcd 0.8679313 778 0.06223369 0.385700 -0.03730598 0.16177336 ## median.gsp -7.5059572 778 -0.53820321 0.000000 -0.63950664 -0.43689977 ## from.Depth 0.1445143 21 0.06307123 0.886472 -0.57621575 0.70235821 ## from.ffp 0.6926126 21 0.30228094 0.496141 -0.34066022 0.94522210 ## from.hli -0.3825715 21 -0.16696791 0.705882 -0.80725628 0.47332045 ## from.pratio -1.2852015 21 -0.56090791 0.212716 -1.21309035 0.09127454 ## from.degree 0.4333201 21 0.18911641 0.669201 -0.45150205 0.82973488 ## from.betweenness -0.4691536 21 -0.20475543 0.643794 -0.84563159 0.43612074 ## from.pwetland -0.4765597 21 -0.20798770 0.638597 -0.84891966 0.43294427 ## median.srr 2.6715256 778 0.19155767 0.007709 0.09181385 0.29130148 ## median.rough27 -1.8697707 778 -0.13406905 0.061891 -0.23369648 -0.03444162 GeNetIt::gravity.es(published_fit) ## t.value df cohen.d p.value low.ci up.ci ## length -23.98326001 779 -1.718578929 0.000000 -1.83496961 -1.6021882 ## median.gsp -10.08366298 779 -0.722569439 0.000000 -0.82521894 -0.6199199 ## from.Depth 0.10013905 26 0.039277765 0.921002 -0.52888551 0.6074410 ## from.hli 0.44108308 26 0.173007017 0.662799 -0.39620349 0.7422175 ## median.cti 0.03131321 779 0.002243825 0.975028 -0.09720768 0.1016953 ## median.srr 2.22992302 779 0.159790567 0.026037 0.06018031 0.2594008 Question: What did you learn? About Columbia spotted frogs in central Idaho? About linking genetic and landscape data with graphs and gravity models? About R programming? "],["Week14.html", "Week 14: Contemporary Gene Flow ", " Week 14: Contemporary Gene Flow "],["WE-14.html", "Worked Example", " Worked Example 1. Overview a. Background and Goals In this lab, you will use the gstudio package to analyse parent-offspring data for the wildflower Pulsatilla vulgaris. You will learn: how to conduct landscape genetic analyses on pollen pools, how to conduct paternity analysis to quantify contemporary pollen flow, and how to relate realized pollen flow events to landscape variables. The data are from DiLeo et al. (in press). The data include genotypes from seeds, mothers, and all potential fathers from seven focal populations. The study area in the Franconian Jura in Germany is the same as for the Dianthus carthusianorum data from Week 7 (see introductory video by Yessica Rico for the Dianthus data set to learn more about the system). Here, we will recreate one analysis from DiLeo et al. (in press), looking at the ecological factors that best explain pollen immigration rates of the seven populations. b. Data set There are three data files associated with this lab: pulsatilla_genotypes.csv, which contains the genotypes of offspring, mothers, and potential fathers from seven populations, pulsatilla_momVariables.csv, which contains ecological variables collect for each mother, and pulsatilla_population.csv, which contains landscape and population size information for each of the seven populations. c. Required packages Install some packages needed for this worked example. if(!requireNamespace(&quot;popgraph&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/popgraph&quot;) if(!requireNamespace(&quot;gstudio&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/gstudio&quot;) library(LandGenCourse) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) library(ggmap) ## Google&#39;s Terms of Service: https://cloud.google.com/maps-platform/terms/. ## Please cite ggmap if you use it! See citation(&quot;ggmap&quot;) for details. #library(gstudio) #library(pegas) #library(vegan) #library(purrr) #library(MuMIn) #library(lme4) #library(cowplot) 2. Import Genotypes The package gstudio makes it very easy to import genotype data (see Week 1). Typically, we store our genotype data in a spreadsheet with one line per individual and two columns per locus. Using the read_population function, we tell gstudio which columns contain our loci. The argument type=\"column\" indicates that our raw text file stores genotypes in two columns per locus. a. Import genotype data dat &lt;- gstudio::read_population(system.file(&quot;extdata&quot;, &quot;pulsatilla_genotypes.csv&quot;, package=&quot;LandGenCourse&quot;), type = &quot;column&quot;, locus.columns = 6:19) dat$ID &lt;- as.factor(dat$ID) Lets take a look at the data: head(dat) ## ID OffID Population X Y loc1_a loc2_a loc3_a loc4_a loc5_a ## 1 62 0 A21 4426941 5427173 340:340 422:422 413:425 446:448 121:121 ## 2 64 0 A21 4426933 5427178 334:334 424:424 417:434 444:449 122:133 ## 3 65 0 A21 4426936 5427173 338:340 417:422 417:419 446:452 135:137 ## 4 66 0 A21 4426937 5427174 340:344 422:422 411:431 446:448 122:126 ## 5 68 0 A21 4426934 5427171 336:342 417:422 423:438 448:448 119:123 ## 6 69 0 A21 4426933 5427166 336:346 422:422 417:421 444:452 122:124 ## loc6_a loc7_a ## 1 155:157 242:244 ## 2 155:161 242:252 ## 3 153:157 244:246 ## 4 157:159 242:252 ## 5 155:155 244:256 ## 6 155:161 256:256 There are 12 columns: ID: A unique identifier for every adult. OffID: Zero if the sample is an adult and &gt; 0 if the sample is an offspring. Population: The population of origin. 2 coordinate columns, X &amp; Y: The geographic coordinates of the sample (Euclidean coordinate system Gauss-Krueger GK4 with CRS(+init=epsg:31468)). 7 locus columns: For each locus, the microsatellite genotype is stored in a single column with alleles separated by : b. Plot map of sampling locations To create a map with qmplot (see Week 4), here we do the following: Use functions from dplyr to get mean coordinates per population Convert the coordinates to lat-lon coordinates with sf_project from the sf package. We use the EPSG reference numbers (http://epsg.io/) to specify the from and to coordinate systems. Download a terrain map from stamen, with zoom level 12. Add population labels with geom_text. We specify an offset along the x-axis,nudge_x, and for sites A25 and A26 also an offset along the y-axis to prevent labels from overlapping. Coords &lt;- dat %&gt;% group_by(Population) %&gt;% summarize(X = mean(X), Y = mean(Y)) Coords &lt;- data.frame(Coords, sf::sf_project(from = &quot;+init=epsg:31468&quot;, to = &quot;+init=epsg:4326&quot;, pts = Coords[c(&quot;X&quot;, &quot;Y&quot;)])) names(Coords)[4:5] &lt;- c(&quot;Longitude&quot;, &quot;Latitude&quot;) bbox &lt;- make_bbox(lon = Longitude, lat = Latitude, data = Coords, f=0.2) MyMap &lt;- ggmap(get_stamenmap(bbox, maptype=&quot;terrain&quot;, zoom=12, force=TRUE)) ## Source : http://tile.stamen.com/terrain/12/2172/1405.png ## Source : http://tile.stamen.com/terrain/12/2173/1405.png ## Source : http://tile.stamen.com/terrain/12/2174/1405.png ## Source : http://tile.stamen.com/terrain/12/2172/1406.png ## Source : http://tile.stamen.com/terrain/12/2173/1406.png ## Source : http://tile.stamen.com/terrain/12/2174/1406.png ## Source : http://tile.stamen.com/terrain/12/2172/1407.png ## Source : http://tile.stamen.com/terrain/12/2173/1407.png ## Source : http://tile.stamen.com/terrain/12/2174/1407.png ## Source : http://tile.stamen.com/terrain/12/2172/1408.png ## Source : http://tile.stamen.com/terrain/12/2173/1408.png ## Source : http://tile.stamen.com/terrain/12/2174/1408.png MyMap + geom_text(data = Coords, mapping = aes(Longitude, Latitude, label = Population), size = 4, col = &quot;black&quot;, hjust = 0, nudge_x = 0.005, nudge_y = c(0,0,0.002,-0.001,0,0,0)) 3. Pollen pool genetic structure a. Subtract maternal contribution to offspring genotype Lets look at the data for the mother with the ID=3083 and her associated offspring: dat[dat$ID == &quot;3083&quot;,] ## ID OffID Population X Y loc1_a loc2_a loc3_a loc4_a ## 207 3083 0 A25 4422659 5425370 338:340 417:424 392:392 452:452 ## 208 3083 1 A25 4422659 5425370 338:340 417:424 392:409 446:452 ## 209 3083 3 A25 4422659 5425370 338:338 417:424 452:452 ## 210 3083 4 A25 4422659 5425370 340:340 417:422 392:411 448:452 ## 211 3083 5 A25 4422659 5425370 340:340 417:424 392:392 452:452 ## 212 3083 6 A25 4422659 5425370 338:338 422:424 392:411 452:452 ## 213 3083 7 A25 4422659 5425370 338:340 424:424 392:392 452:452 ## 214 3083 8 A25 4422659 5425370 340:340 417:417 392:417 452:452 ## loc5_a loc6_a loc7_a ## 207 121:121 157:157 244:252 ## 208 121:121 157:157 240:244 ## 209 121:121 157:157 ## 210 121:121 155:157 236:252 ## 211 121:121 157:157 244:244 ## 212 121:121 157:159 242:244 ## 213 121:121 157:157 244:244 ## 214 121:121 155:157 242:244 The first row is the mother and the subsequent rows are her offspring with OffIDs 1-8. Now, use the minus_mom function to remove the mothers contribution to the offspring genotypes. You will be left with just the paternal contribution. The allele frequecies of all paternal contributions associated with a single mother is called a pollen pool. In some cases, the pollen contribution is ambiguous when the offspring has both alleles in common with the mother (e.g. see OffID 1, loc1_a and loc2_a). In such cases, both alleles are retained. pollen &lt;- gstudio::minus_mom(dat, MomCol = &quot;ID&quot;, OffCol = &quot;OffID&quot;) pollen[pollen$ID == &quot;3083&quot;,] ## ID OffID Population X Y loc1_a loc2_a loc3_a loc4_a loc5_a ## 208 3083 1 A25 4422659 5425370 338:340 417:424 409 446 121 ## 209 3083 3 A25 4422659 5425370 338 417:424 452 121 ## 210 3083 4 A25 4422659 5425370 340 422 411 448 121 ## 211 3083 5 A25 4422659 5425370 340 417:424 392 452 121 ## 212 3083 6 A25 4422659 5425370 338 422 411 452 121 ## 213 3083 7 A25 4422659 5425370 338:340 424 392 452 121 ## 214 3083 8 A25 4422659 5425370 340 417 417 452 121 ## loc6_a loc7_a ## 208 157 240 ## 209 157 ## 210 155 236 ## 211 157 244 ## 212 159 242 ## 213 157 244 ## 214 155 242 The pollen pool data can be analysed as any other population genetic data. Note: There are some missing genotypes for the offspring that trigger warnings, which you may ignore. Here we suppressed the warnings to reduce output. b. TwoGener analysis (AMOVA of pollen pool data) For example, we can conduct an AMOVA (see Week 4) to test if pollen pools belonging to different mothers are signficantly differentiated from one another. When an AMOVA is applied to pollen pools it is called a TwoGener analysis (Two-generational analysis; Smouse et al. 2001, Dyer et al. 2004). Here we use the amova function from the pegas package to perform a hierarchical analysis of variance on genetic distance matrices (AMOVA), with moms nested within patches (this may take a while). D &lt;- gstudio::genetic_distance(pollen,mode=&quot;amova&quot;) D &lt;- as.dist(D) Moms &lt;- pollen$ID Populations &lt;- as.factor(pollen$Population) amova.result &lt;- pegas::amova(D ~ Populations/Moms, nperm=500) ## Registered S3 method overwritten by &#39;spdep&#39;: ## method from ## plot.mst ape ## Registered S3 method overwritten by &#39;pegas&#39;: ## method from ## print.amova ade4 amova.result ## ## Analysis of Molecular Variance ## ## Call: pegas::amova(formula = D ~ Populations/Moms, nperm = 500) ## ## SSD MSD df ## Populations 664.3079 110.717976 6 ## Moms 1252.7296 36.844988 34 ## Error 1895.1503 6.916607 274 ## Total 3812.1878 12.140725 314 ## ## Variance components: ## sigma2 P.value ## Populations 1.6747 0 ## Moms 3.9278 0 ## Error 6.9166 ## ## Phi-statistics: ## Populations.in.GLOBAL (Phi_CT) Moms.in.GLOBAL (Phi_ST) ## 0.1337729 0.4475164 ## Moms.in.Populations (Phi_SC) ## 0.3621954 ## ## Variance coefficients: ## a b c ## 7.619648 7.920406 43.405291 Calculate phi from the variance components: phi &lt;- amova.result$varcomp[1]/sum(amova.result$varcomp[1]) names(phi) &lt;- &quot;phi&quot; phi ## phi ## Populations 0.1337729 ## Moms 0.3137434 ## Error 0.5524836 Looking at the variance components and the phi-statistics (analogue to F-statistics), what can be said about differentiation among pollen pools? Questions: Are pollen pools more differentiated among populations or among mothers within populations? What does this mean biologically? c. Including space: Isolation by distance The TwoGener analysis gives a summary statistic describing overall differentiation, but this does not tell us anything about why the pollen pools are differentiated. In order to better understand why, we can analyse the data spatially just like any other landscape genetic dataset. For example, we can test a basic hypothesis of isolation by distance by asking: are pollen pools from mothers that are geographically close more similar than those that are far? We test this by calculating pairwise genetic similarity (proportion of shared alleles) and relate this to geographic distance. For a more sophistocated analysis, the data could also be analysed as a pollination graph (Dyer et al. 2012), which is similar to population graphs that will be included in the Bonus Material for Week 13, but we will not do this here. First, calculate matrix of pairwise genetic similarity among pollen pools. dps &lt;- 1 - gstudio::genetic_distance(pollen, stratum = &quot;ID&quot;, mode = &quot;Dps&quot;) ## Bray distance will be assumed to be entirely additive across loci. Note: the proportion of shared alleles is a measure of genetic similarity, hence we expect it to decrease with geographic distance. We convert this into a measure of genetic distance by subtracting it from 1 (see Week 5). Calculate matrix of pairwise geographic distance among pollen pools: xy &lt;- unique(data.frame(pollen$X, pollen$Y)) xy.dist &lt;- as.matrix(dist(xy)) Plot dps against geographic distance. par(mar=c(4,4,0,0)) plot(xy.dist[lower.tri(xy.dist)], dps[lower.tri(dps)], xlab = &quot;Geographic distance (m)&quot;, ylab = &quot;1 - Proportion of shared alleles (dps)&quot;) abline(lm(dps[lower.tri(dps)]~xy.dist[lower.tri(xy.dist)])) We see that pollen pools belonging to mothers that are spatially close are more similar. Lets quantify the strength of this relationship using a Mantel test. vegan::mantel(xy.dist, dps) ## ## Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## vegan::mantel(xdis = xy.dist, ydis = dps) ## ## Mantel statistic r: 0.4432 ## Significance: 0.001 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.111 0.152 0.187 0.214 ## Permutation: free ## Number of permutations: 999 Note: it seems that the mantel function in vegan automatically tests a one-sided alternative that the Mantel correlation is greater than zero. This means that if we had used Dps as a measure of genetic similarity, the default setting would have resulted in a p-value of 0.999. Unfortunately, for now, this is not discussed in the help file and there is no option to change the alternative. 4. Paternity analysis TwoGener and related methods are sensitive to inbreeding and strong genetic structure of adult populations, although methods exist to correct for some of these things (e.g. Dyer et al. 2004). However, it is often the best (or only) option for quantifying pollen flow for high density species where other analyses such as paternity assignment cannot be conducted. In our case, we have complete sampling of potential fathers within the seven populations (i.e., patches) from which mothers and their offspring were sampled. Therefore, we are able to conduct a paternity analysis, which will give us more detailed information about contemporary pollen flow. Note that we do not have complete sampling of all potential fathers within the larger study region, and so we assume that any seed that is not assigned to a father is a pollen immigration event. In this section, we will conduct a paternity analysis using the gstudio package and relate the results to landscape variables. a. Exclusion probabilities First lets check if our genetic markers have sufficient information (genetic resolution) to discriminate among alternative fathers by calculating exclusion probabilites. ## e.clusion probabilities pollen.freqs &lt;- gstudio::frequencies(dat) p.excl &lt;- gstudio::exclusion_probability( pollen.freqs ) p.excl ## Locus Pexcl PexclMax Fraction ## 1 loc1_a 0.6817515 0.8926727 0.7637194 ## 2 loc2_a 0.3359125 0.7434082 0.4518547 ## 3 loc3_a 0.7614844 0.9218755 0.8260165 ## 4 loc4_a 0.5451972 0.7718336 0.7063663 ## 5 loc5_a 0.5984514 0.8981062 0.6663481 ## 6 loc6_a 0.5998659 0.8537849 0.7025960 ## 7 loc7_a 0.7002772 0.8424075 0.8312808 The second column, Pexcl, gives us the information we need. We can see that certain loci have poor ability to exclude alternative potential fathers (e.g. loc2) and others are much more informative (e.g. loc3). Luckily, we have multiple loci to use and the multilocus exclusion probability can be calculated by multiplying across loci. The code below does the following: 1 - unlist(p.excl$Pexcl): extract the elements of p.excl.Pexcl (the exclusion probabilities for all loci) as a vector and calculage for each locus, the complement of the exclusion probability. 1 - prod(): multiplies the probabilities, and take the complement to get the multilocus exculsion probability. 1- prod((1-unlist(p.excl$Pexcl))) ## [1] 0.9988959 We see that when using all seven loci, we achieve a very high exlusion probability, and that on average we should be able to exclude 99.9 of alternative fathers. b. Paternity analysis We will use the gstudio function paternity, which conducts a fractional-allocation paternity analyis. This approach is useful when full exclusion cannot be achieved. In case multiple potential fathers cannot be exluded for a particular offspring, a likelihood of paternity is calculated for each non-excluded father based on Mendelian transition probabilites. Note that in the original paper from which these data originate, we used a more sophisticated approach implemented in the program COLONY2. COLONY2 can account for genotyping errors, whereas the gstudio function paternity approach cannot and so we had to throw out some of our data here. Now lets conduct a paternity analysis for the seeds from mother 3083: # Select all offspring of mom 3083: offspring.3083 &lt;- subset(dat, OffID!=0 &amp; ID == &quot;3083&quot;) # Select mom 3083: mother.3083 &lt;- subset(dat, ID == &quot;3083&quot; &amp; dat$OffID == 0 ) # Select all potential fathers in the same population as mom 3083: fathers.3083 &lt;- subset(dat, OffID == 0 &amp; Population %in% offspring.3083$Population) # Paternity analysis: pat.3083 &lt;- gstudio::paternity(offspring.3083, mother.3083, fathers.3083 ) pat.3083 ## MomID OffID DadID Fij ## 1 3083 1 212 1.00000000 ## 2 3083 3 3083 0.55172414 ## 3 3083 3 218 0.13793103 ## 4 3083 3 230 0.13793103 ## 5 3083 3 3084 0.06896552 ## 6 3083 3 255 0.03448276 ## 7 3083 3 261 0.03448276 ## 8 3083 3 3081 0.03448276 ## 9 3083 5 3083 0.72727273 ## 10 3083 5 234 0.18181818 ## 11 3083 5 219 0.04545455 ## 12 3083 5 239 0.04545455 ## 13 3083 6 215 1.00000000 ## 14 3083 7 3083 0.88888889 ## 15 3083 7 219 0.05555556 ## 16 3083 7 239 0.05555556 ## 17 3083 8 212 1.00000000 The output shows the probability of paternity (Fij) for each offspring-father pair, which is calcualted from Mendelian inheritance transition probabilities. Some offspring have multiple potential fathers and so appear in the table more than once (e.g. Offsprings 3 and 5). A higher Fij value indcates that the Offspring-father pair is more likely. Now lets do the paternity analysis for the entire data set. First, we define a function get.parentage that does the paternity analysis for the offspring of one mom (i.e., one family). # make a dataframe just for the offspring: offspring &lt;- subset(dat, OffID!=0) ## h.re is the function that we will apply to all mothers: get.parentage &lt;- function(x){ tmp.offspring &lt;- subset(offspring, ID == x) tmp.mother &lt;- subset(dat, ID == x &amp; OffID == 0) tmp.fathers &lt;- subset(dat, OffID == 0 &amp; Population %in% tmp.offspring$Population) return(gstudio::paternity(tmp.offspring, tmp.mother, tmp.fathers )) } We could use lapply to apply get.parentage to each mom and her offspring. Here, we use an alternative function map from package purr (which is part of the new tidyverse set of packages by Hadley Wickham). This allows us to address the following issue: Because we have not sampled all potential fathers in the larger study region, paternity will return an error when all offspring of a certain mother cannot be assigned to any father. To get around this, we will use the possibly function from the library purrr. When an error occurs, it will simply return NA instead of stopping our function. The function map knows how to apply a purrr-ified function to an object. This will take about a minute to run: # purrr-ify the function so that NA is returned when an error pops up: possible_pat &lt;- purrr::possibly(get.parentage, otherwise = NA_real_) # run the function and store the output: # list of results for each mother: pat.all &lt;- purrr::map(unique(offspring$ID), possible_pat) ## c.nvert the list to a dataframe: pat.all &lt;- do.call(rbind, pat.all[!is.na(pat.all)]) For the purposes of this tutorial, we will keep only the highest-probability father for each offspring. With your own data, you should choose some sort of threshold (e.g. keep only those with probability &gt; 0.9) or you can conduct analyses using the probability as weights. ## c.eate a temporary ID that combines the MomID and the OffID pat.all$tmpID &lt;- paste(pat.all$MomID, pat.all$OffID, sep=&quot;_&quot;) ## g.t rid of all rows with duplicated tmpIDs, leaving just the first entry for each pat.sub &lt;- pat.all[!duplicated(pat.all$tmpID),] ## g.t rid of the tmpID column pat.sub &lt;- pat.sub[,1:4] ## g.t rid of the tmp_ID column c. Visualize reconstructed pollen flow events in space We can use the spiderplot_data function from gstudio to visualize the paternity assignments. spiderplot_data takes the output from paternity and combines it with spatial XY coordinate information. The output includes the coordinates of both mother (X, Y) and father (Xend, Yend) for each offspring. Here we will also add in the population IDs so that we can visualize each of the seven populations separately. Note that the function asks for longitude and latitude but any form of spatial XY coordinates will do. pat.sub &lt;- gstudio::spiderplot_data(pat.sub, dat, longitude = &quot;X&quot;, latitude = &quot;Y&quot;) # Join data to add population IDs pat.sub &lt;- merge(pat.sub, dat[, c(&quot;Population&quot;, &quot;ID&quot; ,&quot;OffID&quot;)], by.x=c(&quot;MomID&quot;, &quot;OffID&quot;), by.y=c(&quot;ID&quot;, &quot;OffID&quot;), all.x=T) head(pat.sub) ## MomID OffID DadID Fij X Xend Y Yend Population ## 1 3080 1 211 1 4422657 4422658 5425371 5425371 A25 ## 2 3080 10 211 1 4422657 4422658 5425371 5425371 A25 ## 3 3080 2 214 1 4422657 4422658 5425371 5425370 A25 ## 4 3080 3 214 1 4422657 4422658 5425371 5425370 A25 ## 5 3080 4 211 1 4422657 4422658 5425371 5425371 A25 ## 6 3080 5 211 1 4422657 4422658 5425371 5425371 A25 Use ggplot2 to visualize the reconstructed pollen flow events. Lets look at population A25 first. The arrows point to mothers, and arrows without lines indicate selfing events. The darkness of the lines is proportional to the number of pollen flow events between a particular mother and father. pop &lt;- &quot;A25&quot; ggplot() + geom_point(data=dat[dat$Population==pop,], aes(X,Y),size=3, color=&quot;red&quot;) + geom_segment(data=pat.sub[pat.sub$Population==pop,], aes(x=X, y=Y, xend=Xend, yend=Yend), size=0.5, alpha=0.2, arrow = arrow(ends = &quot;first&quot;, length = unit(0.3, &quot;cm&quot;))) + theme(legend.position = &quot;none&quot;) Try modifying the code to look at patterns from other populations. Questions: Do pollen flow events appear to be amongst near-neigbours? Are there clear differences in pollen flow patterns among populations? d. Visualize dispersal kernel We can calculate the distance of each pollen flow event, and then plot the distribution to visualize the dispersal kernel. Calculate distance for each event: pat.sub$pollen.dist &lt;- unlist(lapply(1:nrow(pat.sub), function(x) dist(rbind(c(pat.sub$X[x], pat.sub$Y[x]), c(pat.sub$Xend[x], pat.sub$Yend[x]))) )) Plot the distribution of pollination events that are greater than 0 m (i.e. excluding selfing): ggplot(pat.sub[pat.sub$pollen.dist &gt;0,]) + geom_histogram( aes(x=pollen.dist), bins=20) + xlab(&quot;Distance from pollen source (m)&quot;) + ylab(&quot;Number of pollen flow events&quot;) 5. Linking paternity to ecological variables Now that we have conducted our paternity analysis, we can ask which ecological factors explain the patterns that we see. First we will look within populations: what factors explain pollen flow distances within populations? Second, we will look at the population level: what factors explain pollen immigration rates? a. Explain pollen flow within populations We have collected some information about mothers, which we now add to the pat.sub dataframe. Specifically, we have measured: Mother isolation: how far a mom is from other conspecifics, and Local floral density: how many flowers are within 2 m of mother plants. We are only interested in outcrossed pollen events, so we make a new data frame that excludes selfing. # read in the data mom.vars &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;pulsatilla_momVariables.csv&quot;, package=&quot;LandGenCourse&quot;)) ## e.clude selfing pat.outcrossed &lt;- subset(pat.sub, MomID != DadID) ## a.d mom variables to pat.outcrossed pat.outcrossed &lt;- merge(pat.outcrossed, mom.vars, by.x = &quot;MomID&quot;, by.y = &quot;ID&quot;, all.x=T) # look at the data head(pat.outcrossed) ## MomID OffID DadID Fij X Xend Y Yend Population pollen.dist ## 1 3080 1 211 1 4422657 4422658 5425371 5425371 A25 0.8632989 ## 2 3080 10 211 1 4422657 4422658 5425371 5425371 A25 0.8632989 ## 3 3080 2 214 1 4422657 4422658 5425371 5425370 A25 1.3829147 ## 4 3080 3 214 1 4422657 4422658 5425371 5425370 A25 1.3829147 ## 5 3080 4 211 1 4422657 4422658 5425371 5425371 A25 0.8632989 ## 6 3080 5 211 1 4422657 4422658 5425371 5425371 A25 0.8632989 ## mom.isolation flower.density ## 1 10.97917 11 ## 2 10.97917 11 ## 3 10.97917 11 ## 4 10.97917 11 ## 5 10.97917 11 ## 6 10.97917 11 Lets run some models to test if mom.isolation or flower.density explain pollen flow distances. We used mixed models with population and mother ID as random effects to control for multiple sampling from the same mom and populations (see Week 6 videos for linear mixed models). Note: for model selection with varying fixed effects and constant random effects, we fit the models with maximum likelihood ML, hence we set REML=F (see Week 12). # specify the model mod &lt;- lme4::lmer(log(pollen.dist) ~ scale(log(flower.density)) + scale(log(mom.isolation)) + (1|Population/MomID), data=pat.outcrossed, na.action = &quot;na.fail&quot;, REML=F) ## boundary (singular) fit: see ?isSingular Here we use the function dredge from package MuMIn to compare all nested submodels and select the best model. This is not generally recommended (see Week 12), as submodels may not be biologically meaningful. In this case, there is good reason to hypothesize that either mom isolation or local flower density, or both, would affect pollination distance. MuMIn::dredge(mod) ## Fixed term is &quot;(Intercept)&quot; ## boundary (singular) fit: see ?isSingular ## boundary (singular) fit: see ?isSingular ## boundary (singular) fit: see ?isSingular ## Global model call: lme4::lmer(formula = log(pollen.dist) ~ scale(log(flower.density)) + ## scale(log(mom.isolation)) + (1 | Population/MomID), data = pat.outcrossed, ## REML = F, na.action = &quot;na.fail&quot;) ## --- ## Model selection table ## (Int) scl(log(flw.dns)) scl(log(mom.isl)) df logLik AICc delta weight ## 4 1.075 -0.3609 0.3493 6 -102.581 218.0 0.00 0.698 ## 2 1.076 -0.5151 5 -105.193 221.0 2.99 0.157 ## 3 1.094 0.5977 5 -105.280 221.1 3.16 0.144 ## 1 1.081 4 -110.975 230.3 12.35 0.001 ## Models ranked by AICc(x) ## Random terms (all models): ## &#39;1 | Population/MomID&#39; We see that the best model includes both flower density and mom isolation (for model selection, see Week 12 worked example and Week 7 video). Note: we should follow up with residual analysis etc., which well skip here to keep it short. Lets plot the relationships: Mom.isolation.plot &lt;- ggplot(pat.outcrossed, aes(x=log(mom.isolation), y=log(pollen.dist))) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=F) + xlab(&quot;Log(mom isolation)&quot;) + ylab(&quot;Log(pollen flow distance)&quot;) Flower.density.plot &lt;- ggplot(pat.outcrossed, aes(x=log(flower.density), y=log(pollen.dist))) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=F) + xlab(&quot;Log(flower density)&quot;) + ylab(&quot;Log(pollen flow distance)&quot;) cowplot::plot_grid(Mom.isolation.plot, Flower.density.plot, labels = c(&quot;A&quot;, &quot;B&quot;)) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; Questions: Which moms (isolated/non-isolated, surrounded by more/less flowers) tend to receive pollen from further away? What biological processes might explain these patterns? b. Explain pollen flow at the population level Now lets look at the population level. First we need to calculate how many offspring were unable to be assigned a father. We do this by adding the first four columns of the paternity analysis output (which contains only successful assignments) to the offspring dataframe (which includes all offspring, regardless of assignment). If there is no assigment, Fij will be NA in the newly merged data frame. offspring &lt;- merge(offspring, pat.sub[,1:4], by.x=c(&quot;ID&quot;, &quot;OffID&quot;), by.y=c(&quot;MomID&quot;, &quot;OffID&quot;), all.x=T) head(offspring) ## ID OffID Population X Y loc1_a loc2_a loc3_a loc4_a loc5_a ## 1 3080 1 A25 4422657 5425371 334:358 417:424 392:411 446:448 121:121 ## 2 3080 2 A25 4422657 5425371 334:358 422:424 392:434 442:446 121:121 ## 3 3080 3 A25 4422657 5425371 334:354 422:424 392:434 446:446 118:121 ## 4 3080 4 A25 4422657 5425371 334:354 422:424 392:392 446:446 121:121 ## 5 3080 5 A25 4422657 5425371 352:358 417:424 392:411 446:446 121:121 ## 6 3080 6 A25 4422657 5425371 352:354 417:424 392:392 446:446 121:121 ## loc6_a loc7_a DadID Fij ## 1 157:165 242:258 211 1 ## 2 157:157 252:258 214 1 ## 3 157:157 236:258 214 1 ## 4 153:157 242:258 211 1 ## 5 157:165 240:258 211 1 ## 6 153:157 240:256 211 1 Calculate the number of outside pollination events per population by counting the number of NAs in the Fij column: num.out &lt;- sapply(split(offspring$Fij, offspring$Population), function(x) sum(is.na(x))) Total number of pollination events per population (should be same as the number of seeds sampled): num.tot &lt;- table(offspring$Population) Lets put this information into a data frame and add ecological information that we have gathered about each population. Specifically we have measured the proportion of forest cover within radii of 50, 100, 250, 500, and 1000 metres from population centroids, and have measured population size as the total number of flowering P. vulgaris plants per population. # turn it into a dataframe: pop.df &lt;- data.frame(Population=names(num.out), num.out=as.vector(num.out), num.tot=as.vector(num.tot)) # read in the population variable data: pop.vars &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;pulsatilla_population.csv&quot;, package=&quot;LandGenCourse&quot;)) ## a.d the population variables to our outside pollination data: pop.df &lt;- merge(pop.df, pop.vars, by=&quot;Population&quot;) pop.df ## Population num.out num.tot forest.50 forest.100 forest.250 forest.500 ## 1 A03 2 13 0.3531695 0.5305529 0.8116160 0.8809752 ## 2 A21 17 48 0.6036269 0.5291262 0.5581914 0.6131979 ## 3 A25 22 72 0.3622251 0.2200647 0.2445019 0.2904836 ## 4 A26 13 57 0.4948187 0.4838188 0.5212385 0.4778212 ## 5 A41 11 57 0.6843467 0.8159172 0.9087349 0.7966062 ## 6 A45 15 53 0.3824289 0.4297280 0.4747971 0.4027923 ## 7 G05a 4 15 0.6152850 0.4102232 0.3046323 0.2119176 ## forest.1000 population.size ## 1 0.8723172 44 ## 2 0.4553216 21 ## 3 0.2106881 57 ## 4 0.2314417 22 ## 5 0.6717830 15 ## 6 0.2988908 22 ## 7 0.2810409 46 Now we can run a model to see which variables best explain the proportion of immigrated pollen per population. Because we only have n=7 populations, we limit the models to a single explanatory variable. Our response variable is a proportion, so we use a glm (generalized linear model) with binomial error distribution. This model does not include any random effects. To automatize this, we first fit a full model with all six potential predictors. We wont interpret this model as it would be overfitted, but well use it as input for the next step. # specify the model mod2 &lt;- glm(cbind(num.out, num.tot-num.out) ~ forest.50 + forest.100 + forest.250 + forest.500 + forest.1000 + population.size, family = binomial(link = &quot;logit&quot;), data = pop.df, na.action = &quot;na.fail&quot;) We use the function dredge again to select the best model among those with only a single predictors (plus the intercept-only model). This is specified with the argument m.lim=c(0,1). MuMIn::dredge(mod2,m.lim=c(0,1)) ## Fixed term is &quot;(Intercept)&quot; ## Global model call: glm(formula = cbind(num.out, num.tot - num.out) ~ forest.50 + ## forest.100 + forest.250 + forest.500 + forest.1000 + population.size, ## family = binomial(link = &quot;logit&quot;), data = pop.df, na.action = &quot;na.fail&quot;) ## --- ## Model selection table ## (Int) frs.100 frs.1000 frs.250 frs.50 frs.500 ppl.siz df logLik AICc ## 1 -1.0120 1 -16.061 34.9 ## 5 -0.5516 -0.8899 2 -14.866 36.7 ## 2 -0.5758 -0.9236 2 -15.118 37.2 ## 17 -0.6049 -0.8182 2 -15.268 37.5 ## 3 -0.7071 -0.8159 2 -15.318 37.6 ## 33 -1.1830 0.00555 2 -15.807 38.6 ## 9 -0.7084 -0.6136 2 -15.877 38.8 ## delta weight ## 1 0.00 0.392 ## 5 1.81 0.158 ## 2 2.31 0.123 ## 17 2.62 0.106 ## 3 2.71 0.101 ## 33 3.69 0.062 ## 9 3.83 0.058 ## Models ranked by AICc(x) We see that the best model is the intercept-only (null) model, suggesting that none of the predictors fit the data very well. However, the next-best model includes forest.250 with a delta AICc of less than 2, and we take this model to be equally likely. Note: Interestingly, in our original paper with found that forest.500 was the best predictor, followed by population size. This suggests that the choice of paternity analysis method really can make a difference. Lets plot the model including forest.250: forest.250.mod &lt;- glm(cbind(num.out, num.tot-num.out) ~ forest.250, family=binomial(link=&quot;logit&quot;), data=pop.df) ggplot(pop.df, aes(x=forest.250, y=num.out/num.tot)) + geom_point() + geom_line(aes(x=forest.250, y=predict(forest.250.mod, type=&quot;response&quot;))) + xlab(&quot;Proportion of forest at 250 m&quot;) + ylab(&quot;Proportion of immigrant pollen&quot;) We see that populations surrounded by more forest receive less outside pollen, although it is apparent that there is one outlier (population A21) with a high proportion of immigration pollen but intermediate levels of surrounding forest. This is likely also a product of our choice of paternity anlaysis - in our original paper (DiLeo et al. in press) we were able to assign a much higher proportion of fathers in popualtion A21, giving us lower pollen immigration rates. 6. References DiLeo MF, R Holderegger and HH Wagner (in press). Contemporary pollen flow as a multiscale process: evidence from the insect-pollinated herb Pulsatilla vulgaris. Journal of Ecology. https://doi.org/10.1111/1365-2745.12992 Dyer RJ, RD Westfall, VL Sork VL and PE Smouse (2004). Two-generation analysis of pollen flow across a landscape V: a stepwise approach for extracting factors contributing to pollen structure. Heredity 92: 204-211. Dyer RJ, Chan DM, Gardiakos VA, Meadows CA (2012) Pollination graphs: quantifying pollen pool covariance networks and the influence of intervening landscape on genetic connectivity in the North American understory tree, Cornus florida L. Landscape ecology 27:239-51. Smouse PE, RJ Dyer, RD Westfall and VL Sork (2001). Twogeneration analysis of pollen flow across a landscape. I. Male gamete heterogeneity among females. Evolution 55: 260271. "]]
